{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87943807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 다른 곳으로 옮기기 \n",
    "# !mv 0406_word2vec_구현.ipynb kdt_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 이론 #####\n",
    "# word embedding\n",
    "\n",
    "# Word2Vec : 수많은 단어를 통해 단어 벡터를 학습\n",
    "#          : 유사 단어 간에 가까운 경향을 파악 \n",
    "\n",
    "# 많은 양의 데이터 + 단순한 모델이 적은 데이터 + 복잡한 모델보다 더 낫다는 것을 가정\n",
    "# 수십억 단어로 질 좋은 단어 벡터를 학습\n",
    "# 유사 단어 간에는 거리가 가까운 경향이 있고, 단어는 다양한 유사도를 가짐\n",
    "\n",
    "# Model architecture:\n",
    "    # LSA보다 뛰어난 선형 정규성\n",
    "    # LDA는 데이터 양이 많을수록 많은 연산을 필요로 함. (cost가 많이 듬)\n",
    "    # Word2Vec 복잡도 O=E x T x Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "420f11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "    # CBOW (continuous bag of words) : 주변 단어를 보고 해당 타켓 단어를 예측하는 방법\n",
    "        # 1. sliding window=5\n",
    "        # 2. target word\n",
    "        # 3. context \n",
    "        # 이렇게 3가지 정도가 있다고 볼 수 있음.\n",
    "        \n",
    "        # V => unique한 단어 벡터의 수\n",
    "        # N => (사용자가 지정 가능한) N만큼 V의 값을 가져옴\n",
    "            # N만큼의 행렬이 생성 => 여기서 평균을 낸것이 hidden vector \n",
    "            \n",
    "    # skip-gram model\n",
    "        # 하나의 단어를 보고 주변 단어를 예측\n",
    "        # 낮은 연산 복잡도\n",
    "        # 높은 정확도\n",
    "        \n",
    "        \n",
    "# Transfer Learning / Fine Tuning\n",
    "    # transfer learning : 풍부한 데이터가 있는 분야에서 훈련된 모델을 재사용\n",
    "    # fine tuning : 사전 학습된 모델을 task에 적합한 데이터로 추가 학습하여 파라미터 미세 조정\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1085ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 필요과정\n",
    "    # Feed Forwarding\n",
    "    # Backpropagation\n",
    "    # Weights 갱신\n",
    "    # Loss 그래프\n",
    "    # 벡터간 유사도 측정\n",
    "    # 가장 유사한 벡터 목록 출력\n",
    "    \n",
    "# CBOW 학습\n",
    "    # 입력 문장 토큰화 (사용하지 않는 품사는 제거)\n",
    "    # 가중치 (=파라미터=단어벡터) 초기화\n",
    "    # 원핫 인코딩 (one hot encoding)\n",
    "    # epoch 만큼 반복 \n",
    "        # 중심단어 문맥단어 추출\n",
    "        # Feed Forward\n",
    "        # Loss / Gradient 계산\n",
    "        # Weight Parameter 계산\n",
    "\n",
    "## 학습 과정 ##\n",
    "# 1. 학습하는 가중치가 단어 벡터\n",
    "# 2. 입력 단어 (원핫 벡터)와 단어 벡터의 곱으로 hidden layer 계산 (Feed Forward 1)\n",
    "# 3. Hidden layer 선택된 \"하나의\" 벡터 \n",
    "# 4. Hidden layer와 단어 벡터의 곱으로 output layer 계산 (Feed Forward 2)\n",
    "# 5. (단어간 유사도 계산) 선택된 단어 벡터와 전체 단어 벡터간 유사도 ==> softmax()\n",
    "# 6. epoch 별 Loss / Gradient 계산 (W1 행렬, W2 행렬 편미분 그리고 가중치 업데이트)\n",
    "# 7. CrossEntrophy (혹은 BinaryEntrophy) : weights를 gradient * learning rate 만큼 갱신\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ece0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### fasttext #####\n",
    "# 단어를 bag of characters로 보고 n_gram의 characters embedding 함 (skip_gram model 사용)\n",
    "# Fasttext는 단어를 쪼갤 수 있음 ==> subwords (내부 단어)\n",
    "    # 때문에 내부 단어를 통해 모르는 단어 (OOV)에 대해서도 다른 단어와의 유사도를 계산 가능\n",
    "    \n",
    "# Fasttext의 경우 단어에 n_gram을 하기 때문에 학습 경우의 수가 많아지므로 정확도가 높아지는 경향이 있음.\n",
    "# Fasttext는 오타가 섞여도 일정 수준 성능을 보임 ==> 노이즈가 많은 말뭉치에 강함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6f696",
   "metadata": {},
   "source": [
    "***예제***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cff29263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 입력 문장 토큰화 \n",
    "sent = 'you will never know until you try'\n",
    "tokens = sent.split()\n",
    "\n",
    "# 가중치 (초기화)\n",
    "V = len(tokens)\n",
    "N = 4\n",
    "\n",
    "W1 = np.random.rand(V,N)\n",
    "W2 = np.random.rand(V,N)\n",
    "\n",
    "# 원핫 인코딩 \n",
    "onehot_vector = np.zeros(shape=(V,V), dtype =np.int32)\n",
    "unique_tokens = set(tokens)\n",
    "unique_tokens\n",
    "\n",
    "\n",
    "word2idx = {}\n",
    "for idx, word in enumerate(unique_tokens):\n",
    "    onehot_vector[idx][idx] = 1\n",
    "    word2idx[word] = idx\n",
    "    \n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "\n",
    "# 학습 데이터 생성\n",
    "# window_size = 1\n",
    "window_size=1\n",
    "training_data = []\n",
    "for idx, token in enumerate(tokens):\n",
    "    target_word = token\n",
    "    context_words = []\n",
    "    for window in range(1, window_size+1):\n",
    "        if idx-window >= 0:\n",
    "            context_words.append(tokens[idx-window])\n",
    "        if idx+window < len(tokens):\n",
    "            context_words.append(tokens[idx+window])  \n",
    "    #context_words = [tokens[idx-1], tokens[idx+1]] # window 사이즈가 1이고 주변 단어를 가져옴\n",
    "                              \n",
    "    training_data.append([context_words, target_word])\n",
    "    \n",
    "onehot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e2712b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01821127, -0.24519181,  0.26340309])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 값을 1아래로 만들어주는 함수 \n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y=exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "a = np.array([0.3, 2.9, 4.0])\n",
    "predict = softmax(a)\n",
    "output = np.array([0,0,1]) \n",
    "loss = output - predict # 예측한 것과 실제 값의 차이 \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "011ed281",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m         vector_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m context_words:\n\u001b[0;32m---> 15\u001b[0m             vector_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43monehot_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword2idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m         x\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mvstack(vector_list)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#         print(x)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "# Word2Vec 학습구현\n",
    "\n",
    "\n",
    "# 4. Epoch\n",
    "# - epoch 수 만큼 반복\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for context_words, target_word in training_data:\n",
    "        # 1. Feed Forward\n",
    "        # - context 단어를 W1 행렬에 곱해서 hidden vector 만들기\n",
    "        # - 만든 hidden vector들 평균내기\n",
    "        vector_list = []\n",
    "        for word in context_words:\n",
    "            vector_list.append(onehot_vector(word2idx[word]))\n",
    "        x=np.vstack(vector_list)\n",
    "#         print(x)\n",
    "        hidden_vector= x.dot(W1)\n",
    "        hidden_vector_avg = np.sum(hidden_vector, axis=0)/len(hidden_vector)\n",
    "        # - 평균 낸 hidden vector를 가지고 W2에 곱해서 Vx1 크기의 벡터로 만들기\n",
    "        # - softmax 씌어숴 합이 1이 되도록 조정\n",
    "#         hidden_vector_avg.reshape(1, N).dot(W2) # (1, N) x V ( 1 x V ) => V x 1\n",
    "        output_layer = W2.dot(hidden_vector_avg.T) # V x N * N X (1,2) => V X (1,2)\n",
    "#         print(output_layer) # (V, ) => 1차원 \n",
    "        output_layer = softmax(output_layer)\n",
    "        real_vector = onehot_vector[word2idx[target_word]]\n",
    "        \n",
    "        # 2. Loss 계산\n",
    "        # - cross entrophy loss\n",
    "        print(real_vector*np.log(output_layer))\n",
    "        loss = -np.sum(real_vector * np.log(output_vector))\n",
    "#         print(loss) # loss가 나옴. \n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # 3. Backpropagation\n",
    "        # - 각 행렬에 대해서 편미분한 기울기 구하기\n",
    "        # - 각 행렬에 대해 기울기 + learning rate 만큼 가중치 update\n",
    "\n",
    "        # W2 기울기\n",
    "        # Y_hat - Y\n",
    "        diff = output_layer - real_vector.reshape(-1,1) # V x (1,2) - real_vector V, 1 \n",
    "        print(diff) # (V, 1,2) 1,2 차원\n",
    "        # Hidden Vector와 dot_product -> V x N\n",
    "        # V x 1 * 1 X N => V x N\n",
    "        gradient_w2 = diff.reshape(-1, 1).dot(hidden_vector.reshape(1, -1))\n",
    "        print(gradient_w2)\n",
    "        \n",
    "        # W1 기울기 V x N                              \n",
    "        # X * (W2 * diff) => 2 X V * V X N * V x (1,2) \n",
    "        # (V x N * V X (1,2)) => (1,2) X N\n",
    "        # => V x (1,2) * (1,2) X N => V x N\n",
    "        temp = diff.T.dot(W2) # (1,2) x V * V x N => (1,2) x N\n",
    "        gradient_1 = X.T.dot(temp) # V x (1,2) * (1,2) * N => V x N\n",
    "        print(gradient_w1)\n",
    "        \n",
    "        W1 -= learning_rate * gradient_w1\n",
    "        W2 -= leanring_rate * gradient_w2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b46f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'will': 0, 'until': 1, 'know': 2, 'never': 3, 'try': 4, 'you': 5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1v = W1[word2idx['try']] \n",
    "w2v = W1[word2idx['know']]\n",
    "\n",
    "cos_sim = lambda x, y : x * y / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "cos_sim(w1v, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_list) # 반복해서 학습하다보니 튀는 경향이 있음. 학습할 데이터가 너무 적기도 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de751b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 대량 corpus\n",
    "\n",
    "# 1. 입력 계산 (onehot_vector를 만들지 않고, word2idx 딕션너리를 통해서 hidden_layer를 계산)\n",
    "# 2. loss 계산이 전체 단어에 대해서 1, 0을 계산하는 것이 아니라 해당 target_word만 1,0 binary cross entropy 계산 \n",
    "# (sigmoid 함수를 output_layer에)\n",
    "# 3. (오답도 학습을 하기 위해 sampling을 통해서 0으로 예측해야 되는 단어들도 같이 학습시킨다.) W2에서 index를 뽑아서 행렬곱 \n",
    "# (output_layer들 각각 나온다)\n",
    "\n",
    "# 문맥 단어, 타겟 단어 (2천만개) 있음 ==> too many\n",
    "# 4. batch_size(속도 개선) \n",
    "# 128개씩, 256개씩 \n",
    "\n",
    "# 2천만번 계산할 것인지,\n",
    "# 15만 6천번 연산할 것인지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c666a371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['will', 'you'],\n",
       " ['you', 'will'],\n",
       " ['never', 'will'],\n",
       " ['will', 'never'],\n",
       " ['know', 'never'],\n",
       " ['never', 'know'],\n",
       " ['until', 'know'],\n",
       " ['know', 'until'],\n",
       " ['you', 'until'],\n",
       " ['until', 'you'],\n",
       " ['try', 'you'],\n",
       " ['you', 'try']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data2 = []\n",
    "for context_words, target_word in training_data:\n",
    "    for word in context_words:\n",
    "        training_data2.append([word, target_word])\n",
    "    \n",
    "training_data2 # 위의 Word2Vec과 달리 1열로 쭉 나타냄 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbf252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97552d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa99238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ea5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27608aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a416625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0178d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
