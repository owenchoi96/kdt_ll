{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVE5jxvfTYNI",
        "outputId": "9d103705-84f6-428e-ae4d-4a0b36900166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install keras_preprocessing\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LzeKCC_Tjej",
        "outputId": "7c0fc43c-6b0f-4f3c-bb58-889c3d6c2249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL2NUM = {\n",
        "    '310300200':0,\n",
        "    '310300100':1,\n",
        "    '310300300':2,\n",
        "    '310300400':3,\n",
        "    '310300600':4,\n",
        "    '310300500':5,\n",
        "    '310260800':6,\n",
        "    '310260700':7,\n",
        "    '310260600':8,\n",
        "    '310260500':9,\n",
        "    '310260400':10,\n",
        "    '310260100':11,\n",
        "    '310260200':12,\n",
        "    '310260300':13,\n",
        "    '310150080':14,\n",
        "    '310150010':15,\n",
        "    '310150030':16,\n",
        "    '310150090':17,\n",
        "    '310150040':18,\n",
        "    '310150070':19,\n",
        "    '310130030':20,\n",
        "    '310130080':21,\n",
        "    '310130040':22,\n",
        "    '310120030':23,\n",
        "    '310120110':24,\n",
        "    '310120020':25,\n",
        "    '310250':26,\n",
        "    '310400100':27,\n",
        "    '310400200':28,\n",
        "    '310200200':29,\n",
        "    '310200100':30,\n",
        "    '310220300':31,\n",
        "    '310220100':32,\n",
        "    '310220200':33,\n",
        "    '320300300':34,\n",
        "    '320300200':35,\n",
        "    '320300100':36,\n",
        "    '320300400':37,\n",
        "    '320300600':38,\n",
        "    '320300500':39,\n",
        "    '320210600':40,\n",
        "    '320210500':41,\n",
        "    '320210700':42,\n",
        "    '320210400':43,\n",
        "    '320210100':44,\n",
        "    '320210200':45,\n",
        "    '320210300':46,\n",
        "    '320120600':47,\n",
        "    '320120100':48,\n",
        "    '320120200':49,\n",
        "    '320120300':50,\n",
        "    '320120700':51,\n",
        "    '320400':52,\n",
        "    '320500100':53,\n",
        "    '320500200':54,\n",
        "    '320160700':55,\n",
        "    '320160800':56,\n",
        "    '320180700':57,\n",
        "    '320180600':58,\n",
        "    '405100':59,\n",
        "    '405300300':60,\n",
        "    '405300100':61,\n",
        "    '405300200':62,\n",
        "    '405200300':63,\n",
        "    '405200400':64,\n",
        "    '405200100':65,\n",
        "    '405200200':66,\n",
        "    '405400200':67,\n",
        "    '405400300':68,\n",
        "    '405400400':69,\n",
        "    '405400500':70,\n",
        "    '405400100':71,\n",
        "    '405400600':72,\n",
        "    '430100400':73,\n",
        "    '430100100':74,\n",
        "    '430100200':75,\n",
        "    '430100300':76,\n",
        "    '430100500':77,\n",
        "    '430200100':78,\n",
        "    '430200300':79,\n",
        "    '430200200':80,\n",
        "    '430200500':81,\n",
        "    '430200400':82,\n",
        "    '430300100':83,\n",
        "    '430400':84,\n",
        "    '430500':85,\n",
        "    '421200100':86,\n",
        "    '421200200':87,\n",
        "    '421200300':88,\n",
        "    '421200400':89,\n",
        "    '421200500':90,\n",
        "    '421100100':91,\n",
        "    '421100200':92,\n",
        "    '421100300':93,\n",
        "    '421100400':94,\n",
        "    '421100500':95,\n",
        "    '421300100':96,\n",
        "    '422200100':97,\n",
        "    '422200200':98,\n",
        "    '422200300':99,\n",
        "    '422200400':100,\n",
        "    '422200500':101,\n",
        "    '422200600':102,\n",
        "    '422200700':103,\n",
        "    '422100100':104,\n",
        "    '422100200':105,\n",
        "    '422100300':106,\n",
        "    '422100400':107,\n",
        "    '422100500':108,\n",
        "    '422100600':109,\n",
        "    '422300100':110,\n",
        "    '422300200':111,\n",
        "    '422300300':112,\n",
        "    '422400100':113,\n",
        "    '422400200':114,\n",
        "    '422500100':115,\n",
        "    '422500200':116,\n",
        "    '422500300':117,\n",
        "    '422500400':118,\n",
        "    '422500500':119,\n",
        "    '422600100':120,\n",
        "    '422600200':121,\n",
        "    '400070100':122,\n",
        "    '400070700':123,\n",
        "    '400070200':124,\n",
        "    '400070500':125,\n",
        "    '400080300':126,\n",
        "    '400080200':127,\n",
        "    '400120100':128,\n",
        "    '400120200':129,\n",
        "    '400130100':130,\n",
        "    '400130200':131,\n",
        "    '400110100':132,\n",
        "    '400110200':133,\n",
        "    '400140100':134,\n",
        "    '400140200':135,\n",
        "    '400090':136,\n",
        "    '400600':137\n",
        "}\n",
        "\n",
        "LABEL2NAME = {\n",
        "    '310300200':{'대분류':'여성의류', '중분류':'아우터', '소분류':'패딩'},\n",
        "    '310300100':{'대분류':'여성의류', '중분류':'아우터', '소분류':'점퍼'},\n",
        "    '310300300':{'대분류':'여성의류', '중분류':'아우터', '소분류':'코트'},\n",
        "    '310300400':{'대분류':'여성의류', '중분류':'아우터', '소분류':'자켓'},\n",
        "    '310300600':{'대분류':'여성의류', '중분류':'아우터', '소분류':'가디건'},\n",
        "    '310300500':{'대분류':'여성의류', '중분류':'아우터', '소분류':'조끼/베스트'},\n",
        "    '310260800':{'대분류':'여성의류', '중분류':'상의', '소분류':'니트/스웨터'},\n",
        "    '310260700':{'대분류':'여성의류', '중분류':'상의', '소분류':'후드티/후드집업'},\n",
        "    '310260600':{'대분류':'여성의류', '중분류':'상의', '소분류':'맨투맨'},\n",
        "    '310260500':{'대분류':'여성의류', '중분류':'상의', '소분류':'블라우스'},\n",
        "    '310260400':{'대분류':'여성의류', '중분류':'상의', '소분류':'셔츠'},\n",
        "    '310260100':{'대분류':'여성의류', '중분류':'상의', '소분류':'반팔 티셔츠'},\n",
        "    '310260200':{'대분류':'여성의류', '중분류':'상의', '소분류':'긴팔 티셔츠'},\n",
        "    '310260300':{'대분류':'여성의류', '중분류':'상의', '소분류':'민소매 티셔츠'},\n",
        "    '310150080':{'대분류':'여성의류', '중분류':'바지', '소분류':'데님/청바지'},\n",
        "    '310150010':{'대분류':'여성의류', '중분류':'바지', '소분류':'슬랙스'},\n",
        "    '310150030':{'대분류':'여성의류', '중분류':'바지', '소분류':'면바지'},\n",
        "    '310150090':{'대분류':'여성의류', '중분류':'바지', '소분류':'반바지'},\n",
        "    '310150040':{'대분류':'여성의류', '중분류':'바지', '소분류':'트레이닝/조거팬츠'},\n",
        "    '310150070':{'대분류':'여성의류', '중분류':'바지', '소분류':'레깅스'},\n",
        "    '310130030':{'대분류':'여성의류', '중분류':'치마', '소분류':'롱 스커트'},\n",
        "    '310130080':{'대분류':'여성의류', '중분류':'치마', '소분류':'미디 스커트'},\n",
        "    '310130040':{'대분류':'여성의류', '중분류':'치마', '소분류':'미니 스커트'},\n",
        "    '310120030':{'대분류':'여성의류', '중분류':'원피스', '소분류':'롱 원피스'},\n",
        "    '310120110':{'대분류':'여성의류', '중분류':'원피스', '소분류':'미디 원피스'},\n",
        "    '310120020':{'대분류':'여성의류', '중분류':'원피스', '소분류':'미니 원피스'},\n",
        "    '310250':{'대분류':'여성의류', '중분류':'점프수트'},\n",
        "    '310400100':{'대분류':'여성의류', '중분류':'셋업/세트', '소분류':'정장/셋업'},\n",
        "    '310400200':{'대분류':'여성의류', '중분류':'셋업/세트', '소분류':'트레이닝/스웨트 셋업'},\n",
        "    '310200200':{'대분류':'여성의류', '중분류':'언더웨어/홈웨어', '소분류':'홈웨어'},\n",
        "    '310200100':{'대분류':'여성의류', '중분류':'언더웨어/홈웨어', '소분류':'언더웨어'},\n",
        "    '310220300':{'대분류':'여성의류', '중분류':'테마/이벤트', '소분류':'코스튬/코스프레'},\n",
        "    '310220100':{'대분류':'여성의류', '중분류':'테마/이벤트', '소분류':'한복'},\n",
        "    '310220200':{'대분류':'여성의류', '중분류':'테마/이벤트', '소분류':'드레스'},\n",
        "    '320300300':{'대분류':'남성의류', '중분류':'아우터', '소분류':'패딩'},\n",
        "    '320300200':{'대분류':'남성의류', '중분류':'아우터', '소분류':'점퍼'},\n",
        "    '320300100':{'대분류':'남성의류', '중분류':'아우터', '소분류':'코트'},\n",
        "    '320300400':{'대분류':'남성의류', '중분류':'아우터', '소분류':'자켓'},\n",
        "    '320300600':{'대분류':'남성의류', '중분류':'아우터', '소분류':'가디건'},\n",
        "    '320300500':{'대분류':'남성의류', '중분류':'아우터', '소분류':'조끼/베스트'},\n",
        "    '320210600':{'대분류':'남성의류', '중분류':'상의', '소분류':'후드티/후드집업'},\n",
        "    '320210500':{'대분류':'남성의류', '중분류':'상의', '소분류':'맨투맨'},\n",
        "    '320210700':{'대분류':'남성의류', '중분류':'상의', '소분류':'니트/스웨터'},\n",
        "    '320210400':{'대분류':'남성의류', '중분류':'상의', '소분류':'셔츠'},\n",
        "    '320210100':{'대분류':'남성의류', '중분류':'상의', '소분류':'반팔 티셔츠'},\n",
        "    '320210200':{'대분류':'남성의류', '중분류':'상의', '소분류':'긴팔 티셔츠'},\n",
        "    '320210300':{'대분류':'남성의류', '중분류':'상의', '소분류':'민소매 티셔츠'},\n",
        "    '320120600':{'대분류':'남성의류', '중분류':'바지', '소분류':'데님/청바지'},\n",
        "    '320120100':{'대분류':'남성의류', '중분류':'바지', '소분류':'면바지'},\n",
        "    '320120200':{'대분류':'남성의류', '중분류':'바지', '소분류':'슬랙스'},\n",
        "    '320120300':{'대분류':'남성의류', '중분류':'바지', '소분류':'트레이닝/조거팬츠'},\n",
        "    '320120700':{'대분류':'남성의류', '중분류':'바지', '소분류':'반바지'},\n",
        "    '320400':{'대분류':'남성의류', '중분류':'점프수트'},\n",
        "    '320500100':{'대분류':'남성의류', '중분류':'셋업/세트', '소분류':'정장/셋업'},\n",
        "    '320500200':{'대분류':'남성의류', '중분류':'셋업/세트', '소분류':'트레이닝/스웨트 셋업'},\n",
        "    '320160700':{'대분류':'남성의류', '중분류':'언더웨어/홈웨어', '소분류':'언더웨어'},\n",
        "    '320160800':{'대분류':'남성의류', '중분류':'언더웨어/홈웨어', '소분류':'홈웨어'},\n",
        "    '320180700':{'대분류':'남성의류', '중분류':'테마/이벤트', '소분류':'코스튬/코스프레'},\n",
        "    '320180600':{'대분류':'남성의류', '중분류':'테마/이벤트', '소분류':'한복'},\n",
        "    '405100':{'대분류':'신발', '중분류':'스니커즈'},\n",
        "    '405300300':{'대분류':'신발', '중분류':'남성화', '소분류':'샌들/슬리퍼'},\n",
        "    '405300100':{'대분류':'신발', '중분류':'남성화', '소분류':'구두/로퍼'},\n",
        "    '405300200':{'대분류':'신발', '중분류':'남성화', '소분류':'워커/부츠'},\n",
        "    '405200300':{'대분류':'신발', '중분류':'여성화', '소분류':'샌들/슬리퍼'},\n",
        "    '405200400':{'대분류':'신발', '중분류':'여성화', '소분류':'구두'},\n",
        "    '405200100':{'대분류':'신발', '중분류':'여성화', '소분류':'단화/플랫슈즈'},\n",
        "    '405200200':{'대분류':'신발', '중분류':'여성화', '소분류':'워커/부츠'},\n",
        "    '405400200':{'대분류':'신발', '중분류':'스포츠화', '소분류':'농구화'},\n",
        "    '405400300':{'대분류':'신발', '중분류':'스포츠화', '소분류':'골프화'},\n",
        "    '405400400':{'대분류':'신발', '중분류':'스포츠화', '소분류':'축구/풋살화'},\n",
        "    '405400500':{'대분류':'신발', '중분류':'스포츠화', '소분류':'테니스화'},\n",
        "    '405400100':{'대분류':'신발', '중분류':'스포츠화', '소분류':'등산화/트레킹화'},\n",
        "    '405400600':{'대분류':'신발', '중분류':'스포츠화', '소분류':'야구화'},\n",
        "    '430100400':{'대분류':'가방/지갑', '중분류':'여성가방', '소분류':'클러치백'},\n",
        "    '430100100':{'대분류':'가방/지갑', '중분류':'여성가방', '소분류':'숄더백'},\n",
        "    '430100200':{'대분류':'가방/지갑', '중분류':'여성가방', '소분류':'크로스백'},\n",
        "    '430100300':{'대분류':'가방/지갑', '중분류':'여성가방', '소분류':'토트백'},\n",
        "    '430100500':{'대분류':'가방/지갑', '중분류':'여성가방', '소분류':'백팩'},\n",
        "    '430200100':{'대분류':'가방/지갑', '중분류':'남성가방', '소분류':'클러치'},\n",
        "    '430200300':{'대분류':'가방/지갑', '중분류':'남성가방', '소분류':'숄더백'},\n",
        "    '430200200':{'대분류':'가방/지갑', '중분류':'남성가방', '소분류':'크로스백'},\n",
        "    '430200500':{'대분류':'가방/지갑', '중분류':'남성가방', '소분류':'브리프케이스'},\n",
        "    '430200400':{'대분류':'가방/지갑', '중분류':'남성가방', '소분류':'백팩'},\n",
        "    '430300100':{'대분류':'가방/지갑', '중분류':'남성가방', '소분류':'캐리어'},\n",
        "    '430400':{'대분류':'가방/지갑', '중분류':'여성지갑'},\n",
        "    '430500':{'대분류':'가방/지갑', '중분류':'남성지갑'},\n",
        "    '421200100':{'대분류':'시계', '중분류':'남성시계', '소분류':'프리미엄 시계(디지털)'},\n",
        "    '421200200':{'대분류':'시계', '중분류':'남성시계', '소분류':'프리미엄 시계(쿼츠)'},\n",
        "    '421200300':{'대분류':'시계', '중분류':'남성시계', '소분류':'프리미엄 시계(오토매틱)'},\n",
        "    '421200400':{'대분류':'시계', '중분류':'남성시계', '소분류':'일반 시계(메탈 밴드)'},\n",
        "    '421200500':{'대분류':'시계', '중분류':'남성시계', '소분류':'일반 시계(가죽 밴드)'},\n",
        "    '421100100':{'대분류':'시계', '중분류':'여성시계', '소분류':'프리미엄 시계(디지털)'},\n",
        "    '421100200':{'대분류':'시계', '중분류':'여성시계', '소분류':'프리미엄 시계(쿼츠)'},\n",
        "    '421100300':{'대분류':'시계', '중분류':'여성시계', '소분류':'프리미엄 시계(오토매틱)'},\n",
        "    '421100400':{'대분류':'시계', '중분류':'여성시계', '소분류':'일반 시계(메탈 밴드)'},\n",
        "    '421100500':{'대분류':'시계', '중분류':'여성시계', '소분류':'일반 시계(가죽 밴드)'},\n",
        "    '421300100':{'대분류':'시계', '중분류':'시계용품', '소분류':'스트랩'},\n",
        "    '422200100':{'대분류':'쥬얼리', '중분류':'귀걸이/피어싱', '소분류':'다이아몬드 귀걸이'},\n",
        "    '422200200':{'대분류':'쥬얼리', '중분류':'귀걸이/피어싱', '소분류':'금 귀걸이'},\n",
        "    '422200300':{'대분류':'쥬얼리', '중분류':'귀걸이/피어싱', '소분류':'은 귀걸이'},\n",
        "    '422200400':{'대분류':'쥬얼리', '중분류':'귀걸이/피어싱', '소분류':'진주/유색보석 귀걸이'},\n",
        "    '422200500':{'대분류':'쥬얼리', '중분류':'귀걸이/피어싱', '소분류':'패션 귀걸이'},\n",
        "    '422200600':{'대분류':'쥬얼리', '중분류':'귀걸이/피어싱', '소분류':'피어싱'},\n",
        "    '422200700':{'대분류':'쥬얼리', '중분류':'귀걸이/피어싱', '소분류':'귀찌/이어커프'},\n",
        "    '422100100':{'대분류':'쥬얼리', '중분류':'목걸이/펜던트', '소분류':'다이아몬드 목걸이'},\n",
        "    '422100200':{'대분류':'쥬얼리', '중분류':'목걸이/펜던트', '소분류':'금 목걸이'},\n",
        "    '422100300':{'대분류':'쥬얼리', '중분류':'목걸이/펜던트', '소분류':'은 목걸이'},\n",
        "    '422100400':{'대분류':'쥬얼리', '중분류':'목걸이/펜던트', '소분류':'진주/유색보석 목걸이'},\n",
        "    '422100500':{'대분류':'쥬얼리', '중분류':'목걸이/펜던트', '소분류':'패션 목걸이'},\n",
        "    '422100600':{'대분류':'쥬얼리', '중분류':'목걸이/펜던트', '소분류':'초커'},\n",
        "    '422300100':{'대분류':'쥬얼리', '중분류':'팔찌', '소분류':'금팔찌'},\n",
        "    '422300200':{'대분류':'쥬얼리', '중분류':'팔찌', '소분류':'은팔찌'},\n",
        "    '422300300':{'대분류':'쥬얼리', '중분류':'팔찌', '소분류':'패션 팔찌'},\n",
        "    '422400100':{'대분류':'쥬얼리', '중분류':'발찌', '소분류':'금발찌'},\n",
        "    '422400200':{'대분류':'쥬얼리', '중분류':'발찌', '소분류':'패션발찌'},\n",
        "    '422500100':{'대분류':'쥬얼리', '중분류':'반지', '소분류':'다이아몬드 반지'},\n",
        "    '422500200':{'대분류':'쥬얼리', '중분류':'반지', '소분류':'금반지'},\n",
        "    '422500300':{'대분류':'쥬얼리', '중분류':'반지', '소분류':'은반지'},\n",
        "    '422500400':{'대분류':'쥬얼리', '중분류':'반지', '소분류':'진주/유색보석 반지'},\n",
        "    '422500500':{'대분류':'쥬얼리', '중분류':'반지', '소분류':'패션반지'},\n",
        "    '422600100':{'대분류':'쥬얼리', '중분류':'쥬얼리 세트', '소분류':'귀금속 쥬얼리 세트'},\n",
        "    '422600200':{'대분류':'쥬얼리', '중분류':'쥬얼리 세트', '소분류':'패션 쥬얼리 세트'},\n",
        "    '400070100':{'대분류':'패션 액세서리', '중분류':'모자', '소분류':'볼캡'},\n",
        "    '400070700':{'대분류':'패션 액세서리', '중분류':'모자', '소분류':'버킷'},\n",
        "    '400070200':{'대분류':'패션 액세서리', '중분류':'모자', '소분류':'스냅백'},\n",
        "    '400070500':{'대분류':'패션 액세서리', '중분류':'모자', '소분류':'비니'},\n",
        "    '400080300':{'대분류':'패션 액세서리', '중분류':'안경/선글라스', '소분류':'선글라스'},\n",
        "    '400080200':{'대분류':'패션 액세서리', '중분류':'안경/선글라스', '소분류':'안경'},\n",
        "    '400120100':{'대분류':'패션 액세서리', '중분류':'목도리/장갑', '소분류':'목도리'},\n",
        "    '400120200':{'대분류':'패션 액세서리', '중분류':'목도리/장갑', '소분류':'장갑'},\n",
        "    '400130100':{'대분류':'패션 액세서리', '중분류':'스카프/넥타이', '소분류':'스카프'},\n",
        "    '400130200':{'대분류':'패션 액세서리', '중분류':'스카프/넥타이', '소분류':'넥타이'},\n",
        "    '400110100':{'대분류':'패션 액세서리', '중분류':'벨트', '소분류':'남성 벨트'},\n",
        "    '400110200':{'대분류':'패션 액세서리', '중분류':'벨트', '소분류':'여성 벨트'},\n",
        "    '400140100':{'대분류':'패션 액세서리', '중분류':'양말/스타킹', '소분류':'양말'},\n",
        "    '400140200':{'대분류':'패션 액세서리', '중분류':'양말/스타킹', '소분류':'스타킹'},\n",
        "    '400090':{'대분류':'패션 액세서리', '중분류':'우산/양산'},\n",
        "    '400600':{'대분류':'패션 액세서리', '중분류':'키링/키케이스'}\n",
        "}"
      ],
      "metadata": {
        "id": "Ww8JO2COTxvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.auto.modeling_auto import MODEL_FOR_VISION_2_SEQ_MAPPING\n",
        "from transformers import ElectraForSequenceClassification, AutoTokenizer, AdamW, ElectraTokenizer\n",
        "from torch import nn\n",
        "\n",
        "num_classes = 138\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model1 (KoElectra)\n",
        "model1 = torch.load('/content/MyDrive/MyDrive/model/KoElectra/koelectra_small_model_for_ensemble.pt').to(device)\n",
        "model1.load_state_dict(torch.load('/content/MyDrive/MyDrive/model/KoElectra/koelectra_small_model_state_dict_for_ensemble.pt'))\n",
        "# optimizer = AdamW(model1.parameters(), lr = 2e-5, eps = 1e-8)\n",
        "\n",
        "# checkpoint = torch.load('/content/MyDrive/MyDrive/model/KoElectra/koelectra_base_all_v2.tar') # dict 불러오기\n",
        "# model1.load_state_dict(checkpoint['model'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "\n",
        "# model2 (MobileNetv2)\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "\n",
        "model2 = models.mobilenet_v2(pretrained=False)\n",
        "model2.classifier[-1] = nn.Linear(model2.last_channel, num_classes)\n",
        "model2 = model2.to(device)\n",
        "model2.load_state_dict(torch.load('/content/MyDrive/MyDrive/model/MobileNetv2/[ver.3] fine_tuned_mobilenetv2.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eLGFEZFTzsD",
        "outputId": "9842b38e-8cb9-4209-de47-e81c017d7d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class EnsembleModel(nn.Module):\n",
        "  def __init__(self, model1, model2):\n",
        "    super(EnsembleModel, self).__init__()\n",
        "    self.model1 = model1\n",
        "    self.model2 = model2\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, image):\n",
        "    output1 = self.model1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=None).logits[0]\n",
        "    output2 = self.model2(image)\n",
        "\n",
        "    # output1, output2 인덱스 맞추기\n",
        "\n",
        "\n",
        "\n",
        "    # softmax\n",
        "    # output1 = F.softmax(output1, dim=1)\n",
        "    # output2 = F.softmax(output2, dim=1)\n",
        "\n",
        "\n",
        "    # softmax\n",
        "    # output1 = F.softmax(output1, dim=1)\n",
        "    # output2 = F.softmax(output2, dim=1)\n",
        "\n",
        "    # # 평균\n",
        "    # weighted_output = output1 * 2 + output2\n",
        "    # weighted_output = F.softmax(weighted_output, dim=1)\n",
        "\n",
        "    # top3 = torch.topk(weighted_output.data, 3, dim=1)\n",
        "\n",
        "    # output_idx = top3.indices.to('cpu').numpy()\n",
        "    # output_score = top3.values.to('cpu').numpy()\n",
        "\n",
        "    # NUM2LABEL = {v:k for k,v in LABEL2NUM.items()}\n",
        "\n",
        "    # pred_cat_id = [NUM2LABEL[idx] for idx in output_idx[0]]\n",
        "    # pred_cat = [LABEL2NAME[idx] for idx in pred_cat_id]\n",
        "\n",
        "    # result = list(zip(pred_cat, output_score[0]))\n",
        "\n",
        "    return output1, output2"
      ],
      "metadata": {
        "id": "JnfWpUYU7wKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from transformers import ElectraForSequenceClassification, AutoTokenizer, AdamW, ElectraTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "    ToTensorV2()\n",
        "    ])\n",
        "\n",
        "class EnsembleDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 sentence,\n",
        "                 image,\n",
        "                 tokenizer,\n",
        "                 pad_sequences,\n",
        "                 transforms=transform):\n",
        "        self.sentence = sentence\n",
        "        self.image = image\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_sequences = pad_sequences\n",
        "        self.transforms = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # text\n",
        "        tokenized_texts = [self.tokenizer.tokenize(sent) for sent in self.sentence]\n",
        "        input_ids = [self.tokenizer.convert_tokens_to_ids(text) for text in tokenized_texts]\n",
        "        input_ids = pad_sequences(input_ids, maxlen=200, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "        attention_masks = []\n",
        "        for seq in input_ids:\n",
        "            seq_mask = [float(i>0) for i in seq]\n",
        "            attention_masks.append(seq_mask)\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "        # image\n",
        "        image = self.transforms(image=self.image)['image']\n",
        "\n",
        "        return input_ids, attention_masks, image"
      ],
      "metadata": {
        "id": "gaQE-KOcVr3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "img_path = '/content/226536015-310260800 (2).jpg'\n",
        "image = cv2.imread(img_path)\n",
        "data_ensemble = EnsembleDataset('남성 티셔츠',\n",
        "                                image,\n",
        "                                tokenizer,\n",
        "                                transform)"
      ],
      "metadata": {
        "id": "FPROrZVA5tJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EnsembleModel(model1, model2)\n",
        "input_ids, attention_mask, image = data_ensemble[0]\n",
        "# input_ids = input_ids.unsqueeze(dim=0)\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# attention_mask = attention_mask.unsqueeze(dim=0)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "image = image.unsqueeze(dim=0)\n",
        "image = image.to(device)\n",
        "\n",
        "output1, output2 = model(input_ids, attention_mask, image)"
      ],
      "metadata": {
        "id": "oI-51iij6liT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUWmNeuUHb00",
        "outputId": "065db195-22e8-40eb-b013-46ef6adf0f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.0828, -0.9953, -0.9132, -0.2593, -0.1939, -0.5002, -1.0927, -0.3941,\n",
              "        -1.2219, -2.2354,  0.6902, -0.3010, -1.0060, -0.7934, -1.4921, -1.8395,\n",
              "        -1.0230, -1.6708, -0.4499, -1.0643, -0.1490, -0.3984, -0.0308, -2.1803,\n",
              "        -0.4844, -0.5499, -1.1298, -1.1466, -1.7706, -0.4329, -0.7700, -0.0682,\n",
              "        -0.4075, -0.6836, -0.2225, -0.3464,  0.6809,  0.5287, -0.3659, -0.9706,\n",
              "        -0.5675, -1.2748, -0.0145, -0.7328, -1.1290, -0.5504, -0.7356, -2.5615,\n",
              "        -1.8915, -0.4280, -2.1433, -0.7571, -1.1695, -0.4074, -0.7089,  0.1089,\n",
              "        -1.4757, -0.5860, -0.9991, -1.6523, -1.6912, -0.0895, -1.1458, -1.3172,\n",
              "        -0.9424, -2.4284, -0.9287, -1.0079, -1.4982, -0.9184, -1.3307, -2.8832,\n",
              "        -1.1270, -1.1238, -3.1124, -0.9450, -1.8930, -0.7134, -1.5750, -2.3417,\n",
              "        -1.3688, -0.8505, -1.2927, -0.2661, -2.0371, -1.0117, -2.0582, -2.1706,\n",
              "        -1.4536, -0.7295, -0.7845, -1.0218, -0.1746, -0.1191, -0.5968, -0.3965,\n",
              "        -0.8253, -0.0692, -0.4131, -1.7612, -3.1590, -2.8355, -2.9129, -2.8704,\n",
              "        -2.7073, -1.4086, -2.8135, -2.1160, -2.2196, -2.5464, -1.4887, -2.4500,\n",
              "        -3.5263, -2.2010, -2.5069, -2.0251, -2.5196, -2.4808, -2.4472, -1.8758,\n",
              "        -2.5913, -2.5479, -1.8422, -1.9393, -1.9080, -2.5705, -2.1184, -2.2653,\n",
              "        -1.8481, -2.9606, -1.7606, -2.4619, -2.3282, -3.1473, -2.5098, -3.1823,\n",
              "        -1.4775, -1.3474], device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iR8_373hyDT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(output2, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0fZ57OyHRnA",
        "outputId": "de0551d1-6fbd-4b2b-a7cd-c9d33e0ca4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.8816e-02, 7.9025e-03, 7.8597e-03, 7.9348e-02, 8.8193e-03, 1.0585e-02,\n",
              "         8.7959e-03, 6.0062e-03, 7.3030e-03, 9.5024e-03, 3.3561e-02, 2.7906e-03,\n",
              "         1.2407e-02, 4.8886e-03, 2.1143e-02, 2.1496e-03, 9.2525e-03, 1.6173e-02,\n",
              "         5.3586e-03, 6.5975e-03, 1.6327e-03, 9.5764e-05, 1.1286e-03, 2.7621e-04,\n",
              "         1.0185e-03, 5.4562e-04, 1.1427e-02, 6.3195e-02, 3.9720e-02, 2.8379e-03,\n",
              "         1.3455e-03, 7.9611e-03, 3.2277e-02, 2.4350e-03, 1.3674e-02, 1.9013e-02,\n",
              "         1.0717e-02, 4.9132e-02, 1.7770e-02, 1.9124e-02, 1.3127e-02, 3.6663e-03,\n",
              "         5.2919e-03, 1.8363e-02, 3.5075e-03, 5.2624e-03, 4.9362e-03, 1.5747e-02,\n",
              "         5.3082e-03, 1.1445e-02, 2.5380e-03, 1.6398e-02, 1.2286e-02, 3.6855e-02,\n",
              "         1.3929e-02, 1.1960e-03, 6.9382e-03, 8.8888e-03, 1.0694e-02, 3.6127e-03,\n",
              "         8.4411e-04, 2.1907e-03, 1.9692e-03, 1.2156e-03, 6.5964e-04, 1.9336e-03,\n",
              "         7.6373e-04, 5.9844e-04, 1.4781e-03, 7.0512e-05, 1.9808e-03, 1.1968e-03,\n",
              "         7.6578e-04, 5.1077e-03, 7.0887e-03, 1.1150e-02, 1.0532e-02, 1.1692e-03,\n",
              "         3.3157e-03, 8.4955e-03, 6.4826e-03, 1.6901e-03, 7.9019e-04, 1.1480e-03,\n",
              "         2.3478e-04, 1.6368e-03, 2.4652e-05, 2.6399e-04, 3.5294e-04, 1.6833e-03,\n",
              "         1.4762e-03, 3.1141e-04, 7.6658e-04, 4.8095e-04, 1.1525e-03, 1.7167e-03,\n",
              "         1.4855e-03, 8.9048e-04, 2.3292e-04, 9.0984e-04, 8.4352e-04, 1.5129e-03,\n",
              "         4.3098e-04, 3.7642e-04, 5.3974e-04, 1.7092e-03, 5.0235e-04, 6.3575e-03,\n",
              "         8.4341e-03, 2.7970e-03, 9.4293e-04, 5.9562e-04, 2.9199e-03, 9.7739e-04,\n",
              "         1.0783e-03, 7.8142e-04, 8.1760e-05, 2.5444e-03, 5.4923e-04, 4.8570e-04,\n",
              "         1.4009e-03, 8.2476e-03, 3.5750e-04, 2.5230e-03, 7.1480e-04, 1.5758e-03,\n",
              "         1.3978e-04, 4.9492e-04, 8.7607e-03, 8.7843e-03, 2.6865e-02, 5.1363e-03,\n",
              "         5.5806e-03, 1.1317e-02, 4.0850e-03, 4.5655e-03, 2.7718e-03, 1.3862e-03]],\n",
              "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSONTIaDEJ_f",
        "outputId": "11ee0a10-910f-438c-dae9-0480f63c448a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output1.logits[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6RKA2P89olo",
        "outputId": "856ae259-39fc-49d1-98c5-1149b0da6787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([414])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPeH6bXq_Nq0",
        "outputId": "ef0d80bc-d07f-473b-c1ea-d368cd144cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.6732e-02, 6.2364e-03, 5.8172e-03, 8.0215e-02, 1.4443e-02, 7.6287e-03,\n",
              "         7.4251e-03, 5.8567e-03, 6.1188e-03, 8.3940e-03, 4.3248e-02, 2.4283e-03,\n",
              "         6.2075e-03, 3.3859e-03, 1.7734e-02, 1.9480e-03, 2.0596e-02, 1.6324e-02,\n",
              "         9.7727e-03, 6.4439e-03, 8.6067e-04, 7.9305e-05, 1.1644e-03, 5.3526e-04,\n",
              "         1.6066e-03, 5.5657e-04, 9.2797e-03, 3.9383e-02, 6.0510e-02, 7.8246e-03,\n",
              "         1.0770e-03, 1.7898e-02, 6.0296e-02, 3.4234e-03, 1.1784e-02, 1.2095e-02,\n",
              "         9.7992e-03, 3.0542e-02, 2.6180e-02, 1.2109e-02, 9.2517e-03, 2.2641e-03,\n",
              "         4.2279e-03, 1.8747e-02, 2.4936e-03, 3.6257e-03, 2.4649e-03, 1.0351e-02,\n",
              "         5.9837e-03, 1.3098e-02, 2.1322e-03, 1.3674e-02, 1.9095e-02, 2.6783e-02,\n",
              "         1.7892e-02, 3.4489e-03, 1.3052e-02, 9.2635e-03, 8.2870e-03, 2.1857e-03,\n",
              "         6.4185e-04, 8.6171e-04, 1.9220e-03, 4.7111e-04, 6.1966e-04, 5.9691e-04,\n",
              "         9.5317e-04, 3.3003e-04, 7.6184e-04, 3.9365e-05, 1.9444e-03, 1.6068e-03,\n",
              "         3.0103e-04, 3.5364e-03, 4.8588e-03, 1.0836e-02, 8.5217e-03, 1.1013e-03,\n",
              "         2.7429e-03, 7.8414e-03, 6.2135e-03, 5.9863e-04, 4.9391e-04, 1.9768e-03,\n",
              "         2.0016e-04, 1.4038e-03, 4.8121e-05, 3.3049e-04, 3.9643e-04, 3.9551e-03,\n",
              "         2.1809e-03, 4.0011e-04, 5.4378e-04, 3.3399e-04, 2.3629e-03, 8.6021e-04,\n",
              "         2.1667e-03, 7.6118e-04, 4.3515e-04, 6.1632e-04, 5.2248e-04, 2.2256e-03,\n",
              "         4.3497e-04, 1.9934e-04, 7.5412e-04, 1.3968e-03, 6.4695e-04, 4.8168e-03,\n",
              "         8.7645e-03, 1.6673e-03, 1.1439e-03, 8.4375e-04, 1.3581e-03, 7.7805e-04,\n",
              "         7.9030e-04, 6.3436e-04, 3.1378e-05, 2.1588e-03, 6.3354e-04, 6.4699e-04,\n",
              "         1.3138e-03, 4.8241e-03, 4.0870e-04, 3.8268e-03, 9.6669e-04, 2.1526e-03,\n",
              "         5.7231e-05, 3.4224e-04, 1.1240e-02, 1.6158e-02, 2.5432e-02, 7.4660e-03,\n",
              "         6.1647e-03, 1.7102e-02, 2.8350e-03, 4.1390e-03, 2.1970e-03, 3.8812e-03]],\n",
              "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GsPrpzd_Pce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}