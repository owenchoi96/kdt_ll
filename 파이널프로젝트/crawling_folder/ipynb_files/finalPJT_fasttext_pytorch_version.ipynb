{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 사전작업"
      ],
      "metadata": {
        "id": "PEfbaqUPMchg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # -- mecab 설치 --\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "!pip3 install mecab-python3\n",
        "!pip install konlpy\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IyQVSKVXkeLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install portalocker>=2.0.0"
      ],
      "metadata": {
        "id": "zwjqbmR-Wwev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def bring_and_preprocess_df():\n",
        "    df = pd.read_csv('/content/drive/MyDrive/bungae_image_df.csv', encoding='utf-8-sig')\n",
        "    df = df.dropna(axis=0)\n",
        "    df = df.reset_index(drop=True)\n",
        "    df['cat_id'] = df['cat_id'].astype(int).astype(str)\n",
        "    return df\n",
        "\n",
        "df = bring_and_preprocess_df()\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuOadhUcMcBW",
        "outputId": "c7bef2cb-1104-490d-d095-7495a4e73b7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1391022, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pytorch code"
      ],
      "metadata": {
        "id": "G6Chrl3jMeUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5gq78Jdh6J2E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "import transformers\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bigrams(x):\n",
        "    \"\"\"\n",
        "    bi-gram 생성 함수\n",
        "    \"\"\"\n",
        "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "    for n_gram in n_grams:\n",
        "        x.append(' '.join(n_gram))\n",
        "    return x\n",
        "\n",
        "def remove_punct(text):\n",
        "    \"\"\"\n",
        "    문장기호 없애는 함수\n",
        "    \"\"\"\n",
        "    remove_punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
        "    text = text.lower().translate(remove_punct_dict)\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizer1(text):\n",
        "    \"\"\"\n",
        "    data.Field에 사용될 tokenizer 함수\n",
        "    \"\"\"\n",
        "    mecab = Mecab()\n",
        "    text = remove_punct(text)\n",
        "    tokens = mecab.nouns(text)\n",
        "    tokens = [token for token in tokens if len(token) >1]\n",
        "    return tokens\n",
        "\n",
        "# -- koelectra tokenizer --\n",
        "model_path = 'monologg/koelectra-base-v3-discriminator'\n",
        "tokenizer2 = transformers.AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "trLIGOTJ6KYR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- CustomDataset 베이스 코드 --\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, text, label, tokenizer=None, preprocessing=None):\n",
        "        # -- 데이터셋의 전처리를 해주는 부분 --\n",
        "        self.text = text\n",
        "        self.label = label # 여기 부분 어떻게?\n",
        "        self.tokenizer = tokenizer\n",
        "        self.preprocessing = generate_bigrams\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # -- 데이터셋에서 특정 1개의 샘플을 가져오는 함수 -- \n",
        "        text = self.text[index]\n",
        "        \n",
        "        label = self.label[index]\n",
        "        \n",
        "        if self.tokenizer is not None:\n",
        "            tokens = self.tokenizer(text)\n",
        "        \n",
        "        if self.preprocessing is not None:\n",
        "            data = self.preprocessing(tokens)\n",
        "\n",
        "        return data, label\n",
        "        \n",
        "    def __len__(self):\n",
        "        # -- 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분 --\n",
        "        return len(self.text)"
      ],
      "metadata": {
        "id": "quEo6vQn6KdP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "7xdGLuMJKPTp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- train_df와 test_df 나누기 --\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['product_name'], df['cat_id'].values, shuffle=True, test_size=.2)\n",
        "train_df = df.iloc[x_train.index]\n",
        "test_df = df.iloc[x_test.index]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "BwwvmGG-gi6S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- train_df와 test_df 이전에 나누어주기 -- \n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class BungaeDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer=None, preprocessing=None):\n",
        "        # -- 데이터셋의 전처리를 해주는 부분 --\n",
        "        self.data = df['product_name']\n",
        "        self.labels = df['cat_id']\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.preprocessing = generate_bigrams\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # -- 데이터셋에서 특정 1개의 샘플을 가져오는 함수 -- \n",
        "        data = self.data[index]\n",
        "        label = self.labels[index]\n",
        "    \n",
        "        tokens = self.tokenizer(data, \n",
        "                                # padding=True, truncation=True, max_length=32\n",
        "                                )\n",
        "        data = self.preprocessing(tokens)\n",
        "\n",
        "        return data, label\n",
        "        \n",
        "    def __len__(self):\n",
        "        # -- 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분 --\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)\n",
        "    padded_inputs = pad_sequence(inputs, batch_first=True)\n",
        "    return padded_inputs, torch.tensor(labels)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = BungaeDataset(train_df, tokenizer, generate_bigrams)\n",
        "test_dataset = BungaeDataset(test_df, tokenizer, generate_bigrams)\n",
        "\n",
        "# dataloader\n",
        "\n",
        "batch_size = 64\n",
        "shuffle = True\n",
        "num_workers = 2\n",
        "drop_last=True\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                         num_workers=num_workers, drop_last=drop_last,\n",
        "                         collate_fn = collate_fn\n",
        "                          )\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                         num_workers=num_workers, drop_last=drop_last,\n",
        "                         collate_fn = collate_fn\n",
        "                         )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rOIpg8Ko6Kfv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with ChatGPT"
      ],
      "metadata": {
        "id": "Rg_TqkEB6Kp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "def generate_bigrams(x):\n",
        "    \"\"\"\n",
        "    bi-gram 생성 함수\n",
        "    \"\"\"\n",
        "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "    for n_gram in n_grams:\n",
        "        x.append(' '.join(n_gram))\n",
        "    return x\n",
        "\n",
        "def remove_punct(text):\n",
        "    \"\"\"\n",
        "    문장기호 없애는 함수\n",
        "    \"\"\"\n",
        "    remove_punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
        "    text = text.lower().translate(remove_punct_dict)\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    \"\"\"\n",
        "    data.Field에 사용될 tokenizer 함수\n",
        "    \"\"\"\n",
        "    mecab = Mecab()\n",
        "    text = remove_punct(text)\n",
        "    tokens = mecab.nouns(text)\n",
        "    tokens = [token for token in tokens if len(token) >1]\n",
        "    return tokens\n",
        "\n",
        "# -- koelectra tokenizer --\n",
        "model_path = 'monologg/koelectra-base-v3-discriminator'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "vSlUDx_gg5o0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "\n",
        "data = list(df['product_name'])\n",
        "# -- tokenizing --\n",
        "tokenized_data = [tokenizer(text) for text in data]\n",
        "flattened_data = [word for text in tokenized_data for word in text]\n",
        "\n",
        "# -- build the vocabulary -- \n",
        "vocabulary = torchtext.vocab.build_vocab_from_iterator([flattened_data])"
      ],
      "metadata": {
        "id": "_uYb91V9Rslm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, tokenizer, preprocessing):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index]\n",
        "        label = self.labels[index]\n",
        "        tokens = self.tokenizer(data, \n",
        "                                # padding=True, truncation=True, max_length=32\n",
        "                                )\n",
        "        data = self.preprocessing(tokens)\n",
        "\n",
        "        sample = {\n",
        "            'data': data,\n",
        "            'label': label\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have your own data and labels\n",
        "\n",
        "# Create the train dataset\n",
        "train_data = list(train_df['product_name'])\n",
        "train_labels = list(train_df['cat_id'])\n",
        "\n",
        "train_data = [torch.tensor(d) for d in train_data]\n",
        "train_labels = [torch.tensor(l) for l in train_labels]\n",
        "\n",
        "train_dataset = CustomDataset(train_data, train_labels, tokenizer, generate_bigrams)\n",
        "\n",
        "# Create the test dataset\n",
        "test_data = list(test_df['product_name'])\n",
        "test_labels = list(test_df['cat_id'])\n",
        "\n",
        "test_data = [torch.tensor(d) for d in test_data]\n",
        "test_labels = [torch.tensor(l) for l in test_labels]\n",
        "\n",
        "test_dataset = CustomDataset(test_data, test_labels, tokenizer, generate_bigrams)\n"
      ],
      "metadata": {
        "id": "-oYg_xbU6KsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchtext\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the pre-trained embeddings\n",
        "embedding_file = '/content/drive/MyDrive/wiki.ko.vec'\n",
        "embeddings = torchtext.vocab.Vectors(embedding_file)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        fields = [('data', TEXT), ('labels', LABEL)]\n",
        "        examples = [Example.fromlist([d, l], fields) for d, l in zip(data, labels)]\n",
        "        super().__init__(examples, fields)\n",
        "\n",
        "# Assuming you have your own dataset\n",
        "train_data = list(train_df['product_name'])\n",
        "train_labels = list(train_df['cat_id'])\n",
        "\n",
        "# Create the vocabulary\n",
        "TEXT.build_vocab(train_data, vectors=Vectors(embedding_file))\n",
        "# Convert data and labels to tensors\n",
        "train_data = [TEXT.process([d]).data.squeeze() for d in train_data]\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "# Create the dataset instance\n",
        "dataset = MyDataset(train_data, train_labels)\n",
        "\n",
        "\n",
        "# Create the data loader\n",
        "batch_size = 64\n",
        "shuffle = True\n",
        "num_workers = 2\n",
        "drop_last=True\n",
        "\n",
        "# -- collate_fn -- \n",
        "def collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)\n",
        "    padded_inputs = pad_sequence(inputs, batch_first=True)\n",
        "    return padded_inputs, torch.tensor(labels)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                         num_workers=num_workers, drop_last=drop_last,\n",
        "                         collate_fn = collate_fn)\n",
        "# test dataloader\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "#                          num_workers=num_workers, drop_last=drop_last,\n",
        "#                          collate_fn = collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "rvhOiHa66Ku1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchtext\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the pre-trained embeddings\n",
        "embedding_file = '/content/drive/MyDrive/wiki.ko.vec'\n",
        "embeddings = torchtext.vocab.Vectors(embedding_file)\n",
        "\n",
        "embeddings_tensor = torch.Tensor(embeddings.vectors)\n",
        "embedding_dim = embeddings.dim\n",
        "\n",
        "\n",
        "class FastText(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes, embeddings):\n",
        "        super(FastText, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # shape: (batch_size, sequence_length, embedding_dim)\n",
        "        embedded = embedded.permute(1, 0, 2) \n",
        "        pooled = F.avg_pool2d(embedded, (embedded.shape[1],1)).squeeze(1) \n",
        "        return self.fc(pooled)\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have your own dataset and vocabulary\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(embeddings.itos)\n",
        "embedding_dim = 300\n",
        "num_classes = len(df['cat_id'].unique())\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Create model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FastText(vocab_size, embedding_dim, num_classes, embeddings_tensor).to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_inputs, batch_labels in train_loader:\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        acc = accuracy_score(predicted.cpu(), batch_labels.cpu())\n",
        "        print(f\"Accuracy: {acc}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_inputs, batch_labels in test_loader:\n",
        "            # move data to GPU\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_inputs)\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "        \n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "PxGFZng0Zkw_",
        "outputId": "30fecd8a-92ae-4ed3-cb50-e5a8bdd83341"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0846f2a5bdbb>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-10-912546657764>\", line 33, in collate_fn\n    padded_inputs = pad_sequence(inputs, batch_first=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/utils/rnn.py\", line 399, in pad_sequence\n    return torch._C._nn.pad_sequence(sequences, batch_first, padding_value)\nTypeError: expected Tensor as element 0 in argument 0, but got BatchEncoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyTHj_IymiSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}