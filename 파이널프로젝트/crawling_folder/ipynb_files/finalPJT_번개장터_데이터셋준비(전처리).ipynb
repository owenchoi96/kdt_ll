{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8016bf8b",
   "metadata": {},
   "source": [
    "***번개장터 데이터 가져오기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b252a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def concat_bungae_files():\n",
    "    base_df = pd.DataFrame()\n",
    "    for idx in range(9):\n",
    "        df = pd.read_csv(f'./bungae_df_{idx}_fashion.csv')\n",
    "        df = df.dropna(axis=0)\n",
    "        df['cat_id'] = df['cat_id'].astype(int).astype(str)\n",
    "        # -- 필요한 컬럼만 가져오기 -- \n",
    "        df = df[['product_id', 'product_name', 'cat_id']].copy()\n",
    "        # -- concat 할 때마다 base_df 업데이트 -- \n",
    "        base_df = pd.concat([base_df, df], ignore_index=True)\n",
    "\n",
    "    # -- base_df 드라이브에 저장해놓기 --\n",
    "    # base_df.to_csv('/content/drive/MyDrive/bungae_base_df_fashion.csv', index=False)\n",
    "\n",
    "    return base_df\n",
    "\n",
    "\n",
    "sample_df = concat_bungae_files()\n",
    "sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eba328",
   "metadata": {},
   "source": [
    "# 베이스 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 예시로 여성 패딩 카테고리만 가져와보기 --\n",
    "# 310090050 롱패딩\n",
    "# 310090060 숏패딩\n",
    "# 310090020 블루종/항공점퍼\n",
    "df = sample_df[sample_df['cat_id'].isin(['310090050', '310090060', '310090020'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5af6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 전처리 --\n",
    "# -- 기호 없애기 --\n",
    "text = 'bcbg 정품 오리털패딩(택포)'\n",
    "import string\n",
    "def remove_punct(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    text = text.lower().translate(remove_punct_dict)\n",
    "    return text\n",
    "\n",
    "df['product_name'] = df['product_name'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd69d2c",
   "metadata": {},
   "source": [
    "***Word2Vec, FastText 활용하지 않기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8987323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"활용 X\"\"\"\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models.fasttext import FastText\n",
    "# from konlpy.tag import Mecab\n",
    "# import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ed3bc",
   "metadata": {},
   "source": [
    "**숏패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21225c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "short = df[df['cat_id'] == '310090060'].copy()\n",
    "short = short.reset_index(drop=True).copy()\n",
    "short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_filtered = short[~short['product_name'].str.contains('롱|항공|블루종')].copy()\n",
    "short_filtered = short_filtered['product_name'][short_filtered['product_name'].str.contains('숏')].copy()\n",
    "print(len(short_filtered))\n",
    "short_filtered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1d23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6711e51",
   "metadata": {},
   "source": [
    "**롱패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "long = df[df['cat_id'] == '310090050'].copy()\n",
    "long = long.reset_index(drop=True).copy()\n",
    "long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_filtered = long[~long['product_name'].str.contains('숏|항공|블루종')].copy()\n",
    "long_filtered = long_filtered['product_name'][long_filtered['product_name'].str.contains('롱')].copy()\n",
    "print(len(long_filtered))\n",
    "long_filtered.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452aec2d",
   "metadata": {},
   "source": [
    "**항공점퍼|블루종**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ce889",
   "metadata": {},
   "outputs": [],
   "source": [
    "hangong = df[df['cat_id'] == '310090020'].copy()\n",
    "hangong = hangong.reset_index(drop=True).copy()\n",
    "hangong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76af56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hangong_filtered = hangong[~hangong['product_name'].str.contains('숏|롱')].copy()\n",
    "hangong_filtered = hangong_filtered['product_name'][hangong_filtered['product_name'].str.contains('항공|블루종')].copy()\n",
    "print(len(hangong_filtered))\n",
    "hangong_filtered.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"-- 여기까지 여성의류 > 패딩 > 숏패딩, 롱패딩, 항공점퍼|블루종 예시 --\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7def7",
   "metadata": {},
   "source": [
    "# 전체 카테고리에 대한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc75c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "def concat_bungae_files():\n",
    "    base_df = pd.DataFrame()\n",
    "    for idx in range(9):\n",
    "        df = pd.read_csv(f'./bungae_df_{idx}_fashion.csv')\n",
    "        df = df.dropna(axis=0)\n",
    "        df['cat_id'] = df['cat_id'].astype(int).astype(str)\n",
    "        # -- 필요한 컬럼만 가져오기 -- \n",
    "        df = df[['product_id', 'product_name', 'cat_id']].copy()\n",
    "        # -- concat 할 때마다 base_df 업데이트 -- \n",
    "        base_df = pd.concat([base_df, df], ignore_index=True)\n",
    "\n",
    "    # -- base_df 드라이브에 저장해놓기 --\n",
    "    # base_df.to_csv('/content/drive/MyDrive/bungae_base_df_fashion.csv', index=False)\n",
    "\n",
    "    return base_df\n",
    "\n",
    "# -- 전처리 --\n",
    "# -- 기호 없애기 --\n",
    "def remove_punct(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    text = text.lower().translate(remove_punct_dict)\n",
    "    return text\n",
    "\n",
    "def bring_fashion_cat_csv():\n",
    "    \"\"\"\n",
    "    번개장터 카테고리 id와 카테고리 이름이 들어있는\n",
    "    파일 가져오는 함수\n",
    "    \"\"\"\n",
    "    final_cat = pd.read_csv('./final_category.csv')\n",
    "    final_cat = final_cat.drop('Unnamed: 0', axis=1).copy()\n",
    "    final_cat['cat_id'] = final_cat['cat_id'].astype(str)\n",
    "    \n",
    "    return final_cat\n",
    "\n",
    "def get_cat_name(cat_id:str) -> str:\n",
    "    \"\"\"\n",
    "    카테고리 id를 입력하면 카테고리 이름을 반환하는 함수 => 중분류에 대해서\n",
    "    \"\"\"\n",
    "    final_cat = bring_fashion_cat_csv()\n",
    "    if len(cat_id) == 6: # 중분류\n",
    "        return list(final_cat[final_cat['cat_id'] == cat_id]['cat2'])[0]\n",
    "    elif len(cat_id) == 9: # 대분류\n",
    "        return list(final_cat[final_cat['cat_id'] == cat_id]['cat3'])[0]\n",
    "\n",
    "# -- subs 가 있는 것들만 dictionary로 뽑아내기 --\n",
    "def make_category_dict_with_subs() -> dict:\n",
    "    \n",
    "    with open('./bgzt_fashion_category_nums.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    category_dict_with_subs = dict()\n",
    "    for main, mids in data.items():\n",
    "        for mid, subs in mids.items():\n",
    "            d = defaultdict()\n",
    "            if subs != [None]: # 하위 sub들이 있을 때 \n",
    "                d[mid] = subs\n",
    "            category_dict_with_subs.update(d)\n",
    "    return category_dict_with_subs\n",
    "\n",
    "\n",
    "# -- 중분류 내에서의 소분류 morphs 키워드 가져오는 함수 --\n",
    "def make_subcat_morphs_by_midcat(cat_dict:dict) -> dict:\n",
    "    \"\"\"\n",
    "    make_category_dict_with_subs 함수로 부터 받은 딕션어리로\n",
    "    midcat의 하위 subcat들의 morphs 키워드 가져오는 함수 \n",
    "    \"\"\"\n",
    "    mecab = Mecab()\n",
    "    d = defaultdict(list)\n",
    "\n",
    "    for mid, subs in cat_dict.items():\n",
    "        tmp_dict = dict()\n",
    "        for sub in subs:\n",
    "\n",
    "            mid_morph = mecab.morphs(get_cat_name(mid))\n",
    "            sub_morph = mecab.morphs(remove_punct(get_cat_name(sub)))\n",
    "            unique_sub_morph = set(sub_morph) - set(mid_morph)\n",
    "            unique_sub_morph = list(unique_sub_morph)\n",
    "            if unique_sub_morph == []:\n",
    "                d[mid] += sub_morph\n",
    "            d[mid] += unique_sub_morph\n",
    "\n",
    "    return dict(d)\n",
    "\n",
    "# -- 위의 두 함수 합치기 --\n",
    "def get_dict_from_midcat_with_subs():\n",
    "    cat_dict = make_category_dict_with_subs()\n",
    "    morphs_dict = make_subcat_morphs_by_midcat(cat_dict)\n",
    "    return morphs_dict\n",
    "\n",
    "\n",
    "# -- 다음으로는 mid만 있는 것들만 뽑아내기 --\n",
    "def make_category_lists_without_subs():\n",
    "    \"\"\"\n",
    "    소분류 카테고리가 없는 중분류 리스트 => cat_ids_without_subs\n",
    "    \"\"\"\n",
    "    # -- 패션 cat_id 들어있는 json 파일 가져오기 --\n",
    "    with open('./bgzt_fashion_category_nums.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    cat_list_without_subs = list()\n",
    "    morphs_list = list()\n",
    "    for main, mids in data.items():\n",
    "        for mid in mids:\n",
    "            if mids[mid] == [None]:\n",
    "                morphs_list += mecab.morphs(remove_punct(get_cat_name(mid)))\n",
    "                cat_list_without_subs.append(mid)\n",
    "    \n",
    "    morphs_list = list(set(morphs_list))\n",
    "    \n",
    "    return morphs_list, cat_list_without_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce9940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -- 카테고리 번호가 9인, 즉 sub 카테고리가 있는 상품들에 대해 \n",
    "def make_df_for_nine_nums_cat_ids():\n",
    "    sample_df = concat_bungae_files()\n",
    "    morphs_dict_with_subs = get_dict_from_midcat_with_subs()\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    for cat_id in list(sample_df['cat_id'].unique()):\n",
    "        if len(cat_id) == 9:\n",
    "            try:\n",
    "                tmp_df = sample_df[sample_df['cat_id'] == cat_id]\n",
    "                cat_id_morphs = mecab.morphs(remove_punct(get_cat_name(cat_id)))\n",
    "                mid_id_name = mecab.morphs(remove_punct(get_cat_name(cat_id[:6])))\n",
    "                midcat_morphs = morphs_dict_with_subs[cat_id[:6]]\n",
    "\n",
    "                cat_id_morphs = [morph for morph in cat_id_morphs if morph in midcat_morphs]\n",
    "                other_morphs = [morph for morph in midcat_morphs if morph not in cat_id_morphs]\n",
    "                opposite_keywords = '|'.join(other_morphs)\n",
    "\n",
    "                tmp_df_filtered = tmp_df[~tmp_df['product_name'].str.contains(opposite_keywords)]\n",
    "\n",
    "                if '기타' in cat_id_morphs:\n",
    "                    df = tmp_df_filtered\n",
    "                else:\n",
    "                    if '긴팔' in cat_id_morphs:\n",
    "                        cat_id_morphs += ['롱']\n",
    "                    elif '반팔' in cat_id_morphs:\n",
    "                        cat_id_morphs += ['숏']    \n",
    "                    else:\n",
    "                        cat_id_morphs = cat_id_morphs\n",
    "                    df = tmp_df_filtered[tmp_df_filtered['product_name'].str.contains('|'.join(cat_id_morphs))]\n",
    "                    \n",
    "                final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "def make_df_for_six_num_ids():\n",
    "    morphs_list_without_subs, cat_list_without_subs = make_category_lists_without_subs()\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    for cat_id in list(sample_df['cat_id'].unique()):\n",
    "        if cat_id in cat_list_without_subs:\n",
    "            \n",
    "            tmp_df = sample_df[sample_df['cat_id'] == cat_id].copy()\n",
    "            cat_id_morphs = mecab.morphs(remove_punct(get_cat_name(cat_id)))\n",
    "\n",
    "            mid_id_name = mecab.morphs(remove_punct(get_cat_name(cat_id[:6])))\n",
    "            other_morphs = [morph for morph in morphs_list_without_subs if morph not in cat_id_morphs]\n",
    "            opposite_keywords = '|'.join(other_morphs)\n",
    "\n",
    "            tmp_df_filtered = tmp_df[~tmp_df['product_name'].str.contains(opposite_keywords)].copy()\n",
    "\n",
    "            if '기타' in cat_id_morphs:\n",
    "                df = tmp_df_filtered\n",
    "            else:\n",
    "                df = tmp_df_filtered[tmp_df_filtered['product_name'].str.contains('|'.join(cat_id_morphs))].copy()\n",
    "                \n",
    "            final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def concat_two_df(): \n",
    "    final_df1 = make_df_for_nine_nums_cat_ids()\n",
    "    final_df2 = make_df_for_six_num_ids()\n",
    "    df = pd.concat([final_df1, final_df2], ignore_index=True)\n",
    "    return df # 대략 반개 줄어들음 \n",
    "\n",
    "final_df = concat_two_df()\n",
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "final_df['product_name_length'] = final_df['product_name'].apply(lambda x : len(x))\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d059737",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(final_df['product_name_length'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['cat_id'].value_counts().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_count = final_df['cat_id'].value_counts()\n",
    "plt.bar(height = cat_id_count, x= cat_id_count.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb0ab2",
   "metadata": {},
   "source": [
    "# FastText로 진행해보기?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db630193",
   "metadata": {},
   "source": [
    "- FastText를 통해서 진행을 했을때 관련이 없는 단어들이 나오긴 했지만 <br>\n",
    "그래도 해당 단어들로 데이터들을 모았을 때 대략 (데이터가 너무 적은 카테고리를 빼고) <br>\n",
    "500~3000 사이로 들어오는 것 같음\n",
    "- 그래서 해당 방법으로 데이터를 모아보기\n",
    "- 하나의 가설로 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def get_df_by_fasttext(cat_id:str):\n",
    "    df = sample_df[sample_df['cat_id'] == cat_id]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # -- tokenizing --\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    tokens = []\n",
    "    for idx in range(len(df)):\n",
    "        token = tokenizer.morphs(df.loc[idx]['product_name'])\n",
    "        if len(token) > 1:\n",
    "            tokens.append(token)\n",
    "            \n",
    "    # -- fasttext training --       \n",
    "    for _ in range(20):\n",
    "        model2 = FastText(tokens, vector_size=100, window=10, min_count=5, workers=4, sg=0)\n",
    "        model2.build_vocab(tokens)\n",
    "        total_examples = model2.corpus_count\n",
    "        model2.train(tokens, total_examples=model2.corpus_count, epochs=10)\n",
    "        \n",
    "    # -- get most similar words -- \n",
    "    results = model2.wv.most_similar(positive=['스키니', '진'], negative=['일자', '부츠', '컷', '배기', '기타'], topn=20)\n",
    "    keywords_list = [result[0] for result in results]\n",
    "    keywords = str('|'.join(keywords_list))\n",
    "    print(keywords)\n",
    "    \n",
    "    \n",
    "    # -- get df --\n",
    "    df_filtered = df[df['product_name'].str.contains(keywords)]\n",
    "    \n",
    "    del model2\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a98ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_df_by_fasttext('310140010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in keywords_list:\n",
    "    for token in tokens:\n",
    "        if keyword in token:\n",
    "            print(keyword, token)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b6dcd",
   "metadata": {},
   "source": [
    "***전처리 후 진행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc612e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 전처리 함수 -- \n",
    "import re\n",
    "\n",
    "def remove_punct(text: str) -> str:\n",
    "    remove_punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
    "    text = text.lower().translate(remove_punct_dict)\n",
    "    return text\n",
    "\n",
    "def remove_english_and_numbers(text: str) -> str:\n",
    "    text = re.sub('[a-zㄱ-ㅎ0-9]', '', text).strip()\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocessing_product_name(product_name: str) -> str:\n",
    "    product_name = remove_punct(product_name)\n",
    "    product_name = remove_english_and_numbers(product_name)\n",
    "    return product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_df['product_name'] = fashion_df['product_name'].apply(lambda x : preprocessing_product_name(x))\n",
    "fashion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4c737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fashion_df.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9e4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_df_by_fasttext_test_version(cat_id:str, positive:list, negative:list):\n",
    "    df = fashion_df[fashion_df['cat_id'] == cat_id]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # -- tokenizing --\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    tokens = []\n",
    "    for idx in range(len(df)): # 명사인 토큰만 가져오기 \n",
    "        token = [pos[0] for pos in mecab.pos(df.loc[idx]['product_name']) if pos[1][0] == 'N']            \n",
    "        if len(token) > 1:\n",
    "            tokens.append(token)\n",
    "            \n",
    "    # -- fasttext training --       \n",
    "    for _ in range(20):\n",
    "        model2 = FastText(tokens, vector_size=100, window=10, min_count=5, workers=4, sg=0)\n",
    "        model2.build_vocab(tokens)\n",
    "        total_examples = model2.corpus_count\n",
    "        model2.train(tokens, total_examples=model2.corpus_count, epochs=10)\n",
    "        \n",
    "    # -- get most similar words -- \n",
    "    results = model2.wv.most_similar(positive=positive, negative=negative, topn=20)\n",
    "    keywords_list = [result[0] for result in results]\n",
    "    keywords = str('|'.join(keywords_list))\n",
    "    print(keywords)\n",
    "    \n",
    "    \n",
    "    # -- get df --\n",
    "    df_filtered = df[df['product_name'].str.contains(keywords)]\n",
    "    print(len(df_filtered))\n",
    "    \n",
    "    del model2\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_filtered.sample(30)\n",
    "\n",
    "get_df_by_fasttext_test_version('320140400', ['정장', '세트'], ['자켓', '베스트', '바지', '기타'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af33cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_df_by_fasttext_test_version2(cat_id:str, positive:list, negative:list):\n",
    "    df = fashion_df[fashion_df['cat_id'] == cat_id]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # -- tokenizing --\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    tokens = []\n",
    "    for idx in range(len(df)): # 명사인 토큰만 가져오기 \n",
    "        token = [pos[0] for pos in mecab.pos(df.loc[idx]['product_name']) if pos[1][0] == 'N']            \n",
    "        if len(token) > 1:\n",
    "            tokens.append(token)\n",
    "            \n",
    "    # -- model 1 training --       \n",
    "    model1 = FastText(tokens, vector_size=100, window=10, min_count=5, workers=4, sg=0)\n",
    "    model1.build_vocab(tokens)\n",
    "    total_examples1 = model1.corpus_count\n",
    "    model1.train(tokens, total_examples=total_examples1, epochs=10)\n",
    "    \n",
    "    # -- model 2 training --\n",
    "    model2 = FastText(tokens, vector_size=100, window=10, min_count=5, workers=4, sg=0)\n",
    "    model2.build_vocab(tokens)\n",
    "    total_examples2 = model2.corpus_count\n",
    "    model2.train(tokens, total_examples=total_examples2, epochs=10)\n",
    "        \n",
    "    # -- get most similar words -- \n",
    "    results1 = model1.wv.most_similar(positive=positive, topn=20)\n",
    "    results2 = model2.wv.most_similar(positive=negative, topn=20)\n",
    "    \n",
    "    keywords_list1 = [result[0] for result in results1]\n",
    "    keywords_list2 = [result[0] for result in results2]\n",
    "    \n",
    "    keywords1 = str('|'.join(keywords_list1))\n",
    "    keywords2 = str('|'.join(keywords_list2))\n",
    "    \n",
    "    print(keywords1)\n",
    "    print(keywords2)\n",
    "    \n",
    "    \n",
    "    # -- get df --\n",
    "    df1 = df[df['product_name'].str.contains(keywords1)]\n",
    "    df2 = df[df['product_name'].str.contains(keywords2)]\n",
    "    idx_list = list(set(df1.index) - set(df2.index))\n",
    "    df_filtered = df.iloc[idx_list]\n",
    "    print(len(df_filtered))\n",
    "    \n",
    "    del model1, model2\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "get_df_by_fasttext_test_version2('320110500', ['배기', '청바지'], ['일자', '스키니', '진', '부츠', '컷','기타'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_by_fasttext_test_version3(cat_id:str, positive:list, negative=None):\n",
    "    df = fashion_df[fashion_df['cat_id'] == cat_id]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # -- tokenizing --\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    tokens = []\n",
    "    for idx in range(len(df)): # 명사인 토큰만 가져오기 \n",
    "        token = [pos[0] for pos in mecab.pos(df.loc[idx]['product_name']) if pos[1][0] == 'N']            \n",
    "        if len(token) > 1:\n",
    "            tokens.append(token)\n",
    "            \n",
    "    # -- model 1 training --       \n",
    "    model1 = FastText(tokens, vector_size=100, window=10, min_count=5, workers=4, sg=0)\n",
    "    model1.build_vocab(tokens)\n",
    "    total_examples1 = model1.corpus_count\n",
    "    model1.train(tokens, total_examples=total_examples1, epochs=10)\n",
    "    \n",
    "    # -- get most similar words -- \n",
    "    results1 = model1.wv.most_similar(positive=positive, topn=20)\n",
    "    keywords_list1 = [result[0] for result in results1]\n",
    "    keywords1 = str('|'.join(keywords_list1))\n",
    "    print(keywords1)\n",
    "    \n",
    "    # -- 반대되는 데이터 프레임 --\n",
    "    if negative != None:\n",
    "        opposite_keywords = '|'.join(negative)\n",
    "        df = df[~df['product_name'].str.contains(opposite_keywords)].copy()\n",
    "    \n",
    "    # -- get df --\n",
    "    df_filtered = df[df['product_name'].str.contains(keywords1)]\n",
    "    print(len(df_filtered))\n",
    "    \n",
    "    del model1\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "get_df_by_fasttext_test_version3('310110030', ['겨울', '코트'], ['봄', '가을'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab.morphs('맨투맨')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f041e",
   "metadata": {},
   "source": [
    "# 버전 3로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834c68e",
   "metadata": {},
   "source": [
    "해당 작업 목표를 다시 상기하면 <br>\n",
    "- 데이터가 너무 많아서 모델 학습 시간이 오래 걸려 데이터 수를 줄이고자 하였고,\n",
    "- 데이터를 줄일 때 최대한 해당 카테고리별와 연관이 있는 데이터를 가져오고자 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 전처리 함수 -- \n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def remove_punct(text: str) -> str:\n",
    "    remove_punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
    "    text = text.lower().translate(remove_punct_dict)\n",
    "    return text\n",
    "\n",
    "def remove_english_and_numbers(text: str) -> str:\n",
    "    text = re.sub('[a-zㄱ-ㅎ0-9]', '', text).strip()\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_product_name(product_name: str) -> str:\n",
    "    product_name = remove_punct(product_name)\n",
    "    product_name = remove_english_and_numbers(product_name)\n",
    "    return product_name\n",
    "\n",
    "# -- 전체 데이터 프레임 가져오는 함수 --\n",
    "def concat_bungae_files():\n",
    "    base_df = pd.DataFrame()\n",
    "    for idx in range(9):\n",
    "        df = pd.read_csv(f'./bungae_df_{idx}_fashion.csv')\n",
    "        df = df.dropna(axis=0)\n",
    "        df['cat_id'] = df['cat_id'].astype(int).astype(str)\n",
    "        # -- 필요한 컬럼만 가져오기 -- \n",
    "        df = df[['product_id', 'product_name', 'cat_id']].copy()\n",
    "        # -- concat 할 때마다 base_df 업데이트 -- \n",
    "        base_df = pd.concat([base_df, df], ignore_index=True)\n",
    "\n",
    "    # -- base_df 드라이브에 저장해놓기 --\n",
    "    # base_df.to_csv('/content/drive/MyDrive/bungae_base_df_fashion.csv', index=False)\n",
    "\n",
    "    return base_df\n",
    "\n",
    "def pull_and_preprocess_base_df():\n",
    "    base_df = concat_bungae_files()\n",
    "    base_df['product_name'] = base_df['product_name'].apply(lambda x : preprocess_product_name(x))\n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939893a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bring_fashion_cat_csv():\n",
    "    \"\"\"\n",
    "    번개장터 카테고리 id와 카테고리 이름이 들어있는\n",
    "    파일 가져오는 함수\n",
    "    \"\"\"\n",
    "    final_cat = pd.read_csv('./final_category.csv')\n",
    "    final_cat = final_cat.drop('Unnamed: 0', axis=1).copy()\n",
    "    final_cat['cat_id'] = final_cat['cat_id'].astype(str)\n",
    "    \n",
    "    return final_cat\n",
    "\n",
    "def get_cat_name(cat_id:str) -> str:\n",
    "    \"\"\"\n",
    "    카테고리 id를 입력하면 카테고리 이름을 반환하는 함수 => 중분류에 대해서\n",
    "    \"\"\"\n",
    "    final_cat = bring_fashion_cat_csv()\n",
    "    if len(cat_id) == 6: # 중분류\n",
    "        return remove_punct(list(final_cat[final_cat['cat_id'] == cat_id]['cat2'])[0])\n",
    "    elif len(cat_id) == 9: # 대분류\n",
    "        return remove_punct(list(final_cat[final_cat['cat_id'] == cat_id]['cat3'])[0])\n",
    "\n",
    "# -- subs 가 있는 것들만 dictionary로 뽑아내기 --\n",
    "def make_category_dict_with_subs() -> dict:\n",
    "    \n",
    "    with open('./bgzt_fashion_category_nums.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    category_dict_with_subs = dict()\n",
    "    for main, mids in data.items():\n",
    "        for mid, subs in mids.items():\n",
    "            d = defaultdict()\n",
    "            if subs != [None]: # 하위 sub들이 있을 때 \n",
    "                d[mid] = subs\n",
    "            category_dict_with_subs.update(d)\n",
    "    return category_dict_with_subs\n",
    "\n",
    "\n",
    "# -- 중분류 내에서의 소분류 morphs 키워드 가져오는 함수 --\n",
    "def make_subcat_morphs_by_midcat(cat_dict:dict) -> dict:\n",
    "    \"\"\"\n",
    "    make_category_dict_with_subs 함수로 부터 받은 딕션어리로\n",
    "    midcat의 하위 subcat들의 morphs 키워드 가져오는 함수 \n",
    "    \"\"\"\n",
    "    mecab = Mecab()\n",
    "    d = defaultdict(list)\n",
    "\n",
    "    for mid, subs in cat_dict.items():\n",
    "        tmp_dict = dict()\n",
    "        for sub in subs:\n",
    "\n",
    "            mid_morph = mecab.morphs(get_cat_name(mid))\n",
    "            sub_morph = mecab.morphs(remove_punct(get_cat_name(sub)))\n",
    "            unique_sub_morph = set(sub_morph) - set(mid_morph)\n",
    "            unique_sub_morph = list(unique_sub_morph)\n",
    "            if unique_sub_morph == []:\n",
    "                d[mid] += sub_morph\n",
    "            d[mid] += unique_sub_morph\n",
    "\n",
    "    return dict(d)\n",
    "\n",
    "# -- 위의 두 함수 합치기 --\n",
    "def get_dict_from_midcat_with_subs():\n",
    "    cat_dict = make_category_dict_with_subs()\n",
    "    morphs_dict = make_subcat_morphs_by_midcat(cat_dict)\n",
    "    return morphs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70841ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from gensim.models.fasttext import FastText\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "def get_df_by_fasttext_test_version3(cat_id:str):\n",
    "    \n",
    "    # -- 카테고리 id에 맞는 데이터 프레임 가져오기 --\n",
    "    df = fashion_df[fashion_df['cat_id'] == cat_id]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # -- tokenizing --\n",
    "    mecab = Mecab()\n",
    "    tokens = []\n",
    "    for idx in range(len(df)): # 명사인 토큰만 가져오기 \n",
    "        token = [pos[0] for pos in mecab.pos(df.loc[idx]['product_name']) if pos[1][0] == 'N']            \n",
    "        if len(token) > 1:\n",
    "            tokens.append(token)\n",
    "            \n",
    "    # -- model 1 training --       \n",
    "    model1 = FastText(tokens, vector_size=100, window=10, min_count=5, workers=4, sg=0)\n",
    "    model1.build_vocab(tokens)\n",
    "    total_examples1 = model1.corpus_count\n",
    "    model1.train(tokens, total_examples=total_examples1, epochs=10)\n",
    "    \n",
    "    # -- get most similar words -- \n",
    "    cat_name_morphs = mecab.morphs(get_cat_name(cat_id))\n",
    "    mid_name_morphs = mecab.morphs(get_cat_name(cat_id[:6]))\n",
    "    cat_name_morphs = list(set(cat_name_morphs) - set(mid_name_morphs))\n",
    "    print(get_cat_name(cat_id))\n",
    "    \n",
    "    results = model1.wv.most_similar(positive=cat_name_morphs, topn=30)\n",
    "    keywords_list = [result[0] for result in results]\n",
    "    keywords = str('|'.join(keywords_list))\n",
    "    \n",
    "    # -- 반대되는 데이터 프레임 --\n",
    "    # -- category id가 9개 자리수인 것만 소분류 내에서 다른 카테고리 키워드 들어간 데이터 삭제해주기 -- \n",
    "    cat_id_keywords_dict = get_dict_from_midcat_with_subs()\n",
    "    if len(cat_id) == 9:\n",
    "        keyword_list = cat_id_keywords_dict[cat_id[:6]]\n",
    "        opposites = list(set(keyword_list) - set(cat_name_morphs))\n",
    "        opposite_keywords = '|'.join(opposites)\n",
    "        df = df[~df['product_name'].str.contains(opposite_keywords)].copy()\n",
    "    \n",
    "    # -- get df --\n",
    "    df_filtered = df[df['product_name'].str.contains(keywords)]\n",
    "    df_filtered = df_filtered.reset_index(drop=True)\n",
    "    \n",
    "    if len(df_filtered) > 400:\n",
    "        df_filtered = df_filtered.sample(400)\n",
    "    \n",
    "    del model1\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9220eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduce_data():\n",
    "    \n",
    "    base_df = pull_and_preprocess_base_df()\n",
    "    fashion_df = base_df.copy()\n",
    "    \n",
    "    cat_id_list = list(fashion_df['cat_id'].unique())\n",
    "    final_df = pd.DataFrame()\n",
    "    for cat_id in cat_id_list:\n",
    "        try:\n",
    "            df = get_df_by_fasttext_test_version3(cat_id)\n",
    "            final_df = pd.concat([final_df, df])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc40ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reduce_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c5047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df = final_df.reset_index(drop=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import core\n",
    "def remove_emojis(text):\n",
    "    return core.replace_emoji(text, replace='')\n",
    "\n",
    "# -- label encoding --\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# labels = encoder.fit_transform(final_df['cat_id'])\n",
    "# final_df['label'] = labels\n",
    "\n",
    "final_df['product_name'] = final_df['product_name'].apply(lambda x : remove_emojis(x))\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5917ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"나머지 프로세스는 다음 주피터 노트북에서\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319b854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbebe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
