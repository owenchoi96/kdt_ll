{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42e57cc",
   "metadata": {},
   "source": [
    "***데이터 가져오기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e6da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/wonbinchoi/opt/anaconda3/envs/new_project\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    blas-1.0                   |         openblas          45 KB\n",
      "    bottleneck-1.3.5           |   py38h67323c0_0         113 KB\n",
      "    libopenblas-0.3.21         |       h54e7dc3_0         4.9 MB\n",
      "    numexpr-2.8.4              |   py38h57a7bef_0         129 KB\n",
      "    numpy-1.23.5               |   py38h57a7bef_0          11 KB\n",
      "    numpy-base-1.23.5          |   py38hc93c6d9_0         6.4 MB\n",
      "    pandas-1.5.3               |   py38h07fba90_0        11.3 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        22.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               pkgs/main/osx-64::blas-1.0-openblas \n",
      "  bottleneck         pkgs/main/osx-64::bottleneck-1.3.5-py38h67323c0_0 \n",
      "  libgfortran        pkgs/main/osx-64::libgfortran-5.0.0-11_3_0_hecd8cb5_28 \n",
      "  libgfortran5       pkgs/main/osx-64::libgfortran5-11.3.0-h9dfd629_28 \n",
      "  libopenblas        pkgs/main/osx-64::libopenblas-0.3.21-h54e7dc3_0 \n",
      "  numexpr            pkgs/main/osx-64::numexpr-2.8.4-py38h57a7bef_0 \n",
      "  numpy              pkgs/main/osx-64::numpy-1.23.5-py38h57a7bef_0 \n",
      "  numpy-base         pkgs/main/osx-64::numpy-base-1.23.5-py38hc93c6d9_0 \n",
      "  pandas             pkgs/main/osx-64::pandas-1.5.3-py38h07fba90_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "numexpr-2.8.4        | 129 KB    |                                       |   0% \n",
      "pandas-1.5.3         | 11.3 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "blas-1.0             | 45 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numpy-1.23.5         | 11 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bottleneck-1.3.5     | 113 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "numexpr-2.8.4        | 129 KB    | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "blas-1.0             | 45 KB     | #############1                        |  36% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "blas-1.0             | 45 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numexpr-2.8.4        | 129 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numpy-1.23.5         | 11 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bottleneck-1.3.5     | 113 KB    | #####2                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ###8                                  |  11% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ###2                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ##1                                   |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bottleneck-1.3.5     | 113 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ########1                             |  22% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ####                                  |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ########3                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ###########6                          |  32% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ######                                |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ############6                         |  34% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ################3                     |  44% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ########3                             |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ################9                     |  46% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ####################2                 |  55% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ##########3                           |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | #####################2                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ########################2             |  66% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ############5                         |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ##########################4           |  71% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ############################5         |  77% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ##############9                       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ###############################7      |  86% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ################################8     |  89% \u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | #################2                    |  47% \u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ####################6                 |  56% \u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ########################7             |  67% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.21   | 4.9 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | #############################3        |  79% \u001b[A\n",
      "pandas-1.5.3         | 11.3 MB   | ###################################   |  95% \u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "numpy-base-1.23.5    | 6.4 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install pandas --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f2cc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17494, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>title</th>\n",
       "      <th>writer</th>\n",
       "      <th>content</th>\n",
       "      <th>writed_at</th>\n",
       "      <th>url</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346630</td>\n",
       "      <td>정부, '우주항공청' 연내 개청…백지신탁 예외 등 특혜 쏟는다</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n [서울=뉴시스] 정부가 올해 중 우리나라의 우주항공 분야 연구개발(R&amp;D)을 ...</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT/과학</td>\n",
       "      <td>IT 일반</td>\n",
       "      <td>네이버</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346633</td>\n",
       "      <td>우주항공청 전문가 영입 `연봉 10억·외국인 허용`</td>\n",
       "      <td>이준기</td>\n",
       "      <td>\\n '한국형 NASA(미 항공우주국)'로 출범하는 우주항공청이 최고의 민간 전문가...</td>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT/과학</td>\n",
       "      <td>IT 일반</td>\n",
       "      <td>네이버</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347804</td>\n",
       "      <td>박정호의 투자유치 '매직'…\"SK쉴더스 IPO보다 더 이득\"</td>\n",
       "      <td>선한결</td>\n",
       "      <td>\\n \"SK쉴더스의 기업공개(IPO)를 철회해서 아쉬웠는데, IPO보다 더 높은 가...</td>\n",
       "      <td>2023-03-01 18:03:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT/과학</td>\n",
       "      <td>IT 일반</td>\n",
       "      <td>네이버</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347806</td>\n",
       "      <td>글·그림·로고 제작·번역...인공지능에 다 맡겨 보니</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n 최근 텍스트 생성 인공지능(AI) 챗봇 챗GPT(ChatGPT)를 이용해 만든...</td>\n",
       "      <td>2023-03-01 18:02:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT/과학</td>\n",
       "      <td>IT 일반</td>\n",
       "      <td>네이버</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>347807</td>\n",
       "      <td>LG엔솔-혼다, 美 배터리 공장 '첫삽'…기공식 개최</td>\n",
       "      <td>이한얼</td>\n",
       "      <td>\\n LG에너지솔루션은 28일(현지시간) 오하이오 주 파이에트 카운티 제퍼슨빌 인근...</td>\n",
       "      <td>2023-03-01 18:26:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT/과학</td>\n",
       "      <td>IT 일반</td>\n",
       "      <td>네이버</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id                               title writer  \\\n",
       "0   346630  정부, '우주항공청' 연내 개청…백지신탁 예외 등 특혜 쏟는다    NaN   \n",
       "1   346633        우주항공청 전문가 영입 `연봉 10억·외국인 허용`    이준기   \n",
       "2   347804   박정호의 투자유치 '매직'…\"SK쉴더스 IPO보다 더 이득\"    선한결   \n",
       "3   347806       글·그림·로고 제작·번역...인공지능에 다 맡겨 보니    NaN   \n",
       "4   347807       LG엔솔-혼다, 美 배터리 공장 '첫삽'…기공식 개최    이한얼   \n",
       "\n",
       "                                             content            writed_at  \\\n",
       "0  \\n [서울=뉴시스] 정부가 올해 중 우리나라의 우주항공 분야 연구개발(R&D)을 ...  2023-03-02 00:00:00   \n",
       "1  \\n '한국형 NASA(미 항공우주국)'로 출범하는 우주항공청이 최고의 민간 전문가...  2023-03-02 00:00:00   \n",
       "2  \\n \"SK쉴더스의 기업공개(IPO)를 철회해서 아쉬웠는데, IPO보다 더 높은 가...  2023-03-01 18:03:05   \n",
       "3  \\n 최근 텍스트 생성 인공지능(AI) 챗봇 챗GPT(ChatGPT)를 이용해 만든...  2023-03-01 18:02:03   \n",
       "4  \\n LG에너지솔루션은 28일(현지시간) 오하이오 주 파이에트 카운티 제퍼슨빌 인근...  2023-03-01 18:26:01   \n",
       "\n",
       "   url main_category sub_category platform  \n",
       "0  NaN         IT/과학        IT 일반      네이버  \n",
       "1  NaN         IT/과학        IT 일반      네이버  \n",
       "2  NaN         IT/과학        IT 일반      네이버  \n",
       "3  NaN         IT/과학        IT 일반      네이버  \n",
       "4  NaN         IT/과학        IT 일반      네이버  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "it_general = pd.read_csv('/Users/wonbinchoi/AutoNewsDB/뉴스.csv')\n",
    "print(it_general.shape)\n",
    "it_general.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0303e",
   "metadata": {},
   "source": [
    "# 클러스터링으로 문장 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd376de",
   "metadata": {},
   "source": [
    "***토큰화***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3945567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "text = docs[0]\n",
    "sent_list = text.split('.')\n",
    "sentences = [s.strip() for s in sent_list]\n",
    "sentences\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af900e2a",
   "metadata": {},
   "source": [
    "***mecab으로 전처리***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 전처리 코드는 gensim에서는 사용하지 않고 나중에 사용할 예정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939798ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 토큰화 => pos tagging => 품사 중 N과 V만 가져오는 함수. \n",
    "def mecab_preprocessor(text):\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "    \n",
    "    posed_list = []\n",
    "    for token in tokens:\n",
    "        posed_list += mecab.pos(token)\n",
    "        \n",
    "    return \" \".join([token[0] for token in posed_list if token[1][0] in ['N', 'V']])\n",
    "    \n",
    "def return_preprocessed_docs(docs):\n",
    "    preprocessed_docs = []\n",
    "    for doc in docs:\n",
    "        preprocessed_docs.append(mecab_preprocessor(doc))\n",
    "    return preprocessed_docs\n",
    "\n",
    "preprocessed_docs = return_preprocessed_docs(summed_docs)\n",
    "preprocessed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c6b7b",
   "metadata": {},
   "source": [
    "***feature vectorization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e96cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_keywords(keywords):\n",
    "    return sorted(zip(keywords.col, keywords.data), key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_keywords(feature_names, sorted_keywords, n=5):\n",
    "    return [(feature_names[idx], score) for idx, score in sorted_keywords[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3643c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_words = [\n",
    "    '대표', '야구', '부회장', '자신', '대비'\n",
    "]\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words=stop_words, max_df=.95, min_df=2, max_features=10000)\n",
    "ftr_vect = tfidf_vect.fit_transform(preprocessed_docs)\n",
    "sorted_keywords = sort_keywords(ftr_vect.tocoo())\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "\n",
    "keywords = extract_keywords(feature_names, sorted_keywords, 15)\n",
    "keywords_list = [keyword_tuple[0] for keyword_tuple in keywords]\n",
    "keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c27101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "    지우지 않는 punctuation 빼고 나머지 삭제 \n",
    "    \"\"\"\n",
    "    \n",
    "    # string의 translate() 메소드를 사용하여 punctuation 제거\n",
    "    remove_punct_dict = dict((ord(i), \" \") for i in string.punctuation \\\n",
    "                             if i not in [\".\"])\n",
    "    \n",
    "    return text.translate(remove_punct_dict)\n",
    "\n",
    "temp_content = temp['content'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67e379",
   "metadata": {},
   "source": [
    "***클렌징 코드***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee76f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_up_contents(text):\n",
    "    \"\"\"not preprocessing\"\"\"\n",
    "    \n",
    "    text = re.sub('[▶△▶️◀️▲▷ⓒ■◆●©️]', '', str(text)) # remove unnecessary markers\n",
    "    text = re.sub('(\\[?\\[\\w+)\\s?\\w{2,}?\\s?(\\w+\\]\\]?)\\s+?(\\[?\\w{2,}?\\]?)', '', text) # remove_reporter_01, [KBS 창원] [앵커], [이코노미스트 정두용 기자] 온라인 \n",
    "    text = re.sub('(\\[\\w+\\s+\\w+.\\s+\\w+\\s+\\w+\\s+\\w+\\s+\\w+])', \"\", text) # '[산업통상자원부 제공. 재판매 및 DB 금지]'\n",
    "    text = re.sub('(\\(\\w+)=(\\w+\\))(\\s+\\w+\\s+\\w+\\s+)=', '', text) # remove_reporter_02\n",
    "    text = re.sub('(\\【)\\w+\\(\\w+\\)=\\w+\\s+\\w+(\\】)', '', text) # remove_reporter_03\n",
    "    text = re.sub('(\\[\\w+\\s?)(\\(\\w+\\))?=(\\s?\\w+\\s?\\w+?\\s?\\w+?\\])(\\w{2,}?\\s+?\\w{2,}?\\s+?)=?', ' ', text) # remove_reporter_04 revised, [서울=뉴시스]최은수 기자 = \n",
    "    #text = re.sub('(\\[\\w+\\s?)=?(\\s?\\w+\\])', ' ', text) # remove_reporter_04\n",
    "    text = re.sub(\"(\\[\\w+\\s+\\w+in\\s+\\w+\\s+\\w+\\])\", \"\", text)# [이코노미스트 마켓in 허지은 기자] \n",
    "    text = re.sub('(\\w+\\(\\w+\\))=', ' ', text)\n",
    "    text = re.sub('(\\<)\\w+\\s*\\d-?\\d?(\\>)', \"\", text)\n",
    "    text = re.sub('(\\<\\w+\\s+?\\w+?\\s+?):?(\\w+\\s+?\\w+?\\s+?\\w+?\\>)', \"\", text) # <삼성전자 OLED TV 라인업 현황> \n",
    "    text = re.sub('(\\[\\w+\\s+\\w+):(\\s+\\w+)-(\\w+\\]).*', '', text)\n",
    "    text = re.sub('(\\w+\\(?=\\w+\\s?\\w{2,}?\\(?)', \"\", text) # 바르셀로나=선한결 기자, 바르셀로나(=스페인), 바르셀로나(=스페인)= \n",
    "    text = re.sub('(\\w+\\s+\\w+\\s+)기자', \"\", text) # 이혁기 더스쿠프 기자\n",
    "    text = re.sub('KBS(\\s+\\w+\\s+\\w+).\\s+(\\w+:\\w+\\/\\w+:\\w+)', \"\", text) # KBS 뉴스 천현수입니다. 촬영기자:지승환/그래픽:박재희\n",
    "    text = re.sub('(\\[\\w+\\],\\[\\w+\\])', \"\", text) # [뉴스리뷰],[앵커]\n",
    "    text = re.sub('(연합뉴스TV\\s+\\w+)', \"\", text) # 연합뉴스TV 김장현입니다.\n",
    "    text = re.sub('(\\w+\\s+기자)', \"\", text) # 손봉석 기자\n",
    "    \n",
    "    \n",
    "    text = re.sub('(\\w+)@(\\w+).(\\w+).(\\w+)', ' ', text) # remove emails\n",
    "    text = re.sub(\"(트위터\\s+)(@)(\\w+)\", \" \", text) # remove twitter accounts\n",
    "#     text = re.sub(\"(\\w+).(\\w+)/(\\w+)\", \" \", text) # remove facebook accounts\n",
    "    text = text.replace('페이스북 /LeYN1', '')\n",
    "    \n",
    "    text = re.sub('(더 많은 글로벌투자).*', ' ', text) # remove uncessary words\n",
    "    text = re.sub(\"(#[\\w+]).*\", \"\", text)\n",
    "    text = text.replace(\".,\", \". \")\n",
    "    \n",
    "    \n",
    "    text = re.sub('[ ]{2,}', ' ', text) # remove redundant empty spaces\n",
    "    text = text.strip() # strip empty spaces\n",
    "    if text[0] == ',':\n",
    "        text = text.strip(',')\n",
    "        \n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "# 빈 콘텐츠 삭제 함수\n",
    "def remove_empty_contents(df):\n",
    "    \n",
    "    for idx, content in enumerate(df['content_cleaned']):\n",
    "        if content == '':\n",
    "            df.drop(index=[idx], axis=0, inplace=True) \n",
    "        if content.count('.') < 2: # '.'이 없는 경우 관련없는 콘텐츠가 있었음. \n",
    "            df.drop(index=[idx], axis=0, inplace=True)\n",
    "            \n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# 글자수가 일정 숫자 아래면 제거 -> why? 사진이 메인이고 사진을 설명하는 description일 가능성이 높음. \n",
    "def remove_contents_less_than_280(df):\n",
    "    for ind, content in enumerate(df['content_cleaned']):\n",
    "        if len(content) < 280:\n",
    "            df.drop(index=[ind], axis=0, inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 최종 클렌징 코드\n",
    "def cleanse_contents(df):\n",
    "    \n",
    "    df['content_cleaned'] = df['content'].apply(lambda x : clean_up_contents(x)) # content 클렌징 함수\n",
    "    df = remove_empty_contents(df) # 컨텐츠가 빈 row 제거 \n",
    "    df = remove_contents_less_than_280(df)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "it_general = cleanse_contents(it_general)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c6f57",
   "metadata": {},
   "source": [
    "***KMeans로 클러스터링 summarize 하기***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca52e1",
   "metadata": {},
   "source": [
    "***요약된 문장 가져오기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148e10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.summarization import summarize\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# tokenizer\n",
    "def mecab_tokenizer(doc):\n",
    "    mecab = Mecab()\n",
    "    sentences = [sentences.strip() for sentences in doc.split('.')]\n",
    "    pos_list = []\n",
    "    for sent in sentences:\n",
    "        for pos in mecab.pos(sent):\n",
    "            if pos[1][0] in ['N']:\n",
    "                if len(pos[0]) > 1:\n",
    "                    pos_list.append(pos[0])\n",
    "\n",
    "    return pos_list\n",
    "\n",
    "\n",
    "\n",
    "def create_summarized_sentences(docs_df, n_clusters, n_top_words=7):\n",
    "    \n",
    "    docs = docs_df['content_cleaned']\n",
    "    \n",
    "\n",
    "    stop_words_list = ['대표이사', '대표', '이벤트', '컴투스', '넷마블', '프라시아 전기', '렐름',\n",
    "        '사진공동취재단', \n",
    "        '산업통상자원부 제공',]\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(\n",
    "                                 tokenizer=mecab_tokenizer,\n",
    "                                 max_df=.95,\n",
    "                                 min_df=2, \n",
    "                                 ngram_range=(1,3), \n",
    "                                 stop_words=stop_words_list\n",
    "                                 )\n",
    "    ftr_vect = tfidf_vect.fit_transform(docs)\n",
    "    \n",
    "\n",
    "    # KMeans 사용하여 클러스터링 (문서 군집화)\n",
    "    km_cluster = KMeans(n_clusters=3, max_iter=1000, random_state=0)\n",
    "    km_cluster.fit(ftr_vect)\n",
    "    \n",
    "    # 문서를 label로 나누기 위해 km_cluster.labels_를 데이터프레임에 컬럼으로 넣어주기 \n",
    "    docs_df['label'] = km_cluster.labels_\n",
    "    \n",
    "    # 요약된 문장 한 리스트 안에 넣어주기 \n",
    "    summed_list = []\n",
    "    for n in range(n_clusters):\n",
    "        summed_list.append(\" \".join(summarize(\"\\n\\n\".join(docs_df[docs_df['label'] == n]['content_cleaned'].values.tolist()), \n",
    "                              ratio=.3, word_count=50, split=True)))\n",
    "        \n",
    "    for idx, results in enumerate(summed_list):\n",
    "        print (\"\\n{} 번째 군집화의 요약된 문장 ==> {}\".format(idx, results))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return summed_list\n",
    "\n",
    "it_general_summed_list = create_summarized_sentences(it_general, n_clusters=3, n_top_words=5) \n",
    "it_general_summed_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9b72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4daa244",
   "metadata": {},
   "source": [
    "# 토픽모델링으로 문장 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42682dff",
   "metadata": {},
   "source": [
    "***하이퍼 파라미터 튜닝 + 문서 요약***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b66854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\"\"\"토큰화 함수\"\"\"\n",
    "def mecab_tokenizer(doc):\n",
    "    mecab = Mecab()\n",
    "    sentences = [sentences.strip() for sentences in doc.split('.')]\n",
    "    pos_list = []\n",
    "    for sent in sentences:\n",
    "        for pos in mecab.pos(sent):\n",
    "            if pos[1][0] in ['N']: # 우선 명사만 토큰화 \n",
    "                if len(pos[0]) > 1:\n",
    "                    pos_list.append(pos[0])\n",
    "\n",
    "    return pos_list\n",
    "\n",
    "\"\"\"하이퍼 파라미터 튜닝 함수\"\"\"\n",
    "def hyperparms_tuning(train_data, n_jobs=1):\n",
    "    lda_pipeline = Pipeline([               # tokenizer 바꿔서 써보기 \n",
    "        ('tfidf_vect', TfidfVectorizer(tokenizer=mecab_tokenizer, max_df=.95, min_df=2, stop_words=['휘센', '대표'])),\n",
    "        ('lda', LatentDirichletAllocation(max_iter=10, random_state=0, learning_method='online'))\n",
    "    ])\n",
    "    \n",
    "    search_params = {\n",
    "        'tfidf_vect__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "        'tfidf_vect__use_idf' : (True, False),\n",
    "        'lda__n_components' : [3,4,5]\n",
    "    }\n",
    "    \n",
    "    gs_lda = GridSearchCV(lda_pipeline, param_grid=search_params, n_jobs=n_jobs)\n",
    "    gs_lda = gs_lda.fit(train_data)\n",
    "#     print('best score : {}'.format(gs_lda.best_score_))\n",
    "    best_parameters = gs_lda.best_estimator_.get_params()\n",
    "#     for params_name in sorted(list(best_parameters.keys())):\n",
    "#         print('\\t{0}: {1}'.format(params_name, best_parameters[params_name]))\n",
    "    return gs_lda.best_estimator_\n",
    "\n",
    "\n",
    "\"\"\"문장 요약 함수\"\"\"\n",
    "def extract_summarized_sentences(df, column_name='content'):\n",
    "    lda_fitted = hyperparms_tuning(df[column_name][:100]) # 여기서 [:100]는 나중에 빼주기 \n",
    "    new_topics = lda_fitted.transform(df[column_name][:100])\n",
    "    topic_names = ['Topic #' + str(i) for i in range(lda_fitted.get_params()['lda__n_components'])] # n_components 수만큼 Topic 가져오기 \n",
    "    topic_df = pd.DataFrame(data=news_topics, index=df.title[:100], columns = topic_names)\n",
    "    temp_df = topic_df.join(df[:100][[column_name, 'title']].set_index('title'), on='title')\n",
    "\n",
    "\n",
    "    # 임계값 기준으로 토픽 나누기 \n",
    "    threshold=0.5 # 임계값은 임의로 .5로 먼저 설정 \n",
    "    topic_classfication_dict = dict()\n",
    "\n",
    "    for topic in temp_df.columns[:-1].tolist(): # 마지막 컬럼 빼고 \n",
    "        idx_list = [idx for idx, value in enumerate(temp_df[topic].values) if value > .5]\n",
    "        value_list = [value for idx, value in enumerate(temp_df[topic].values) if value > .5]\n",
    "        topic_classfication_dict[topic] = {}\n",
    "        topic_classfication_dict[topic]['idx'] = idx_list\n",
    "        topic_classfication_dict[topic]['value'] = value_list\n",
    "\n",
    "    # 토픽별 인덱스 값으로 뉴스 요약 출력하기 \n",
    "    for com_num in range(lda_fitted.get_params()['lda__n_components']):\n",
    "        topic_idx = [idx for idx in topic_classfication_dict[f'Topic #{com_num}']['idx']]\n",
    "        print (\"-\"*110)            \n",
    "        print (summarize(\"\".join(temp_df.iloc[topic_idx][column_name].tolist()), word_count=30, ratio=.2))\n",
    "                                                                                        # 문장길이 조절 가능 \n",
    "    return\n",
    "\n",
    "\n",
    "extract_summarized_sentences(it_general, column_name = 'reduced_content')\n",
    "\n",
    "# \"\"\"마지막으로 요약된 문장 내 이메일로 보내는 함수\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f15d4",
   "metadata": {},
   "source": [
    "***문장 길이 조절 후 요약해보기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def remove_punct(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation if punct not in ['.'])\n",
    "    return text.translate(remove_punct_dict)\n",
    "    \n",
    "len_list = [len(content) for content in it_general['content_cleaned'].apply(lambda x : remove_punct(x))]\n",
    "\n",
    "new_content_list = []\n",
    "for content in it_general['content_cleaned']:\n",
    "    if len(remove_punct(content)) > np.median(len_list):\n",
    "        new_content_list.append(summarize(content, word_count=np.median(len_list)))\n",
    "    else:\n",
    "        new_content_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4eafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_general['reduced_content'] = new_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 변화전 문장 길이 분포도 \n",
    "len_list = [len(content) for content in it_general['content_cleaned']]\n",
    "plt.hist(len_list, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc643594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 변화 후 분포도 \n",
    "new_len_list = [len(content) for content in new_content_list]\n",
    "\n",
    "plt.hist(new_len_list, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b63c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"문장 길이 변화후 결론 ==> 요약에 크게 변화된 점이 발견되지 않음. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a359a8",
   "metadata": {},
   "source": [
    "***문장별 전처리 코드 (여기서는 사용하지 않음)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punct(sent):\n",
    "    remove_punct_dict = dict((ord(punct), \" \") for punct in string.punctuation if punct not in ['(', ')', ',', '&', '\"'])\n",
    "    return sent.translate(remove_punct_dict)\n",
    "\n",
    "doc = it_general['content_cleaned'][2]\n",
    "sentence = ''\n",
    "for sent in doc.split('.'):\n",
    "    sent = sent.strip()\n",
    "    sent = remove_punct(sent)\n",
    "    if len(sent) == 0:\n",
    "        continue\n",
    "    sent = sent.strip()\n",
    "    sent += '. '\n",
    "    sentence += sent\n",
    "    \n",
    "re.sub('[ ]{2,}',' ',sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a0f55",
   "metadata": {},
   "source": [
    "***잠깐 전처리***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017eca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 김현일·] 없애기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25cd756e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/wonbinchoi/opt/anaconda3/envs/new_project\n",
      "\n",
      "  added / updated specs:\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  joblib             conda-forge/noarch::joblib-1.2.0-pyhd8ed1ab_0 \n",
      "  libblas            conda-forge/osx-64::libblas-3.9.0-16_osx64_openblas \n",
      "  libcblas           conda-forge/osx-64::libcblas-3.9.0-16_osx64_openblas \n",
      "  liblapack          conda-forge/osx-64::liblapack-3.9.0-16_osx64_openblas \n",
      "  pooch              conda-forge/noarch::pooch-1.7.0-pyha770c72_3 \n",
      "  python_abi         conda-forge/osx-64::python_abi-3.8-2_cp38 \n",
      "  scikit-learn       conda-forge/osx-64::scikit-learn-1.2.2-py38h1081964_1 \n",
      "  scipy              conda-forge/osx-64::scipy-1.10.1-py38hfb8b963_0 \n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.1.0-pyh8a188c0_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2022.12.7-h033912b_0 \n",
      "  certifi            pkgs/main/osx-64::certifi-2022.12.7-p~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0 \n",
      "  openssl              pkgs/main::openssl-1.1.1t-hca72f7f_0 --> conda-forge::openssl-1.1.1t-hfd90126_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge scikit-learn --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04eca3dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/wonbinchoi/opt/anaconda3/envs/new_project\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim==3.8.3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    gensim-3.8.3               |   py38h23ab428_2        18.4 MB\n",
      "    smart_open-5.2.1           |   py38hecd8cb5_0          77 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        18.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  gensim             pkgs/main/osx-64::gensim-3.8.3-py38h23ab428_2 \n",
      "  smart_open         pkgs/main/osx-64::smart_open-5.2.1-py38hecd8cb5_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2022.12.~ --> pkgs/main::ca-certificates-2023.01.10-hecd8cb5_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2022.12.7~ --> pkgs/main/osx-64::certifi-2022.12.7-py38hecd8cb5_0 \n",
      "  openssl            conda-forge::openssl-1.1.1t-hfd90126_0 --> pkgs/main::openssl-1.1.1t-hca72f7f_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "smart_open-5.2.1     | 77 KB     |                                       |   0% \n",
      "smart_open-5.2.1     | 77 KB     | ##################################### | 100% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   |                                       |   0% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | 1                                     |   0% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | 3                                     |   1% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | 5                                     |   1% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | 8                                     |   2% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | #2                                    |   3% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | #8                                    |   5% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | ##6                                   |   7% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | ###8                                  |  10% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | #####5                                |  15% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | #######9                              |  22% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | ##########6                           |  29% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | ##############2                       |  38% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | #################8                    |  48% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | #####################8                |  59% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | ##########################1           |  71% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | ##############################1       |  82% \u001b[A\n",
      "gensim-3.8.3         | 18.4 MB   | #################################9    |  92% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install gensim==3.8.3 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b811991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaddcb6",
   "metadata": {},
   "source": [
    "**********************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "deb02866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "# from scrapy.utils.project import get_project_settings\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.summarization import summarize # keywords\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import smtplib\n",
    "# import email\n",
    "# from email.mime.multipart import MIMEMultipart\n",
    "# from email.mime.text import MIMEText\n",
    "# from email.mime.base import MIMEBase\n",
    "# from email import encoders\n",
    "# from email.header import Header\n",
    "# from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcef0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# scrapy 변수 설정, settings에서 데이터 이름, 형식 지정\n",
    "SPIDER_NAME = 'naver_spider'\n",
    "PATH = 'C:/Users/tlrks/Desktop/workspace/exercise/NLP/ScrapyNewsCrawler/Naver_news/Naver_news/test.csv'\n",
    "\n",
    "# email 변수 설정\n",
    "SMTP_SERVER = ''\n",
    "SMTP_PORT = 465\n",
    "SMTP_USER = ''\n",
    "SMTP_PASSWORD = ''\n",
    "\n",
    "to_users = ['prbl@kakao.com']\n",
    "target_addr = ','.join(to_users)\n",
    "subject = datetime.now().date().strftime('%Y. %m. %d') + '기사 요약 메일'\n",
    "\n",
    "# 불용어 불러오기\n",
    "stopword_file = open('/Users/wonbinchoi/Downloads/stopword.txt', 'r', encoding='utf-8')\n",
    "stopwords= []\n",
    "for word in stopword_file.readlines():\n",
    "    stopwords.append(word.rstrip())\n",
    "stopword_file.close()\n",
    "\n",
    "CATEGORIES = {\n",
    "        ('경제', 101) : {\n",
    "            '금융' : '259',\n",
    "            '증권' : '258',\n",
    "            '산업/재계' : '261',\n",
    "            '중기/벤처' : '771',\n",
    "            '부동산' : '260',\n",
    "            '글로벌 경제' : '262',\n",
    "            '생활경제' : '310',\n",
    "            '경제 일반' : '263'\n",
    "        },\n",
    "        ('IT/과학', 105) : {\n",
    "            '모바일' : '731',\n",
    "            '인터넷/SNS' : '226',\n",
    "            '통신/뉴미디어' : '227',\n",
    "            'IT 일반' : '230',\n",
    "            '보안/해킹' : '732',\n",
    "            '컴퓨터' : '283',\n",
    "            '게임/리뷰' : '229',\n",
    "            '과학 일반' : '228'\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d60c5acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6436, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/wonbinchoi/Downloads/day_data.csv', encoding='utf-8')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65b69ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 전 기사 수 : 6436\n",
      "처리 후 기사 수 : 5989\n",
      "11:59:06 카테고리별 기사 추출 시작\n",
      "11:59:28금융 추출 완료\n",
      "11:59:53증권 추출 완료\n",
      "12:00:46산업/재계 추출 완료\n",
      "12:00:54중기/벤처 추출 완료\n",
      "12:01:10부동산 추출 완료\n",
      "12:01:18글로벌 경제 추출 완료\n",
      "12:01:37생활경제 추출 완료\n",
      "12:02:48경제 일반 추출 완료\n",
      "12:02:50모바일 추출 완료\n",
      "12:02:55인터넷/SNS 추출 완료\n",
      "12:03:00통신/뉴미디어 추출 완료\n",
      "12:03:21IT 일반 추출 완료\n",
      "12:03:22보안/해킹 추출 완료\n",
      "12:03:25컴퓨터 추출 완료\n",
      "12:03:29게임/리뷰 추출 완료\n",
      "12:03:34과학 일반 추출 완료\n",
      "{'금융': [('추경호, IDB 총재 만나…“韓 중남미 진출 교두보 마련 협조 당부”', 'https://n.news.naver.com/mnews/article/030/0003090951?sid=101', '추경호 부총리 겸 기획재정부 장관(오른쪽)이 12일(현지시간) 미국 국제통화기금(IMF)에서 국제신용평가사 피치(Fitch)의 제임스 맥코맥 국가신용등급 글로벌총괄과 악수하며 인사를 나누고 있다.\\n추경호 부총리 겸 기획재정부 장관(오른쪽)이 12일(현지시간) 국제통화기금(IMF)에서 마그달레나 제치코브스카 폴란드 재무장관과 면담에 앞서 악수하며 기념사진을 찍고 있다.')], '증권': [('물가 호재보다 경기침체가…\"어닝 쇼크 주의, 변동성 대비해야\"', 'https://n.news.naver.com/mnews/article/008/0004874791?sid=101', '임종철 디자인기자 /사진=임종철 디자인기자 경기침체 우려가 커지며 증시의 상승 동력도 점차 약해진다.\\n인플레이션 완화 기대감에 미국 증시는 장 초반 상승세를 보였으나 곧 경기침체 우려가 부각되며 하락 전환했다.')], '산업/재계': [('S-OIL, 전기차 전용 윤활유 브랜드 ‘세븐 EV’ 출시', 'https://n.news.naver.com/mnews/article/366/0000893136?sid=101', \"에쓰오일은 자회사인 윤활유 전문업체 에쓰오일 토탈윤활유(STLC)를 통해 하이브리드차량 엔진 전용 윤활유의 국내 판매를 시작했으며, 액슬(Axle) 오일 등 기타 전기차 전용 제품들도 순차적으로 출시할 예정이다.\\n에쓰오일은 세계적 수준의 윤활기유 경쟁력을 바탕으로 최고급 윤활유 브랜드 '에쓰오일 7′으로 국내외 프리미엄 윤활유 시장을 확대하고 있다.\")], '중기/벤처': [('냉온수 탱크 30년 한우물 ‘뚝심’ 통했다', 'https://n.news.naver.com/mnews/article/016/0002129841?sid=101', '큐비에스의 인천 송도 본사 생산라인 [큐비에스 제공] 코로나19 봉쇄와 한-중 자유무역협정(FTA) 등 교역조건 위기를 극복하고 글로벌 시장에서 승승장구하고 있는 강소기업이 업계의 주목을 받고 있다.\\n한편 중진공은 올해 100억원의 무역조정자금을 무역피해 기업에 지원한다.')], '부동산': [('서부산 개발호재 수혜 기대… ‘서부산 SK V1’ 분양', 'https://n.news.naver.com/mnews/article/011/0004178648?sid=101', \"[서울경제] 굵직한 개발호재로 주목을 받고 있는 서부산에서 지식산업센터 '서부산 SK V1'이 분양 중이다.\\n신평장림산업단지는 가덕도 신공항 개발예정지, 에코델타시티 인근이라 다양한 개발 호재로 주목받고 있다.\\n서부산 SK V1의 제1홍보관은 수영구에, 제2홍보관은 사하구에 마련돼 있다.\")], '글로벌 경제': [('뉴욕증시, 도매물가 냉각에 장초반 상승세…나스닥 1%대 ↑', 'https://n.news.naver.com/mnews/article/277/0005245311?sid=101', '경제매체 CNBC는 \"인플레이션 냉각을 보여주는 또 다른 보고서(PPI)로 증시가 상승세\"라며 \"다만 경기침체가 임박했다는 우려로 (다우, S&P500지수의) 상승폭은 크지 않다\"고 분위기를 전했다.\\n시카고상품거래소(CME) 페드워치에 따르면 현재 연방기금금리 선물시장은 Fed가 5월 기준금리를 올리는 베이비스텝에 나설 가능성을 60%이상 반영하고 있다.')], '생활경제': [('잠실에 등장한 ‘초록색 개구리’의 정체는?', 'https://n.news.naver.com/mnews/article/119/0002702291?sid=101', '윤창욱 롯데백화점 Casual2(캐주얼2)팀 치프바이어는 \"4월은 동남아시아 국가들의 황금연휴가 몰려있는 만큼 국내 MZ세대 고객뿐 아니라 외국인 관광객들도 좋아할만한 콘텐츠를 선보이기 위해 심혈을 기울여 준비했다\"며 \"앞으로도 고객들에게 색다른 경험과 즐거움을 선사할 수 있도록 지속적으로 노력하겠다\"고 밝혔다.')], '경제 일반': [('[굿모닝 마켓] \"연준 너마저\"…경기침체 우려에 다우 지수까지 하락', 'https://n.news.naver.com/mnews/article/374/0000331408?sid=101', '3월 연방공개시장위원회 FOMC 의사록을 보니, 연준마저 경기침체를 예상하고 있다는 것이 확인됐기 때문입니다.\\n그래서 제러미 시걸 와튼 스쿨 교수는 연준이 금리를 인하할 것이라고 보고 있습니다.')], '모바일': [('티라유텍, 국가산업대상 스마트팩토리 부문 대상', 'https://n.news.naver.com/mnews/article/092/0002288777?sid=105', '\"2차전지 특화 솔루션 개발 등 국가경쟁력 기여 기대\" 스마트팩토리 솔루션 전문기업 티라유텍은 13일 산업정책연구원이 주최하는 \\'2023 국가산업대상 수여식\\'에서 스마트팩토리 부문 대상을 수상했다.\\n(사진=티라유텍) 티라유텍은 스마트팩토리 구축에 필요한 생산계획 수립부터 생산관리, 소프트웨어, 자율주행물류로봇(AMR)까지 통합 솔루션을 제시하고 있다.')], '인터넷/SNS': [('네이버 자회사, 장애인고용촉진대회서 표창', 'https://n.news.naver.com/mnews/article/092/0002288781?sid=105', \"네이버핸즈는 장애인 고용 촉진에 기여한 우수 사업주로 선정돼 고용노동부 장관 표창을 수상했다.\\n일 서울 송파구에서 진행된 '2023 장애인고용촉진대회'에서 고용노동부 장관 표창을 받고 있는 노세관 네이버핸즈 대표.\")], '통신/뉴미디어': [('금융권 알뜰폰은 규제 없는데… 통신 요금 사전규제 강화하나', 'https://n.news.naver.com/mnews/article/029/0002794645?sid=105', '과기정통부 관계자는 \"간담회에서는 요금 규제보다 최적요금제 고지, 통신요금 모니터링 등 요금정보 제공 강화방안이 주로 논의됐다\"며 \"구체적인 정보제공 방법과 정보 제공자의 신뢰성 등에 대한 다양한 의견이 제시됐다\"고 말했다.')], 'IT 일반': [('수트 대신 후드티... 김 검사는 왜 로펌 말고 블록체인을 택했을까?[C레벨 탐구]', 'https://n.news.naver.com/mnews/article/469/0000733888?sid=105', '법무실은 여기서 나아가 신규 서비스를 출시하거나, 새로운 정책이 도입될 때 기획 초기 단계부터 개입해 촘촘하게 법률적 조언을 하고 있습니다.\\n\"운영정책실은 두나무의 여러 서비스, 특히 가상화폐 등 디지털자산 분야와 관련된 정책을 검토하고 수립하는 업무를 맡고 있습니다.')], '보안/해킹': [('똑똑한 챗GPT 하나면 보안도 OK…마이크로소프트 AI 보안비서 공개', 'https://n.news.naver.com/mnews/article/421/0006744503?sid=105', '보안업계 \"챗GPT 잘 활용땐 실시간 취약점 대응 도움\" MS \\'AI 보안비서\\' 선봬…국내 보안기업도 챗GPT 적용 \\n13일 업계에 따르면, 마이크로소프트(MS)는 최근 챗GPT 기반의 보안 제품 \\'시큐리티 코파일럿\\'을 공개했다.')], '컴퓨터': [('오라클, OCI 도입한 맨텍-보나캠프-FNS 사례 공개', 'https://n.news.naver.com/mnews/article/138/0002146407?sid=105', '맨텍 이진현 상무는 \"맨텍은 OCI를 통해 솔루션 교육 서비스 비용을 대폭 절감시켰으며, 이로써 많은 기업들이 클라우드 네이티브 앱을 보다 간편하게 개발 운영하도록 지원할 수 있게 됐다.')], '게임/리뷰': [('적자폭 줄인 라인게임즈…경영 효율화·신작 러시로 올해 달린다', 'https://n.news.naver.com/mnews/article/138/0002146340?sid=105', '12일 공시된 라인게임즈 연결감사보고서에 따르면 라인게임즈는 지난해 연결 기준 매출 827억5657만원을 기록했다.\\n라인게임즈는 지난해 1월 모바일 핵앤슬래시 액션 역할수행게임(RPG) 언디셈버 출시를 시작으로, 같은해 8월 모바일 다중접속역할수행게임(MMORPG) 대항해시대 오리진을 선보이며 괄목할만한 성과를 거뒀다.')], '과학 일반': [('튀르키예 지진파, 9분50초 만에 한반도 강타했다', 'https://n.news.naver.com/mnews/article/277/0005245154?sid=105', '사진출처=한국지질자원연구원(KIGAM) 제공 튀르키예 지진은 지자연 지질재해연구본부에서 운영 중인 광대역 관측소 모두에서 관측됐다.\\n앞서 지자연은 튀르키예 대지진으로 한반도 지하수위까지 영향을 받았다는 사실을 확인한 바 있다.')]}\n",
      "{'금융': [('추경호, IDB 총재 만나…“韓 중남미 진출 교두보 마련 협조 당부”', 'https://n.news.naver.com/mnews/article/030/0003090951?sid=101', '추경호 부총리 겸 기획재정부 장관(오른쪽)이 12일(현지시간) 미국 국제통화기금(IMF)에서 국제신용평가사 피치(Fitch)의 제임스 맥코맥 국가신용등급 글로벌총괄과 악수하며 인사를 나누고 있다.\\n추경호 부총리 겸 기획재정부 장관(오른쪽)이 12일(현지시간) 국제통화기금(IMF)에서 마그달레나 제치코브스카 폴란드 재무장관과 면담에 앞서 악수하며 기념사진을 찍고 있다.')], '증권': [('물가 호재보다 경기침체가…\"어닝 쇼크 주의, 변동성 대비해야\"', 'https://n.news.naver.com/mnews/article/008/0004874791?sid=101', '임종철 디자인기자 /사진=임종철 디자인기자 경기침체 우려가 커지며 증시의 상승 동력도 점차 약해진다.\\n인플레이션 완화 기대감에 미국 증시는 장 초반 상승세를 보였으나 곧 경기침체 우려가 부각되며 하락 전환했다.')], '산업/재계': [('S-OIL, 전기차 전용 윤활유 브랜드 ‘세븐 EV’ 출시', 'https://n.news.naver.com/mnews/article/366/0000893136?sid=101', \"에쓰오일은 자회사인 윤활유 전문업체 에쓰오일 토탈윤활유(STLC)를 통해 하이브리드차량 엔진 전용 윤활유의 국내 판매를 시작했으며, 액슬(Axle) 오일 등 기타 전기차 전용 제품들도 순차적으로 출시할 예정이다.\\n에쓰오일은 세계적 수준의 윤활기유 경쟁력을 바탕으로 최고급 윤활유 브랜드 '에쓰오일 7′으로 국내외 프리미엄 윤활유 시장을 확대하고 있다.\")], '중기/벤처': [('냉온수 탱크 30년 한우물 ‘뚝심’ 통했다', 'https://n.news.naver.com/mnews/article/016/0002129841?sid=101', '큐비에스의 인천 송도 본사 생산라인 [큐비에스 제공] 코로나19 봉쇄와 한-중 자유무역협정(FTA) 등 교역조건 위기를 극복하고 글로벌 시장에서 승승장구하고 있는 강소기업이 업계의 주목을 받고 있다.\\n한편 중진공은 올해 100억원의 무역조정자금을 무역피해 기업에 지원한다.')], '부동산': [('서부산 개발호재 수혜 기대… ‘서부산 SK V1’ 분양', 'https://n.news.naver.com/mnews/article/011/0004178648?sid=101', \"[서울경제] 굵직한 개발호재로 주목을 받고 있는 서부산에서 지식산업센터 '서부산 SK V1'이 분양 중이다.\\n신평장림산업단지는 가덕도 신공항 개발예정지, 에코델타시티 인근이라 다양한 개발 호재로 주목받고 있다.\\n서부산 SK V1의 제1홍보관은 수영구에, 제2홍보관은 사하구에 마련돼 있다.\")], '글로벌 경제': [('뉴욕증시, 도매물가 냉각에 장초반 상승세…나스닥 1%대 ↑', 'https://n.news.naver.com/mnews/article/277/0005245311?sid=101', '경제매체 CNBC는 \"인플레이션 냉각을 보여주는 또 다른 보고서(PPI)로 증시가 상승세\"라며 \"다만 경기침체가 임박했다는 우려로 (다우, S&P500지수의) 상승폭은 크지 않다\"고 분위기를 전했다.\\n시카고상품거래소(CME) 페드워치에 따르면 현재 연방기금금리 선물시장은 Fed가 5월 기준금리를 올리는 베이비스텝에 나설 가능성을 60%이상 반영하고 있다.')], '생활경제': [('잠실에 등장한 ‘초록색 개구리’의 정체는?', 'https://n.news.naver.com/mnews/article/119/0002702291?sid=101', '윤창욱 롯데백화점 Casual2(캐주얼2)팀 치프바이어는 \"4월은 동남아시아 국가들의 황금연휴가 몰려있는 만큼 국내 MZ세대 고객뿐 아니라 외국인 관광객들도 좋아할만한 콘텐츠를 선보이기 위해 심혈을 기울여 준비했다\"며 \"앞으로도 고객들에게 색다른 경험과 즐거움을 선사할 수 있도록 지속적으로 노력하겠다\"고 밝혔다.')], '경제 일반': [('[굿모닝 마켓] \"연준 너마저\"…경기침체 우려에 다우 지수까지 하락', 'https://n.news.naver.com/mnews/article/374/0000331408?sid=101', '3월 연방공개시장위원회 FOMC 의사록을 보니, 연준마저 경기침체를 예상하고 있다는 것이 확인됐기 때문입니다.\\n그래서 제러미 시걸 와튼 스쿨 교수는 연준이 금리를 인하할 것이라고 보고 있습니다.')], '모바일': [('티라유텍, 국가산업대상 스마트팩토리 부문 대상', 'https://n.news.naver.com/mnews/article/092/0002288777?sid=105', '\"2차전지 특화 솔루션 개발 등 국가경쟁력 기여 기대\" 스마트팩토리 솔루션 전문기업 티라유텍은 13일 산업정책연구원이 주최하는 \\'2023 국가산업대상 수여식\\'에서 스마트팩토리 부문 대상을 수상했다.\\n(사진=티라유텍) 티라유텍은 스마트팩토리 구축에 필요한 생산계획 수립부터 생산관리, 소프트웨어, 자율주행물류로봇(AMR)까지 통합 솔루션을 제시하고 있다.')], '인터넷/SNS': [('네이버 자회사, 장애인고용촉진대회서 표창', 'https://n.news.naver.com/mnews/article/092/0002288781?sid=105', \"네이버핸즈는 장애인 고용 촉진에 기여한 우수 사업주로 선정돼 고용노동부 장관 표창을 수상했다.\\n일 서울 송파구에서 진행된 '2023 장애인고용촉진대회'에서 고용노동부 장관 표창을 받고 있는 노세관 네이버핸즈 대표.\")], '통신/뉴미디어': [('금융권 알뜰폰은 규제 없는데… 통신 요금 사전규제 강화하나', 'https://n.news.naver.com/mnews/article/029/0002794645?sid=105', '과기정통부 관계자는 \"간담회에서는 요금 규제보다 최적요금제 고지, 통신요금 모니터링 등 요금정보 제공 강화방안이 주로 논의됐다\"며 \"구체적인 정보제공 방법과 정보 제공자의 신뢰성 등에 대한 다양한 의견이 제시됐다\"고 말했다.')], 'IT 일반': [('수트 대신 후드티... 김 검사는 왜 로펌 말고 블록체인을 택했을까?[C레벨 탐구]', 'https://n.news.naver.com/mnews/article/469/0000733888?sid=105', '법무실은 여기서 나아가 신규 서비스를 출시하거나, 새로운 정책이 도입될 때 기획 초기 단계부터 개입해 촘촘하게 법률적 조언을 하고 있습니다.\\n\"운영정책실은 두나무의 여러 서비스, 특히 가상화폐 등 디지털자산 분야와 관련된 정책을 검토하고 수립하는 업무를 맡고 있습니다.')], '보안/해킹': [('똑똑한 챗GPT 하나면 보안도 OK…마이크로소프트 AI 보안비서 공개', 'https://n.news.naver.com/mnews/article/421/0006744503?sid=105', '보안업계 \"챗GPT 잘 활용땐 실시간 취약점 대응 도움\" MS \\'AI 보안비서\\' 선봬…국내 보안기업도 챗GPT 적용 \\n13일 업계에 따르면, 마이크로소프트(MS)는 최근 챗GPT 기반의 보안 제품 \\'시큐리티 코파일럿\\'을 공개했다.')], '컴퓨터': [('오라클, OCI 도입한 맨텍-보나캠프-FNS 사례 공개', 'https://n.news.naver.com/mnews/article/138/0002146407?sid=105', '맨텍 이진현 상무는 \"맨텍은 OCI를 통해 솔루션 교육 서비스 비용을 대폭 절감시켰으며, 이로써 많은 기업들이 클라우드 네이티브 앱을 보다 간편하게 개발 운영하도록 지원할 수 있게 됐다.')], '게임/리뷰': [('적자폭 줄인 라인게임즈…경영 효율화·신작 러시로 올해 달린다', 'https://n.news.naver.com/mnews/article/138/0002146340?sid=105', '12일 공시된 라인게임즈 연결감사보고서에 따르면 라인게임즈는 지난해 연결 기준 매출 827억5657만원을 기록했다.\\n라인게임즈는 지난해 1월 모바일 핵앤슬래시 액션 역할수행게임(RPG) 언디셈버 출시를 시작으로, 같은해 8월 모바일 다중접속역할수행게임(MMORPG) 대항해시대 오리진을 선보이며 괄목할만한 성과를 거뒀다.')], '과학 일반': [('튀르키예 지진파, 9분50초 만에 한반도 강타했다', 'https://n.news.naver.com/mnews/article/277/0005245154?sid=105', '사진출처=한국지질자원연구원(KIGAM) 제공 튀르키예 지진은 지자연 지질재해연구본부에서 운영 중인 광대역 관측소 모두에서 관측됐다.\\n앞서 지자연은 튀르키예 대지진으로 한반도 지하수위까지 영향을 받았다는 사실을 확인한 바 있다.')]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# scrapy crawler 실행하는 함수(파일 경로는 settings에서 설정)\n",
    "# def run_spider():\n",
    "#     process = CrawlerProcess(get_project_settings())\n",
    "#     process.crawl(SPIDER_NAME)\n",
    "#     process.start()\n",
    "\n",
    "# # 크롤링한 데이터 불러오는 함수\n",
    "# def load_data(path):\n",
    "#     data = pd.read_csv(path, encoding='utf-8', dtype=object)\n",
    "#     # print(data.groupby(by=['main_category', 'sub_category']).id.count())\n",
    "#     return data\n",
    "\n",
    "# 사망기사 관련 키워드 두 개 이상 포함 시 삭제\n",
    "def isObituary(text):\n",
    "    words = ['별세', '장례', '발인', '부고', '친상']\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            count += 1\n",
    "            if count > 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# 주어진 길이보다 긴 경우 True 반환\n",
    "def isLonger(text, length):\n",
    "    if len(text) > length:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "def preprocess(data):\n",
    "    print('처리 전 기사 수 :', len(data))\n",
    "    data.dropna(subset=['content'], inplace=True)\n",
    "    data.drop_duplicates(subset=['title', 'writer'], keep='first', inplace=True)\n",
    "    data.drop_duplicates(subset=['title', 'content'], keep='first', inplace=True)\n",
    "    data = data[data.content.apply(isLonger, args=[150])]\n",
    "    data = data[~data.content.apply(isObituary)]\n",
    "    data = data[~data.title.str.contains('\\[인사\\]')]\n",
    "    data = data[~data.title.str.contains('\\[.*부고.*\\]')]\n",
    "    data = data[~data.title.str.contains('\\[포토뉴스\\]')]\n",
    "    data = data[~data.title.str.contains('코스피') | data.content.apply(isLonger, args=[300])]\n",
    "    data = data[~data.title.str.contains('증시') | data.content.apply(isLonger, args=[300])]\n",
    "    print('처리 후 기사 수 :', len(data))\n",
    "    data['tokens'] = data.content.apply(mecab_tokenizer)\n",
    "    return data\n",
    "\n",
    "def mecab_tokenizer(doc):\n",
    "#     mecab = Mecab('C:/mecab/mecab-ko-dic')\n",
    "    sentences = [sentences.strip() for sentences in doc.split('.')]\n",
    "    pos_list = []\n",
    "    for sent in sentences:\n",
    "        for pos in mecab.pos(sent):\n",
    "            if pos[1][0] in ['N']: # 우선 명사만 토큰화 \n",
    "                if len(pos[0]) > 1:\n",
    "                    if pos[0] not in stopwords:\n",
    "                        pos_list.append(pos[0])\n",
    "    return ' '.join(pos_list)\n",
    "\n",
    "def find_best_lda(train_data, n_jobs=-1):\n",
    "    lda_pipeline = Pipeline([\n",
    "        ('tfidf_vect', TfidfVectorizer(max_df=.9, min_df=2, lowercase=False, use_idf=False)),\n",
    "        ('lda', LatentDirichletAllocation(max_iter=10, random_state=0, learning_method='online'))\n",
    "    ])\n",
    "    \n",
    "    search_params = {\n",
    "        'tfidf_vect__max_features' : [500, 700, 1000],\n",
    "        'tfidf_vect__ngram_range' : [(1,1), (1,2)],\n",
    "        'lda__n_components' : [3, 5, 10]\n",
    "    }\n",
    "    \n",
    "    gs_lda = GridSearchCV(lda_pipeline, param_grid=search_params, n_jobs=n_jobs)\n",
    "    gs_lda = gs_lda.fit(train_data)\n",
    "    # print('best score : {}'.format(gs_lda.best_score_))\n",
    "    # best_parameters = gs_lda.best_estimator_.get_params()\n",
    "    # for params_name in sorted(list(best_parameters.keys())):\n",
    "    #     print('\\t{0}: {1}'.format(params_name, best_parameters[params_name]))\n",
    "    return gs_lda.best_estimator_\n",
    "\n",
    "# lda의 토픽별 문서 인덱스가 담긴 리스트 반환\n",
    "def get_idxs_by_topic(model, train_data, threshold=0.5):\n",
    "    doc_topic_matrix = model.transform(train_data)\n",
    "    idxs_by_topic = []\n",
    "    for i in range(model.get_params()['lda__n_components']):\n",
    "        idxs = []\n",
    "        for j, doc in enumerate(doc_topic_matrix):\n",
    "            max_topic = np.argmax(doc)\n",
    "            max_prob = doc[max_topic]\n",
    "            if max_topic == i and max_prob >= threshold:\n",
    "                idxs.append(j)\n",
    "        idxs_by_topic.append(idxs)\n",
    "    return idxs_by_topic\n",
    "\n",
    "# 서브카테고리별 가장 많이 나타난 토픽에 대한 상위기사 k개를 찾아 딕셔너리 형태로 반환\n",
    "def top_articles_by_category(preprocessed_data, k=1):\n",
    "    print(datetime.now().strftime('%H:%M:%S')+' 카테고리별 기사 추출 시작')\n",
    "    SUB_CATEGORIES = [] # 서브카테고리 목록\n",
    "    top_articles = {}   # 서브카테고리별 대표기사의 제목과 url\n",
    "    for main_category in CATEGORIES:\n",
    "        for sub_category in CATEGORIES[main_category].items():\n",
    "            SUB_CATEGORIES.append(sub_category[0])\n",
    "            top_articles.setdefault(sub_category[0], [])\n",
    "\n",
    "    for sub_category in SUB_CATEGORIES:\n",
    "        category_data = preprocessed_data[preprocessed_data.sub_category==sub_category]\n",
    "        category_content = category_data.content.values\n",
    "        category_tokens = category_data.tokens.values\n",
    "        # 파라미터 튜닝, 토픽 개수 저장\n",
    "        best_model = find_best_lda(category_tokens)\n",
    "        number_of_topic = best_model.get_params()['lda__n_components']\n",
    "\n",
    "        # 토픽별 인덱스 리스트를 가져와 가장 많이 나타난 토픽의 인덱스 찾기\n",
    "        idxs_by_topic = get_idxs_by_topic(best_model, category_tokens, 0.6)\n",
    "        topic_count = list(map(len, idxs_by_topic))\n",
    "        most_topic = np.argmax(topic_count)\n",
    "\n",
    "        # lda 점수가 가장 높은 문서 찾아 제목과 url 저장\n",
    "        most_topic_weights = best_model.transform(category_tokens)[:, most_topic]\n",
    "        top_doc_idx = np.argsort(most_topic_weights)[::-1][:k]\n",
    "        for idx in top_doc_idx:\n",
    "            top_articles[sub_category].append((\n",
    "                        category_data.title.values[idx],\n",
    "                        category_data.link.values[idx],\n",
    "                        summarize(category_content[idx], word_count=20, ratio=.2)\n",
    "        ))\n",
    "        print(datetime.now().strftime('%H:%M:%S')+sub_category+' 추출 완료')\n",
    "    print(top_articles)\n",
    "    return top_articles\n",
    "\n",
    "def main():\n",
    "    preprocessed_data = preprocess(data)\n",
    "\n",
    "    # 각 서브카테고리별로 상위기사 1개에 대해 제목, 링크, 요약을 top_article에 저장\n",
    "    top_article = top_articles_by_category(preprocessed_data, 1)\n",
    "    print (top_article)\n",
    "    return \n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db671762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 전 기사 수 : 6436\n",
      "처리 후 기사 수 : 5989\n",
      "13:36:40 카테고리별 기사 추출 시작\n",
      "13:37:01 금융 추출 완료\n",
      "13:37:26 증권 추출 완료\n",
      "13:38:18 산업/재계 추출 완료\n",
      "13:38:27 중기/벤처 추출 완료\n",
      "13:38:45 부동산 추출 완료\n",
      "13:38:52 글로벌 경제 추출 완료\n",
      "13:39:15 생활경제 추출 완료\n",
      "13:40:32 경제 일반 추출 완료\n",
      "13:40:35 모바일 추출 완료\n",
      "13:40:39 인터넷/SNS 추출 완료\n",
      "13:40:45 통신/뉴미디어 추출 완료\n",
      "13:41:08 IT 일반 추출 완료\n",
      "13:41:09 보안/해킹 추출 완료\n",
      "13:41:12 컴퓨터 추출 완료\n",
      "13:41:16 게임/리뷰 추출 완료\n",
      "13:41:21 과학 일반 추출 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# scrapy crawler 실행하는 함수(파일 경로는 settings에서 설정)\n",
    "# def run_spider():\n",
    "#     process = CrawlerProcess(get_project_settings())\n",
    "#     process.crawl(SPIDER_NAME)\n",
    "#     process.start()\n",
    "\n",
    "# # 크롤링한 데이터 불러오는 함수\n",
    "# def load_data(path):\n",
    "#     data = pd.read_csv(path, encoding='utf-8', dtype=object)\n",
    "#     # print(data.groupby(by=['main_category', 'sub_category']).id.count())\n",
    "#     return data\n",
    "\n",
    "# 사망기사 관련 키워드 두 개 이상 포함 시 삭제\n",
    "\n",
    "def save_articles(top_articles):\n",
    "    fname = ['category', 'title', 'link', 'summary']\n",
    "    with open('./articles.csv', 'w') as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fname)\n",
    "        w.writeheader()\n",
    "        w.writerows(top_articles)\n",
    "        return\n",
    "    \n",
    "    \n",
    "def isObituary(text):\n",
    "    words = ['별세', '장례', '발인', '부고', '친상']\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            count += 1\n",
    "            if count > 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# 주어진 길이보다 긴 경우 True 반환\n",
    "def isLonger(text, length):\n",
    "    if len(text) > length:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "def preprocess(data):\n",
    "    print('처리 전 기사 수 :', len(data))\n",
    "    data.dropna(subset=['content'], inplace=True)\n",
    "    data.drop_duplicates(subset=['title', 'writer'], keep='first', inplace=True)\n",
    "    data.drop_duplicates(subset=['title', 'content'], keep='first', inplace=True)\n",
    "    data = data[data.content.apply(isLonger, args=[150])]\n",
    "    data = data[~data.content.apply(isObituary)]\n",
    "    data = data[~data.title.str.contains('\\[인사\\]')]\n",
    "    data = data[~data.title.str.contains('\\[.*부고.*\\]')]\n",
    "    data = data[~data.title.str.contains('\\[포토뉴스\\]')]\n",
    "    data = data[~data.title.str.contains('코스피') | data.content.apply(isLonger, args=[300])]\n",
    "    data = data[~data.title.str.contains('증시') | data.content.apply(isLonger, args=[300])]\n",
    "    print('처리 후 기사 수 :', len(data))\n",
    "    data['tokens'] = data.content.apply(mecab_tokenizer)\n",
    "    return data\n",
    "\n",
    "def mecab_tokenizer(doc):\n",
    "#     mecab = Mecab('C:/mecab/mecab-ko-dic')\n",
    "    sentences = [sentences.strip() for sentences in doc.split('.')]\n",
    "    pos_list = []\n",
    "    for sent in sentences:\n",
    "        for pos in mecab.pos(sent):\n",
    "            if pos[1][0] in ['N']: # 우선 명사만 토큰화 \n",
    "                if len(pos[0]) > 1:\n",
    "                    if pos[0] not in stopwords:\n",
    "                        pos_list.append(pos[0])\n",
    "    return ' '.join(pos_list)\n",
    "\n",
    "def find_best_lda(train_data, n_jobs=-1):\n",
    "    lda_pipeline = Pipeline([\n",
    "        ('tfidf_vect', TfidfVectorizer(max_df=.9, min_df=2, lowercase=False, use_idf=False)),\n",
    "        ('lda', LatentDirichletAllocation(max_iter=10, random_state=0, learning_method='online'))\n",
    "    ])\n",
    "    \n",
    "    search_params = {\n",
    "        'tfidf_vect__max_features' : [500, 700, 1000],\n",
    "        'tfidf_vect__ngram_range' : [(1,1), (1,2)],\n",
    "        'lda__n_components' : [3, 5, 10]\n",
    "    }\n",
    "    \n",
    "    gs_lda = GridSearchCV(lda_pipeline, param_grid=search_params, n_jobs=n_jobs)\n",
    "    gs_lda = gs_lda.fit(train_data)\n",
    "    # print('best score : {}'.format(gs_lda.best_score_))\n",
    "    # best_parameters = gs_lda.best_estimator_.get_params()\n",
    "    # for params_name in sorted(list(best_parameters.keys())):\n",
    "    #     print('\\t{0}: {1}'.format(params_name, best_parameters[params_name]))\n",
    "    return gs_lda.best_estimator_\n",
    "\n",
    "# lda의 토픽별 문서 인덱스가 담긴 리스트 반환\n",
    "def get_idxs_by_topic(model, train_data, threshold=0.5):\n",
    "    doc_topic_matrix = model.transform(train_data)\n",
    "    idxs_by_topic = []\n",
    "    for i in range(model.get_params()['lda__n_components']):\n",
    "        idxs = []\n",
    "        for j, doc in enumerate(doc_topic_matrix):\n",
    "            max_topic = np.argmax(doc)\n",
    "            max_prob = doc[max_topic]\n",
    "            if max_topic == i and max_prob >= threshold:\n",
    "                idxs.append(j)\n",
    "        idxs_by_topic.append(idxs)\n",
    "    return idxs_by_topic\n",
    "\n",
    "# 서브카테고리별 가장 많이 나타난 토픽에 대한 상위기사 k개를 찾아 딕셔너리 형태로 반환\n",
    "def top_articles_by_category(preprocessed_data, k=1):\n",
    "    print(datetime.now().strftime('%H:%M:%S')+' 카테고리별 기사 추출 시작')\n",
    "    SUB_CATEGORIES = [] # 서브카테고리 목록\n",
    "    top_articles = []   # 서브카테고리별 대표기사의 제목과 url\n",
    "    for main_category in CATEGORIES:\n",
    "        for sub_category in CATEGORIES[main_category].items():\n",
    "            SUB_CATEGORIES.append(sub_category[0])\n",
    "            # top_articles.setdefault(sub_category[0], [])\n",
    "\n",
    "    for sub_category in SUB_CATEGORIES:\n",
    "        category_data = preprocessed_data[preprocessed_data.sub_category==sub_category]\n",
    "        category_content = category_data.content.values\n",
    "        category_tokens = category_data.tokens.values\n",
    "        # 파라미터 튜닝, 토픽 개수 저장\n",
    "        best_model = find_best_lda(category_tokens)\n",
    "        number_of_topic = best_model.get_params()['lda__n_components']\n",
    "\n",
    "        # 토픽별 인덱스 리스트를 가져와 가장 많이 나타난 토픽의 인덱스 찾기\n",
    "        idxs_by_topic = get_idxs_by_topic(best_model, category_tokens, 0.6)\n",
    "        topic_count = list(map(len, idxs_by_topic))\n",
    "        most_topic = np.argmax(topic_count)\n",
    "\n",
    "        # lda 점수가 가장 높은 문서 찾아 제목과 url 저장\n",
    "        most_topic_weights = best_model.transform(category_tokens)[:, most_topic]\n",
    "        top_doc_idx = np.argsort(most_topic_weights)[::-1][:k]\n",
    "        for idx in top_doc_idx:\n",
    "            top_articles.append({\n",
    "                                'category' : sub_category,\n",
    "                        'title' : category_data.title.values[idx],\n",
    "                        'link' : category_data.link.values[idx],\n",
    "                                'summary' : summarize(category_content[idx], word_count=20, ratio=.2)\n",
    "            })\n",
    "        print(datetime.now().strftime('%H:%M:%S'), sub_category, '추출 완료')\n",
    "\n",
    "    return top_articles\n",
    "\n",
    "def main():\n",
    "    preprocessed_data = preprocess(data)\n",
    "\n",
    "    # 각 서브카테고리별로 상위기사 1개에 대해 제목, 링크, 요약을 top_article에 저장\n",
    "    top_article = top_articles_by_category(preprocessed_data, 1)\n",
    "    save_articles(top_article)\n",
    "    return \n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "656a6365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0406_word2vec_구현.ipynb\r\n",
      "README.md\r\n",
      "Untitled.ipynb\r\n",
      "articles.csv\r\n",
      "k_means_practice.ipynb\r\n",
      "kdt_lesson_learned_pca.ipynb\r\n",
      "kdt_ll_Apr_4th_RPA.ipynb\r\n",
      "kdt_nlp_project_notebook_and_lesson_learned.ipynb\r\n",
      "lesson_learned_kdt_NLP_0320.ipynb\r\n",
      "lesson_learned_kdt_단어의_표현_0321.ipynb\r\n",
      "lesson_learned_kdt_알고리즘&자료구조_1일차_and_2일차.ipynb\r\n",
      "lesson_learned_kdt_알고리즘&자료구조_3일차.ipynb\r\n",
      "news_4_project.ipynb\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96b7df0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추경호 부총리 겸 기획재정부 장관(오른쪽)이 12일(현지시간) 미국 국제통화기금(IMF)에서 국제신용평가사 피치(Fitch)의 제임스 맥코맥 국가신용등급 글로벌총괄과 악수하며 인사를 나누고 있다.\n"
     ]
    }
   ],
   "source": [
    "temp = pd.read_csv('./articles.csv')\n",
    "print(temp['summary'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
