{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "Dt1pO_RVqwNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 임의로 데이터 개수 줄임 --\n",
        "# -- 토큰나이징 하는데 시간이 너무 오래 걸림 --\n",
        "\n",
        "# counts = None\n",
        "# # cat_id_over_2000 = list()\n",
        "# # cat_id_under_2000 = list()\n",
        "# final_df = pd.DataFrame()\n",
        "\n",
        "# for cat_id in df['cat_id'].unique():\n",
        "#     counts = df['cat_id'].value_counts()[df['cat_id'].value_counts().index == cat_id][0]\n",
        "#     if counts > 2000:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id].sample(2000)\n",
        "#     else:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id]\n",
        "#     final_df = pd.concat([tmp_df, final_df])\n",
        "\n",
        "# print(final_df.shape)\n",
        "# final_df.to_csv('/content/drive/MyDrive/reduced_bungae_df.csv')"
      ],
      "metadata": {
        "id": "caq77Bxj4Al0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 데이터 개수 한번 더 줄임 --\n",
        "# -- gpu가 감당하지 못하는 데이터 양인 것 같아서 거의 1/10 수준으로 줄임 --\n",
        "# counts = None\n",
        "# # cat_id_over_2000 = list()\n",
        "# # cat_id_under_2000 = list()\n",
        "# final_df = pd.DataFrame()\n",
        "\n",
        "# for cat_id in df['cat_id'].unique():\n",
        "#     counts = df['cat_id'].value_counts()[df['cat_id'].value_counts().index == cat_id][0]\n",
        "#     if counts > 600:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id].sample(600)\n",
        "#     else:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id]\n",
        "#     final_df = pd.concat([tmp_df, final_df], ignore_index=True)\n",
        "\n",
        "# print(final_df.shape)\n",
        "# final_df.to_csv('/content/drive/MyDrive/reduced_twice_bungae_df.csv')"
      ],
      "metadata": {
        "id": "hQj3UJy9tsl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- google drive에 연결 --\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # -- mecab 설치 --\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "!pip3 install mecab-python3\n",
        "!pip install konlpy\n",
        "!pip install transformers\n",
        "!pip install apex"
      ],
      "metadata": {
        "id": "s-2JnSZscDNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 필요한 함수들 --\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "def bring_and_preprocess_df():\n",
        "    # -- 상대적으로 개수가 적은 csv 파일 사용 -- \n",
        "    df = pd.read_csv('/content/drive/MyDrive/reduced_twice_bungae_df.csv', encoding='utf-8-sig')\n",
        "    df['cat_id'] = df['cat_id'].astype(int).astype(str)\n",
        "\n",
        "    # -- 패션과 관련 없는 카테고리 제거 --\n",
        "    idx_list = list()\n",
        "    for idx in range(len(df)):\n",
        "        if (len(df.loc[idx, 'cat_id']) == 3) | (df.loc[idx, 'cat_id'][0] not in ['3', '4']): \n",
        "            idx_list.append(idx)\n",
        "\n",
        "    df = df[~df.index.isin(idx_list)]\n",
        "    # -- 카테고리당 상품이 1~2개 있을 경우 train_test_split 했을 때 갯수가 너무 적어 안됨, 이후에 수정 필요 --\n",
        "    df = df[~df['cat_id'].isin(['310060999', '310220999', '320180500'])]\n",
        "    \n",
        "    df = df.dropna(axis=0)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        return text, label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    indexed_texts = [torch.LongTensor([word2index.get(token, 0) for token in text]) for text in texts]\n",
        "    padded_texts = torch.nn.utils.rnn.pad_sequence(indexed_texts, batch_first=True)\n",
        "    padded_labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(label) for label in labels], batch_first=True)\n",
        "    return padded_texts, padded_labels\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     texts, labels = zip(*batch)\n",
        "#     text_lengths = [len(text) for text in texts]\n",
        "#     max_length = max(text_lengths)\n",
        "\n",
        "#     texts = [[word2index.get(token, 0) for token in tokenizer(text)] for text in texts]\n",
        "#     indexed_texts = [torch.LongTensor(text) for text in texts]\n",
        "#     labels = torch.LongTensor(labels)\n",
        "#     return indexed_texts, labels\n",
        "\n",
        "def remove_punct(text):\n",
        "    \"\"\"\n",
        "    문장기호 없애는 함수\n",
        "    \"\"\"\n",
        "    remove_punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
        "    text = text.lower().translate(remove_punct_dict)\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    mecab = Mecab()\n",
        "    tokens = mecab.morphs(text)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "4lRn8bvibdtR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- data --\n",
        "df = bring_and_preprocess_df()\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_ZZK39LdhzZ",
        "outputId": "8b9edd17-f845-4150-a7de-1e9378510486"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100169 entries, 0 to 100168\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   Unnamed: 0.1  100169 non-null  int64  \n",
            " 1   Unnamed: 0    100169 non-null  int64  \n",
            " 2   product_id    100169 non-null  int64  \n",
            " 3   product_name  100169 non-null  object \n",
            " 4   image_url     100169 non-null  object \n",
            " 5   image_cnt     100169 non-null  float64\n",
            " 6   cat_id        100169 non-null  object \n",
            "dtypes: float64(1), int64(3), object(3)\n",
            "memory usage: 5.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- preprocessing --\n",
        "\n",
        "# -- label encoding --\n",
        "# encoder = LabelEncoder()\n",
        "# df['label'] = encoder.fit_transform(df['cat_id']) # 이후에 매칭 다시 필요 \n",
        "\n",
        "# -- removing unnecessary texts --\n",
        "df['product_name'] = df['product_name'].apply(lambda x : remove_punct(x))"
      ],
      "metadata": {
        "id": "KY37_ZV0dkYW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess your own data\n",
        "texts = list(df['product_name'])\n",
        "labels = df['cat_id'].values # 뒤에서 label encoder 다시 사용\n",
        "\n",
        "# Instantiate tokenizer\n",
        "# tokenizer = Mecab()\n",
        "\n",
        "# word2index\n",
        "def get_word2index(texts, max_vocab_size=None):\n",
        "    def yield_tokens(texts):\n",
        "        for text in texts:\n",
        "            yield tokenizer(text)\n",
        "    \n",
        "    vocab = build_vocab_from_iterator(yield_tokens(texts))\n",
        "    word2index = vocab.get_stoi()\n",
        "\n",
        "    if max_vocab_size is not None:\n",
        "        word2index = {k: v for k, v in word2index.items() if v < max_vocab_size}\n",
        "    \n",
        "    return word2index\n",
        "\n",
        "max_vocab_size = 10000 # 바꿔야 될수도?\n",
        "word2index = get_word2index(texts, max_vocab_size)\n"
      ],
      "metadata": {
        "id": "cS_LBznhcd-0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test dataset\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "texts_train = [tokenizer(text) for text in texts_train]\n",
        "texts_test = [tokenizer(text) for text in texts_test]\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_train = label_encoder.fit_transform(labels_train)\n",
        "labels_test = label_encoder.transform(labels_test) # 여기서 label_train에 없는 클래스가 있는 경우 오류가 남. \n"
      ],
      "metadata": {
        "id": "MBo-X2lAt5tJ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train \n",
        "batch_size = 32\n",
        "train_dataset = CustomDataset(texts_train, labels_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "# test\n",
        "test_dataset = CustomDataset(texts_test, labels_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
      ],
      "metadata": {
        "id": "ACa4QGEGSQnq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained embeddings \n",
        "embedding_file = '/content/drive/MyDrive/wiki.ko.vec'\n",
        "pretrained_embeddings = torchtext.vocab.Vectors(embedding_file)\n",
        "\n",
        "# vocab = Vocab(word2index, vectors=pretrained_embeddings)\n",
        "\n",
        "# parameter\n",
        "vocab_size = len(word2index)\n",
        "num_classes = 170\n",
        "\n",
        "embedding_dim = pretrained_embeddings.dim\n",
        "embedding = nn.Embedding.from_pretrained(pretrained_embeddings.vectors, padding_idx=0)\n",
        "\n",
        "\n",
        "class FastText(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes, embeddings):\n",
        "        super(FastText, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(embeddings, requires_grad=False)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs).permute(1, 0, 2)\n",
        "        pooled = torch.mean(embedded, dim=1)\n",
        "        output = self.fc(pooled)\n",
        "        return output\n",
        "\n",
        "# -- 계속 int is not callable 문제가 떴을 때 해결 방법 --\n",
        "embedding = torch.FloatTensor(pretrained_embeddings.vectors)\n",
        "\n",
        "model = FastText(vocab_size, embedding_dim, num_classes, embedding)"
      ],
      "metadata": {
        "id": "wSo3f0D-fy4H"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- training --\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "accuracy_history = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_inputs, batch_labels in train_dataloader:\n",
        "\n",
        "        batch_size = batch_inputs.size(0)  # Get the actual batch size\n",
        "\n",
        "        if batch_size != batch_inputs.size(0):\n",
        "            continue  \n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        acc = (predicted == batch_labels).sum().item() / batch_size\n",
        "        print(f\"Accuracy: {acc}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_inputs, batch_labels in test_dataloader:\n",
        "            # move data to GPU\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_inputs)\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "        \n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "-rex5932bd4Y",
        "outputId": "19d1a36f-06b4-4a81-8313-fd9fa58e9e69"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-54e11ecc81f5>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the actual batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-2880c06bc405>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mindexed_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpadded_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mpadded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# assuming trailing dimensions and type of all the Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;31m# in sequences are same and fetching those from sequences[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension specified as 0 but tensor has no dimensions"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KfjxIrwGYi_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}