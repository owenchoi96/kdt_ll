{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "Dt1pO_RVqwNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 임의로 데이터 개수 줄임 --\n",
        "# -- 토큰나이징 하는데 시간이 너무 오래 걸림 --\n",
        "\n",
        "# counts = None\n",
        "# # cat_id_over_2000 = list()\n",
        "# # cat_id_under_2000 = list()\n",
        "# final_df = pd.DataFrame()\n",
        "\n",
        "# for cat_id in df['cat_id'].unique():\n",
        "#     counts = df['cat_id'].value_counts()[df['cat_id'].value_counts().index == cat_id][0]\n",
        "#     if counts > 2000:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id].sample(2000)\n",
        "#     else:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id]\n",
        "#     final_df = pd.concat([tmp_df, final_df])\n",
        "\n",
        "# print(final_df.shape)\n",
        "# final_df.to_csv('/content/drive/MyDrive/reduced_bungae_df.csv')"
      ],
      "metadata": {
        "id": "caq77Bxj4Al0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 데이터 개수 한번 더 줄임 --\n",
        "# -- gpu가 감당하지 못하는 데이터 양인 것 같아서 거의 1/10 수준으로 줄임 --\n",
        "# counts = None\n",
        "# # cat_id_over_2000 = list()\n",
        "# # cat_id_under_2000 = list()\n",
        "# final_df = pd.DataFrame()\n",
        "\n",
        "# for cat_id in df['cat_id'].unique():\n",
        "#     counts = df['cat_id'].value_counts()[df['cat_id'].value_counts().index == cat_id][0]\n",
        "#     if counts > 600:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id].sample(600)\n",
        "#     else:\n",
        "#         tmp_df = df[df['cat_id'] == cat_id]\n",
        "#     final_df = pd.concat([tmp_df, final_df], ignore_index=True)\n",
        "\n",
        "# print(final_df.shape)\n",
        "# final_df.to_csv('/content/drive/MyDrive/reduced_twice_bungae_df.csv')"
      ],
      "metadata": {
        "id": "hQj3UJy9tsl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- google drive에 연결 --\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # -- mecab 설치 --\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "!pip3 install mecab-python3\n",
        "!pip install konlpy\n",
        "!pip install transformers\n",
        "!pip install apex"
      ],
      "metadata": {
        "id": "s-2JnSZscDNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 필요한 함수들 --\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "def bring_and_preprocess_df():\n",
        "    # -- 상대적으로 개수가 적은 csv 파일 사용 -- \n",
        "    df = pd.read_csv('/content/drive/MyDrive/reduced_twice_bungae_df.csv', encoding='utf-8-sig')\n",
        "    df['cat_id'] = df['cat_id'].astype(int).astype(str)\n",
        "\n",
        "    # -- 패션과 관련 없는 카테고리 제거 --\n",
        "    idx_list = list()\n",
        "    for idx in range(len(df)):\n",
        "        if (len(df.loc[idx, 'cat_id']) == 3) | (df.loc[idx, 'cat_id'][0] not in ['3', '4']): \n",
        "            idx_list.append(idx)\n",
        "\n",
        "    df = df[~df.index.isin(idx_list)]\n",
        "    df = df.dropna(axis=0)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        return text, label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    text_lengths = [len(text) for text in texts]\n",
        "    indexed_texts = [torch.LongTensor([word2index.get(token, 0) for token in tokenizer(text)]) for text in texts]\n",
        "    padded_texts = torch.nn.utils.rnn.pad_sequence(indexed_texts, batch_first=True)\n",
        "    return padded_texts, torch.LongTensor(labels)\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     texts, labels = zip(*batch)\n",
        "#     text_lengths = [len(text) for text in texts]\n",
        "#     max_length = max(text_lengths)\n",
        "\n",
        "#     texts = [[word2index.get(token, 0) for token in tokenizer(text)] for text in texts]\n",
        "#     indexed_texts = [torch.LongTensor(text) for text in texts]\n",
        "#     labels = torch.LongTensor(labels)\n",
        "#     return indexed_texts, labels\n",
        "\n",
        "def remove_punct(text):\n",
        "    \"\"\"\n",
        "    문장기호 없애는 함수\n",
        "    \"\"\"\n",
        "    remove_punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
        "    text = text.lower().translate(remove_punct_dict)\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    mecab = Mecab()\n",
        "    tokens = mecab.morphs(text)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "4lRn8bvibdtR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- data --\n",
        "df = bring_and_preprocess_df()\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_ZZK39LdhzZ",
        "outputId": "0e8b051a-5747-4522-d1da-4983a87115df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100173 entries, 0 to 100172\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   Unnamed: 0.1  100173 non-null  int64  \n",
            " 1   Unnamed: 0    100173 non-null  int64  \n",
            " 2   product_id    100173 non-null  int64  \n",
            " 3   product_name  100173 non-null  object \n",
            " 4   image_url     100173 non-null  object \n",
            " 5   image_cnt     100173 non-null  float64\n",
            " 6   cat_id        100173 non-null  object \n",
            "dtypes: float64(1), int64(3), object(3)\n",
            "memory usage: 5.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- preprocessing --\n",
        "\n",
        "# -- label encoding --\n",
        "encoder = LabelEncoder()\n",
        "df['label'] = encoder.fit_transform(df['cat_id']) # 이후에 매칭 다시 필요 \n",
        "\n",
        "# -- removing unnecessary texts --\n",
        "df['product_name'] = df['product_name'].apply(lambda x : remove_punct(x))"
      ],
      "metadata": {
        "id": "KY37_ZV0dkYW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess your own data\n",
        "texts = list(df['product_name'])\n",
        "labels = df['label'].values\n",
        "\n",
        "# Instantiate tokenizer\n",
        "# tokenizer = Mecab()\n",
        "\n",
        "# word2index\n",
        "def get_word2index(texts, max_vocab_size=None):\n",
        "    def yield_tokens(texts):\n",
        "        for text in texts:\n",
        "            yield tokenizer(text)\n",
        "    \n",
        "    vocab = build_vocab_from_iterator(yield_tokens(texts))\n",
        "    word2index = vocab.get_stoi()\n",
        "\n",
        "    if max_vocab_size is not None:\n",
        "        word2index = {k: v for k, v in word2index.items() if v < max_vocab_size}\n",
        "    \n",
        "    return word2index\n",
        "\n",
        "max_vocab_size = 10000 \n",
        "word2index = get_word2index(texts, max_vocab_size)\n"
      ],
      "metadata": {
        "id": "cS_LBznhcd-0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test dataset\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# train \n",
        "batch_size = 8\n",
        "train_dataset = CustomDataset(texts_train, labels_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "# test\n",
        "test_dataset = CustomDataset(texts_test, labels_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
      ],
      "metadata": {
        "id": "MBo-X2lAt5tJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained embeddings \n",
        "embedding_file = '/content/drive/MyDrive/wiki.ko.vec'\n",
        "pretrained_embeddings = torchtext.vocab.Vectors(embedding_file)\n",
        "\n",
        "# vocab = Vocab(word2index, vectors=pretrained_embeddings)\n",
        "\n",
        "vocab_size, embedding_dim = pretrained_embeddings.vectors.shape\n",
        "num_classes = 170\n",
        "\n",
        "# -- defining FastText -- \n",
        "class FastText(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes, embeddings):\n",
        "        super(FastText, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embeddings)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        pooled = embedded.mean(dim=1)\n",
        "        output = self.fc(pooled)\n",
        "        return output\n",
        "\n",
        "# -- 계속 int is not callable 문제가 떴을 때 해결 방법 --\n",
        "pretrained_embeddings = torch.FloatTensor(pretrained_embeddings.vectors)\n",
        "\n",
        "model = FastText(vocab_size, embedding_dim, num_classes, pretrained_embeddings)"
      ],
      "metadata": {
        "id": "wSo3f0D-fy4H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- optional --\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "uAEiKsEGlmdQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- training --\n",
        "from sklearn.metrics import accuracy_score\n",
        "from apex import amp\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "accuracy_history = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_inputs, batch_labels in train_dataloader:\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        # loss.backward()\n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "            scaled_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        acc = accuracy_score(predicted.cpu(), batch_labels.cpu())\n",
        "        print(f\"Accuracy: {acc}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_inputs, batch_labels in test_dataloader:\n",
        "            # move data to GPU\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_inputs)\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "        \n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "-rex5932bd4Y",
        "outputId": "6f581cf4-42dc-4c67-b9f5-d6e99420d445"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/apex/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0d59cd288377>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'O1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/apex/apex/amp/frontend.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(models, optimizers, enabled, opt_level, cast_model_type, patch_torch_functions, keep_batchnorm_fp32, master_weights, loss_scale, cast_model_outputs, num_losses, verbosity, min_loss_scale, max_loss_scale)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mmaybe_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:22} : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_amp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_model_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/apex/apex/amp/_initialize.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(models, optimizers, properties, num_losses, cast_model_outputs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0m_amp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scalers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         _amp_state.loss_scalers.append(LossScaler(properties.loss_scale,\n\u001b[0m\u001b[1;32m    232\u001b[0m                                                   \u001b[0mmin_loss_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_amp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_loss_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                                   max_loss_scale=_amp_state.max_loss_scale))\n",
            "\u001b[0;32m/content/apex/apex/amp/scaler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loss_scale, init_scale, scale_factor, scale_window, min_loss_scale, max_loss_scale)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unskipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_overflow_buf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_tensor_applier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mamp_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from apex import amp"
      ],
      "metadata": {
        "id": "_z6mw20-1e01"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/apex.git\n",
        "%cd apex\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "metadata": {
        "id": "QJ6XkYjP1EER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}