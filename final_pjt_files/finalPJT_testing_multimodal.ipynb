{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# pre works"
      ],
      "metadata": {
        "id": "4VGOT0M5RyQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXuxmhxb65Yp"
      },
      "outputs": [],
      "source": [
        "# -- 필요한 라이브러리 설치 --\n",
        "!pip install transformers\n",
        "!pip install Keras-Preprocessing\n",
        "# !pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- import --\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import ElectraForSequenceClassification, AutoTokenizer, AdamW, ElectraTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "# GPU 사용\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "dU_48D1z68NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -- making temporary df to test multimodal --\n",
        "# df_for_multimodal = pd.DataFrame()\n",
        "# for cat_id in df['cat_id'].unique():\n",
        "#     df_for_multimodal = pd.concat([df_for_multimodal, df[df['cat_id'] == cat_id].head(1)])\n",
        "# df_for_multimodal = df_for_multimodal.reset_index(drop=True)\n",
        "# df_for_multimodal.to_csv('/content/drive/MyDrive/df_for_multimodal.csv', index=False)\n",
        "\n",
        "# -- 필요 데이터 --\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# -- 간단한 테스트용 170개 row 데이터 --\n",
        "# df = pd.read_csv('/content/drive/MyDrive/df_for_multimodal.csv')\n",
        "# df['product_name'] = df['product_name'].apply(lambda x : re.sub('[^가-힣\\sa-zA-Z]', '', x))\n",
        "# df.info()\n",
        "\n",
        "# -- 실제 이미지 크롤링 할 때 사용한 파일 --\n",
        "df = pd.read_csv('/content/drive/MyDrive/bungae_df_for_image_crawling.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anUoQpej68UY",
        "outputId": "3d103b75-5195-439c-aff0-c87825eadf77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100169 entries, 0 to 100168\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   product_id    100169 non-null  int64  \n",
            " 1   product_name  100169 non-null  object \n",
            " 2   image_url     100169 non-null  object \n",
            " 3   image_cnt     100169 non-null  float64\n",
            " 4   cat_id        100169 non-null  object \n",
            " 5   main_cat      100169 non-null  object \n",
            " 6   mid_cat       100169 non-null  object \n",
            "dtypes: float64(1), int64(1), object(5)\n",
            "memory usage: 5.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 다시 필요한 작업 --\n",
        "# 1. 이미지를 크롤링 하면서 사라진 것들이 있음. 이미지가 없는 row는 삭제해주기\n",
        "# 2. 메인 카테고리로 할 것이기 때문에 이미지 폴더 대분류 기준으로 다시 만들고 이미지들도 그에 맞게 옮겨주기\n",
        "\n",
        "# --> 주피터 노트북에서 작업"
      ],
      "metadata": {
        "id": "xoUoaEC3uynG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----- main below ------"
      ],
      "metadata": {
        "id": "esrQA2IOuoXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal test model (Fasttext + MobileNet2) -> failed"
      ],
      "metadata": {
        "id": "DTHEQwvoRl7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- model --\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import mobilenet_v2\n",
        "import fasttext\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gensim\n",
        "\n",
        "\n",
        "# Define the multi-modal model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        # Load the FastText word embedding model\n",
        "        self.word_embedding = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/wiki.ko.vec', binary=False)\n",
        "\n",
        "        # Load the MobileNetV2 pre-trained model\n",
        "        self.image_model = mobilenet_v2(pretrained=True)\n",
        "        self.image_model.classifier = nn.Identity()\n",
        "\n",
        "        # Text processing layers\n",
        "        self.text_fc = nn.Linear(300, 128)\n",
        "\n",
        "        # Image processing layers\n",
        "        self.image_fc = nn.Linear(1280, 128)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, texts, images):\n",
        "        # Text processing\n",
        "        if isinstance(texts, tuple):\n",
        "            # Handle batch input\n",
        "            text_tokens = texts[0].split()\n",
        "            text_embedding = torch.mean(\n",
        "                torch.stack([torch.from_numpy(self.word_embedding.get_vector(token))\n",
        "                            if token in self.word_embedding.key_to_index\n",
        "                            else torch.zeros(300)\n",
        "                            for token in text_tokens]), dim=0)\n",
        "        else:\n",
        "            # Handle single input\n",
        "            text_tokens = texts.split()\n",
        "            text_embedding = torch.mean(\n",
        "                torch.stack([torch.from_numpy(self.word_embedding.get_vector(token))\n",
        "                            if token in self.word_embedding.key_to_index\n",
        "                            else torch.zeros(300)\n",
        "                            for token in text_tokens]), dim=0)\n",
        "\n",
        "        text_output = F.relu(self.text_fc(text_embedding))\n",
        "\n",
        "        # Image processing\n",
        "        image_output = self.image_model(images)\n",
        "\n",
        "        # Combine modalities\n",
        "        combined = torch.cat((text_output, image_output), dim=1)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(combined)\n",
        "        return output\n",
        "\n",
        "# Custom Dataset class\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, texts, images, labels):\n",
        "        self.texts = texts\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "        return text, image, label\n"
      ],
      "metadata": {
        "id": "eB_uX2uW9N_r"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- text data --\n",
        "texts = list(df['product_name'])\n",
        "\n",
        "# -- image paths --\n",
        "\n",
        "images = list()\n",
        "for idx in range(len(df)):\n",
        "    cat_id = df.loc[idx, 'cat_id']\n",
        "    prd_id = df.loc[idx, 'product_id']\n",
        "    image_path = f'data/drive/MyDrive/bungae_fashion_image/{cat_id}/{prd_id}_image.jpg'\n",
        "    images.append(image_path)\n",
        "\n",
        "# -- label --\n",
        "encoder = LabelEncoder()\n",
        "df['label'] = encoder.fit_transform(df['cat_id'])\n",
        "labels = torch.tensor(list(df['label']))\n",
        "\n",
        "# Create the multi-modal dataset and data loader\n",
        "dataset = MultiModalDataset(texts, images, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "zuihJs2C-dDW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "num_classes = 170\n",
        "model = MultiModalModel(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8jQdg5t-gHd",
        "outputId": "d4ebc8be-1a08-465e-bc8e-0337ae1b15b1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for texts, images, labels in dataloader:\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(texts, images)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the loss for monitoring\n",
        "        print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "zX85ScG4AasY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal test model (Koelectra + MobileNet2) -> working"
      ],
      "metadata": {
        "id": "_IGgxicSRe7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ElectraTokenizer, ElectraModel\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, text_data, image_data, labels, text_tokenizer):\n",
        "        self.text_data = text_data\n",
        "        self.image_data = image_data\n",
        "        self.labels = labels\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        image = Image.open(self.image_data[idx]).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return text, image, label\n",
        "\n",
        "# Define your data examples (text, image paths, and labels)\n",
        "text_data = list(df['product_name'])\n",
        "\n",
        "image_data = list()\n",
        "for idx in range(len(df)):\n",
        "    cat_id = df.loc[idx, 'cat_id']\n",
        "    prd_id = df.loc[idx, 'product_id']\n",
        "    image_path = f'data/drive/MyDrive/bungae_fashion_image/{cat_id}/{prd_id}_image.jpg'\n",
        "    image_data.append(image_path)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['label'] = encoder.fit_transform(df['cat_id'])\n",
        "labels = df['label']\n",
        "\n",
        "# Initialize the text tokenizer\n",
        "text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "\n",
        "# Create the custom dataset\n",
        "dataset = MultiModalDataset(text_data, image_data, labels, text_tokenizer)\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 8\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the multi-modal model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        # Text Model (KoElectra)\n",
        "        self.text_model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "        self.text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "        # Image Model (MobileNetV2)\n",
        "        self.image_model = models.mobilenet_v2(pretrained=True)\n",
        "        self.image_model.classifier = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # Fusion Layer\n",
        "        fusion_dim = self.text_model.config.hidden_size + self.image_model.last_channel\n",
        "        self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "    def forward(self, text_inputs, image_inputs):\n",
        "        # Text Encoding\n",
        "        text_input_ids = self.text_tokenizer.batch_encode_plus(\n",
        "            text_inputs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        text_outputs = self.text_model(input_ids=text_input_ids)[0][:, 0, :]  # Use the CLS token embedding\n",
        "\n",
        "        # Image Encoding\n",
        "        image_outputs = self.image_model.features(image_inputs)\n",
        "        image_outputs = torch.nn.functional.adaptive_avg_pool2d(image_outputs, 1).reshape(image_outputs.size(0), -1)\n",
        "\n",
        "        # Fusion\n",
        "        fusion_inputs = torch.cat((text_outputs, image_outputs), dim=1)\n",
        "        fused_outputs = self.fusion_layer(fusion_inputs)\n",
        "\n",
        "        # Output\n",
        "        logits = self.output_layer(fused_outputs)\n",
        "\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "bGkHIid3qgRm"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the multi-modal model\n",
        "num_classes = 170  # Specify the number of output classes\n",
        "model = MultiModalModel(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for texts, images, labels in data_loader:\n",
        "        texts = list(texts)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(texts, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the loss for every epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# After training, you can use the model for predictions on new data"
      ],
      "metadata": {
        "id": "0deyXFUVwGY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal (KoElectra + MobileNet2) -> train, val, and test"
      ],
      "metadata": {
        "id": "eaEMLVbwR_c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 필요한 라이브러리 설치 --\n",
        "!pip install transformers\n",
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "id": "hNU2dxNqDZew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 필요한 데이터 --\n",
        "pd.read_csv('/content/drive/MyDrive/')"
      ],
      "metadata": {
        "id": "VHS_Kji-Dbph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ElectraTokenizer, ElectraModel\n",
        "from torchvision.models import mobilenet_v2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Custom Dataset\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, text_data, image_data, labels, text_tokenizer, transform):\n",
        "        self.text_data = text_data\n",
        "        self.image_data = image_data\n",
        "        self.labels = labels\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        image = Image.open(self.image_data[idx]).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return text, image, label\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(val_data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define your data examples (text, image paths, and labels)\n",
        "text_train_data = list(train_data['product_name'])\n",
        "text_val_data = list(val_data['product_name'])\n",
        "text_test_data = list(test_data['product_name'])\n",
        "\n",
        "image_train_data = list()\n",
        "image_val_data = list()\n",
        "image_test_data = list()\n",
        "for idx in range(len(df)):\n",
        "    cat_id = df.loc[idx, 'cat_id']\n",
        "    prd_id = df.loc[idx, 'product_id']\n",
        "    # -- image_path 조정 필요 --\n",
        "    image_path = f'data/drive/MyDrive/bungae_fashion_image/{cat_id}/{prd_id}_image.jpg'\n",
        "    if idx in train_data.index:\n",
        "        image_train_data.append(image_path)\n",
        "    elif idx in val_data.index:\n",
        "        image_val_data.append(image_path)\n",
        "    else:\n",
        "        image_test_data.append(image_path)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['label'] = encoder.fit_transform(df['cat_id'])\n",
        "labels_train = df.loc[train_data.index, 'label']\n",
        "labels_val = df.loc[val_data.index, 'label']\n",
        "labels_test = df.loc[test_data.index, 'label']"
      ],
      "metadata": {
        "id": "-94aWgDwSHXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text tokenizer\n",
        "text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "\n",
        "# Define transformations for image data\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create the custom datasets\n",
        "train_dataset = MultiModalDataset(text_train_data, image_train_data, labels_train, text_tokenizer, image_transform)\n",
        "val_dataset = MultiModalDataset(text_val_data, image_val_data, labels_val, text_tokenizer, image_transform)\n",
        "test_dataset = MultiModalDataset(text_test_data, image_test_data, labels_test, text_tokenizer, image_transform)\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "3pUXiEuKScqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the multi-modal model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        # Text Model (KoElectra)\n",
        "        self.text_model = ElectraModel.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "        self.text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
        "\n",
        "        # Image Model (MobileNetV2)\n",
        "        self.image_model = mobilenet_v2(pretrained=True)\n",
        "        self.image_model.classifier = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # Fusion Layer\n",
        "        fusion_dim = self.text_model.config.hidden_size + self.image_model.last_channel\n",
        "        self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "    def forward(self, text_inputs, image_inputs):\n",
        "        # Text Encoding\n",
        "        text_input_ids = self.text_tokenizer.batch_encode_plus(\n",
        "            text_inputs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        text_outputs = self.text_model(input_ids=text_input_ids)[0][:, 0, :]  # Use the CLS token embedding\n",
        "\n",
        "        # Image Encoding\n",
        "        image_outputs = self.image_model.features(image_inputs)\n",
        "        image_outputs = torch.nn.functional.adaptive_avg_pool2d(image_outputs, 1).reshape(image_outputs.size(0), -1)\n",
        "\n",
        "        # Fusion\n",
        "        fusion_inputs = torch.cat((text_outputs, image_outputs), dim=1)\n",
        "        fused_outputs = self.fusion_layer(fusion_inputs)\n",
        "\n",
        "        # Output\n",
        "        logits = self.output_layer(fused_outputs)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Iu-QPeCmSocr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the multi-modal model\n",
        "num_classes = 170  # Specify the number of output classes\n",
        "model = MultiModalModel(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for texts, images, labels in train_loader:\n",
        "        texts = list(texts)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(texts, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = 100.0 * train_correct / train_total\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, images, labels in val_loader:\n",
        "            texts = list(texts)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(texts, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100.0 * val_correct / val_total\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print the loss and accuracy for every epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "Fky8OXtaSqJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal with data augumentation and dropout (KoElectra + MobileNet2)"
      ],
      "metadata": {
        "id": "VlKX-l7Ze2m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import ElectraTokenizer, ElectraModel\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, text_data, image_data, labels, text_tokenizer, transform=None):\n",
        "        self.text_data = text_data\n",
        "        self.image_data = image_data\n",
        "        self.labels = labels\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        image = Image.open(self.image_data[idx]).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return text, image, label\n",
        "\n",
        "\n",
        "# Define your data examples (text, image paths, and labels)\n",
        "text_data = list(df['product_name'])\n",
        "\n",
        "image_data = list()\n",
        "for idx in range(len(df)):\n",
        "    cat_id = df.loc[idx, 'cat_id']\n",
        "    prd_id = df.loc[idx, 'product_id']\n",
        "    image_path = f'data/drive/MyDrive/bungae_fashion_image/{cat_id}/{prd_id}_image.jpg'\n",
        "    image_data.append(image_path)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['label'] = encoder.fit_transform(df['cat_id'])\n",
        "labels = df['label']\n",
        "\n",
        "# Initialize the text tokenizer\n",
        "text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "# Define data augmentation transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create the custom dataset with data augmentation\n",
        "dataset = MultiModalDataset(text_data, image_data, labels, text_tokenizer, transform=image_transform)\n"
      ],
      "metadata": {
        "id": "g8YhKqP5e_D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = (len(dataset) - train_size) // 2\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Define data loaders for train, validation, and test sets\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "IXroEykfgK6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the multi-modal model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.5):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        # Text Model (KoElectra)\n",
        "        self.text_model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "        self.text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "        # Image Model (MobileNetV2)\n",
        "        self.image_model = models.mobilenet_v2(pretrained=True)\n",
        "        self.image_model.classifier = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # Fusion Layer\n",
        "        fusion_dim = self.text_model.config.hidden_size + self.image_model.last_channel\n",
        "        self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, text_inputs, image_inputs):\n",
        "        # Text Encoding\n",
        "        text_input_ids = self.text_tokenizer.batch_encode_plus(\n",
        "            text_inputs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        text_outputs = self.text_model(input_ids=text_input_ids)[0][:, 0, :]  # Use the CLS token embedding\n",
        "\n",
        "        # Image Encoding\n",
        "        image_outputs = self.image_model.features(image_inputs)\n",
        "        image_outputs = torch.nn.functional.adaptive_avg_pool2d(image_outputs, 1).reshape(image_outputs.size(0), -1)\n",
        "\n",
        "        # Fusion\n",
        "        fusion_inputs = torch.cat((text_outputs, image_outputs), dim=1)\n",
        "        fused_outputs = self.fusion_layer(fusion_inputs)\n",
        "\n",
        "        # Dropout\n",
        "        fused_outputs = self.dropout(fused_outputs)\n",
        "\n",
        "        # Output\n",
        "        logits = self.output_layer(fused_outputs)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Initialize the multi-modal model\n",
        "num_classes = 170  # Specify the number of output classes\n",
        "model = MultiModalModel(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "nNbOafGogP39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = []\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for texts, images, labels in train_loader:\n",
        "        texts = list(texts)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(texts, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track training loss\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        # Track training accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = sum(train_loss) / len(train_loss)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = 100.0 * train_correct / train_total\n",
        "\n",
        "    # Validation loop\n",
        "    val_loss = []\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, images, labels in val_loader:\n",
        "            texts = list(texts)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(texts, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track validation loss\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            # Track validation accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Calculate average validation loss\n",
        "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = 100.0 * val_correct / val_total\n",
        "\n",
        "    # Print the loss and accuracy for every epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# After training, you can use the model for predictions on new data\n"
      ],
      "metadata": {
        "id": "RyqEfPQ5gVMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with SMOTE"
      ],
      "metadata": {
        "id": "gWR2yi-DTlOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import ElectraTokenizer, ElectraModel\n",
        "from PIL import Image\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, text_data, image_data, labels, text_tokenizer, transform=None):\n",
        "        self.text_data = text_data\n",
        "        self.image_data = image_data\n",
        "        self.labels = labels\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        image = Image.open(self.image_data[idx]).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return text, image, label\n",
        "\n",
        "\n",
        "# Define your data examples (text, image paths, and labels)\n",
        "text_data = list(df['product_name'])\n",
        "\n",
        "image_data = list()\n",
        "for idx in range(len(df)):\n",
        "    cat_id = df.loc[idx, 'cat_id']\n",
        "    prd_id = df.loc[idx, 'product_id']\n",
        "    image_path = f'data/drive/MyDrive/bungae_fashion_image/{cat_id}/{prd_id}_image.jpg'\n",
        "    image_data.append(image_path)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['label'] = encoder.fit_transform(df['cat_id'])\n",
        "labels = df['label']\n",
        "\n",
        "# Apply SMOTE for oversampling\n",
        "oversampler = SMOTE()\n",
        "text_data, image_data, labels = oversampler.fit_resample(text_data, image_data, labels)\n",
        "\n",
        "# Apply RandomUnderSampler for undersampling\n",
        "undersampler = RandomUnderSampler()\n",
        "text_data, image_data, labels = undersampler.fit_resample(text_data, image_data, labels)\n",
        "\n",
        "# Initialize the text tokenizer\n",
        "text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "# Define data augmentation transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create the custom dataset with data augmentation\n",
        "dataset = MultiModalDataset(text_data, image_data, labels, text_tokenizer, transform=image_transform)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = (len(dataset) - train_size) // 2\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Define data loaders for train, validation, and test sets\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the multi-modal model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.5):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        # Text Model (KoElectra)\n",
        "        self.text_model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "        self.text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "        # Image Model (MobileNetV2)\n",
        "        self.image_model = models.mobilenet_v2(pretrained=True)\n",
        "        self.image_model.classifier = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # Fusion Layer\n",
        "        fusion_dim = self.text_model.config.hidden_size + self.image_model.last_channel\n",
        "        self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, text_inputs, image_inputs):\n",
        "        # Text Encoding\n",
        "        text_input_ids = self.text_tokenizer.batch_encode_plus(\n",
        "            text_inputs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        text_outputs = self.text_model(input_ids=text_input_ids)[0][:, 0, :]  # Use the CLS token embedding\n",
        "\n",
        "        # Image Encoding\n",
        "        image_outputs = self.image_model.features(image_inputs)\n",
        "        image_outputs = torch.nn.functional.adaptive_avg_pool2d(image_outputs, 1).reshape(image_outputs.size(0), -1)\n",
        "\n",
        "        # Fusion\n",
        "        fusion_inputs = torch.cat((text_outputs, image_outputs), dim=1)\n",
        "        fused_outputs = self.fusion_layer(fusion_inputs)\n",
        "\n",
        "        # Dropout\n",
        "        fused_outputs = self.dropout(fused_outputs)\n",
        "\n",
        "        # Output\n",
        "        logits = self.output_layer(fused_outputs)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Initialize the multi-modal model\n",
        "num_classes = 170  # Specify the number of output classes\n",
        "model = MultiModalModel(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = []\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for texts, images, labels in train_loader:\n",
        "        texts = list(texts)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(texts, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track training loss\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        # Track training accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = sum(train_loss) / len(train_loss)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = 100.0 * train_correct / train_total\n",
        "\n",
        "    # Validation loop\n",
        "    val_loss = []\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, images, labels in val_loader:\n",
        "            texts = list(texts)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(texts, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track validation loss\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            # Track validation accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Calculate average validation loss\n",
        "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = 100.0 * val_correct / val_total\n",
        "\n",
        "    # Print the loss and accuracy for every epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# After training, you can use the model for predictions on new data\n"
      ],
      "metadata": {
        "id": "6alVE4GdUXap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- version 2 --"
      ],
      "metadata": {
        "id": "cB2XYZIbYmvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import ElectraTokenizer, ElectraModel\n",
        "from PIL import Image\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, text_data, image_data, labels, text_tokenizer, transform=None):\n",
        "        self.text_data = text_data\n",
        "        self.image_data = image_data\n",
        "        self.labels = labels\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        image = Image.open(self.image_data[idx]).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return text, image, label\n",
        "\n",
        "\n",
        "# Define your data examples (text, image paths, and labels)\n",
        "text_data = list(df['product_name'])\n",
        "image_data = list()\n",
        "\n",
        "for idx in range(len(df)):\n",
        "    cat_id = df.loc[idx, 'cat_id']\n",
        "    prd_id = df.loc[idx, 'product_id']\n",
        "    image_path = f'data/drive/MyDrive/bungae_fashion_image/{cat_id}/{prd_id}_image.jpg'\n",
        "    image_data.append(image_path)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['label'] = encoder.fit_transform(df['cat_id'])\n",
        "labels = df['label']\n",
        "\n",
        "# Initialize the text tokenizer\n",
        "text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "# Define data augmentation transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create the custom dataset with data augmentation\n",
        "dataset = MultiModalDataset(text_data, image_data, labels, text_tokenizer, transform=image_transform)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = (len(dataset) - train_size) // 2\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Apply SMOTE and RandomUnderSampler to balance the dataset\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "under_sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "train_text = [sample[0] for sample in train_dataset]\n",
        "train_images = torch.stack([sample[1] for sample in train_dataset])\n",
        "train_labels = torch.tensor([sample[2] for sample in train_dataset])\n",
        "\n",
        "train_text_reshape = [text.reshape(-1, 1) for text in train_text]\n",
        "train_text_resampled, train_labels_resampled = smote.fit_resample(torch.cat(train_text_reshape), train_labels)\n",
        "train_text_resampled, train_labels_resampled = under_sampler.fit_resample(train_text_resampled, train_labels_resampled)\n",
        "\n",
        "train_text_resampled = [text.flatten().tolist() for text in train_text_resampled]\n",
        "train_labels_resampled = train_labels_resampled.tolist()\n",
        "\n",
        "# Convert the resampled data back to tensors\n",
        "train_text_resampled = [torch.tensor(text) for text in train_text_resampled]\n",
        "train_text_resampled = [text.squeeze() for text in train_text_resampled]\n",
        "train_text_resampled = [text.tolist() for text in train_text_resampled]\n",
        "train_images_resampled = train_images[train_labels_resampled]\n",
        "train_labels_resampled = torch.tensor(train_labels_resampled)\n",
        "\n",
        "# Reconstruct the resampled dataset\n",
        "train_dataset_resampled = [(text, image, label) for text, image, label in zip(train_text_resampled, train_images_resampled, train_labels_resampled)]\n",
        "\n",
        "# Define data loaders for train, validation, and test sets\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset_resampled, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the multi-modal model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.5):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        # Text Model (KoElectra)\n",
        "        self.text_model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "        self.text_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "        # Image Model (MobileNetV2)\n",
        "        self.image_model = models.mobilenet_v2(pretrained=True)\n",
        "        self.image_model.classifier = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # Fusion Layer\n",
        "        fusion_dim = self.text_model.config.hidden_size + self.image_model.last_channel\n",
        "        self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, text_inputs, image_inputs):\n",
        "        # Text Encoding\n",
        "        text_input_ids = self.text_tokenizer.batch_encode_plus(\n",
        "            text_inputs,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        text_outputs = self.text_model(input_ids=text_input_ids)[0][:, 0, :]  # Use the CLS token embedding\n",
        "\n",
        "        # Image Encoding\n",
        "        image_outputs = self.image_model.features(image_inputs)\n",
        "        image_outputs = torch.nn.functional.adaptive_avg_pool2d(image_outputs, 1).reshape(image_outputs.size(0), -1)\n",
        "\n",
        "        # Fusion\n",
        "        fusion_inputs = torch.cat((text_outputs, image_outputs), dim=1)\n",
        "        fused_outputs = self.fusion_layer(fusion_inputs)\n",
        "\n",
        "        # Dropout\n",
        "        fused_outputs = self.dropout(fused_outputs)\n",
        "\n",
        "        # Output\n",
        "        logits = self.output_layer(fused_outputs)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Initialize the multi-modal model\n",
        "num_classes = 170  # Specify the number of output classes\n",
        "model = MultiModalModel(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for text, image, label in train_loader:\n",
        "        text = text.to(device)\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(text, image)\n",
        "        loss = criterion(logits, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * text.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "BEZT9_BmYn9p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}