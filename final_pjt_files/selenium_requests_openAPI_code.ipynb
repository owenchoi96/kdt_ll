{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bc4496",
   "metadata": {},
   "source": [
    "# 엑셀 파일 말고 사이트별 category id CSV 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./NS_main_mid_sub1_cat.csv')\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b653441",
   "metadata": {},
   "source": [
    "# selenium 크롤링 -> 사용안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent(verify_ssl=False) # inspried by Nara's code\n",
    "fake_ua = ua.random\n",
    "headers = { 'user-agent' : fake_ua }\n",
    "\n",
    "url = 'https://search.shopping.naver.com/search/category/{}'.format('50003086')\n",
    "res = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "# soup.select('.style_content__xWg5l .noResult_no_result__6kgzY')\n",
    "\n",
    "mid_cat_list = soup.select('.filter_finder_list__4PE1C .filter_text_over__iesoO')\n",
    "mid_cat_list\n",
    "\n",
    "# mid_cat_list = [mid_cat.text for mid_cat in mid_cat_list] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ae52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눌렀는데 없는 거면 pass\n",
    "# 1. 눌렀는데 있는 건데 동일하면 pass\n",
    "# 2. 동일하지 않으면 긁어옴\n",
    "# 3. 여기서 만약에 들어갔는데 리스트로로 또 뽑았는데 없으면 => 검색결과가 없습니다\n",
    "#        있으면 검색결과가 있는 것이라고 확인해주는 건 있으면 좋을 것 같음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c852c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why this process?\n",
    "# => 하려고하는 메인 카테고리를 정하고 그에 따른 카테고리 이름, 아이디를 추출하는 데이터 프레임을 뽑기 위해\n",
    "# => 스마트 스토어 접근이 어려웠다면? \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver # webdriver를 이용해 해당 브라우저 열기\n",
    "from selenium.webdriver import ActionChains # 일련의 작업들을 연속적으로 진행할 수 있도록 도와줌\n",
    "from selenium.webdriver.common.keys import Keys # 키보드 입력을 할 수 있게 하기 위해\n",
    "from selenium.webdriver.common.by import By # html요소 탐색을 할 수 있게 하기 위해\n",
    "from selenium.webdriver.support.ui import WebDriverWait # 브라우저 응답을 기다릴 수 있게 하기 위해\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC # html 요소의 상태를 체크할 수 있게 하기 위해\n",
    "import pandas as pd\n",
    "\n",
    "# -- 처음 데이터 프레임 -- \n",
    "df = pd.read_csv('./NS_main_mid_sub1_cat.csv') \n",
    "\n",
    "def make_dict_by_mainCat(mainCat: str) -> dict:\n",
    "    \"\"\"\n",
    "    mainCat별 midCat과 sub1Cat 리스트로 나누어주는 \n",
    "    딕션어리를 만들어주는 함수입니다. \n",
    "    \"\"\"\n",
    "    main_mid_sub1_dict = {}\n",
    "    \n",
    "    # -- mainCat 기준으로 데이터 프레임 재생성 -- \n",
    "    df_by_main = df[df['mainCat_name'] == mainCat]\n",
    "    \n",
    "    # -- 데이터 프레임에서 unique한 midCat 구하기 --\n",
    "    midCat_list = list(df_by_main['midCat_name'].unique())\n",
    "    \n",
    "    # -- midCat에 따라 sub1Cat 뽑아와서 dictionary 형태로 만들기 --\n",
    "    mid_sub1_dict = defaultdict(list)\n",
    "    for midCat in midCat_list:\n",
    "        sub1_list = list(df_by_main[df_by_main['midCat_name'] == midCat]['sub1Cat_name'].unique()) # midCat에 따른 unique한 sub1 리스트\n",
    "        mid_sub1_dict[midCat] = sub1_list\n",
    "        \n",
    "    # -- mid_sub1_dict와 mainCat합쳐서 새로운 딕션어리 생성 --\n",
    "    main_mid_sub1_dict[mainCat] = dict(mid_sub1_dict)\n",
    "    \n",
    "    return main_mid_sub1_dict\n",
    "\n",
    "def bring_sub2Cat_id_from_url():\n",
    "    \"\"\"\n",
    "    url에서 sub2Cat_id만 가져오는 함수\n",
    "    \"\"\"\n",
    "    current_url = dr.current_url # 현재 주소에서 가져옴\n",
    "    cat_id = re.findall(r'\\/\\w+\\?', current_url)[0].lstrip('/').rstrip('?') \n",
    "    return cat_id\n",
    "\n",
    "def click_by_cat(cat):\n",
    "    \"\"\"\n",
    "    category별로 클릭해주는 함수\n",
    "    \"\"\"\n",
    "    el = dr.find_element_by_link_text(cat) \n",
    "    el.click()\n",
    "    ### 여기도 clickable 하게 만드는 코드 ###\n",
    "    return \n",
    "\n",
    "def bring_sub2Cat_Series(mainCat_dict: dict): # 대분류 딕션어리 하나 받았다고 가정하고\n",
    "    \n",
    "    sub2Cat_list = list()       #### 여기 바뀌어야되 ####\n",
    "    for mainCat, midCat_dict in mainCat_dict.items(): # mainCat & midCat list\n",
    "        for midCat, sub1Cat_list in midCat_dict.items(): # midCat & sub1Cat list\n",
    "            for sub1Cat in sub1Cat_list: # sub1Cat\n",
    "                \n",
    "                # -- 다음 sub2Cat 리스트 뽑아내기 --\n",
    "                url = 'https://search.shopping.naver.com/search/category/{}'.format(sub1Cat) # 처음부터 sub1Cat따라 url 들어오기\n",
    "                dr.get(url)\n",
    "                \n",
    "                # -- class 부분이 클릭이 가능해질 때까지 wait --\n",
    "                wait = WebDriverWait(driver, 8)\n",
    "                element = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'filter_finder_list__4PE1C')))\n",
    "                \n",
    "                # -- beatifulsoup으로 sub2에 해당하는 리스트 가져오기 -- \n",
    "                soup = BeautifulSoup(dr.page_source, 'html.parser')\n",
    "                sub2_list = soup.select('.filter_finder_list__4PE1C .filter_text_over__iesoO')\n",
    "                sub2_list = [sub2.text for sub2 in sub2_list]\n",
    "                \n",
    "                if (sub2_list == sub1Cat_list) | (sub2_list == []):\n",
    "                    sub2Cat_list.append([sub1Cat, ' ', ' ']) # sub2가 없는 경우 빈칸과 '_' 조인\n",
    "                    \n",
    "                else:    \n",
    "                    for sub2Cat_name in sub2_list:\n",
    "                        # -- sub2Cat 클릭 --\n",
    "                        click_by_cat(sub2Cat)\n",
    "                        # -- 잠시 대기 --\n",
    "                        dr.implicitly_wait(3)\n",
    "                        # -- url에서 카테고리 아이디 가져오기 --\n",
    "                        sub2Cat_id = bring_sub2Cat_id_from_url()\n",
    "                        sub2Cat_list.append([sub1Cat, sub2Cat_id, sub2Cat_name]) # sub2 있는 경우 '_' 조인\n",
    "                        # -- 다시 뒤로 돌아가기 -- \n",
    "                        click_by_cat(sub1Cat)\n",
    "                        # -- 잠기 대기 --\n",
    "                        dr.implicitly_wait(3)\n",
    "                \n",
    "                dr.close() # 다음 카테고리로 넘어가기 위해 창 닫아주기\n",
    "                time.sleep(0.07)\n",
    "    sub2Cat_df = pd.DataFrame(sub2Cat_list, columns=['sub1Cat_name', 'sub2Cat_id', 'sub2Cat_name'])\n",
    "    return sub2Cat_df  # 기존 데이터 프레임에 merge시키기 위해 시리즈 데이터로 변경 \n",
    "    \n",
    "def merge_mainCat_sub2Cat_df(mainCat):\n",
    "    mainCat_dict = make_dict_by_mainCat(mainCat)\n",
    "    sub2Cat_df = bring_sub2Cat_Series(mainCat_dict)\n",
    "    df_by_main = df[df['mainCat_name'] == mainCat]\n",
    "    final_df = pd.merge(df_by_main, sub2Cat_df, left_on='sub1Cat_name', right_on='sub1Cat_name', how='left')\n",
    "    \n",
    "    return final_df # sub2Cat_name, sub2Cat_id까지 합쳐진 mainCat 별 데이터 프레임\n",
    "\n",
    "\n",
    "# 1. series 데이터 '_'로 분리시켜서 데이터 프레임으로 변경뒤\n",
    "# 2. 기존 데이터 프레임에 left merge 시켜주기 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b738b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 심지어 없는 부분에 대해서도 알아서 Nan 처리를 해줌 => 거의 다 왔다 조금만..!\n",
    "pd.merge(tmp_df, tmp_df2, left_on='sub1Cat_name', right_on = 0, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4715d4",
   "metadata": {},
   "source": [
    "# Open API 크롤링 -> 안씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec807a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "query = \"남성 모자\"\n",
    "query = urllib.parse.quote(query)\n",
    "\n",
    "display = \"10\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/shop?query=\" + query + \"&display=\" + display\n",
    "\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header('X-Naver-Client-Id', '5oXnmo8N6zm_ipiF9EEr')\n",
    "request.add_header('X-Naver-Client-Secret', '3y6lUnczAb')\n",
    "\n",
    "response = urllib.request.urlopen(request)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb58b37",
   "metadata": {},
   "source": [
    "# requests, json 크롤링 -> 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycryptodome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679bed0",
   "metadata": {},
   "source": [
    "***catId를 넣어 크롤링 해오는 함수***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffca1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "def get_ns_prd_list(cat_id: str, page_num: int) ->list:\n",
    "    sbth = get_sbth()\n",
    "    ua = UserAgent(verify_ssl=False) \n",
    "    fake_ua = ua.random\n",
    "    headers = {\n",
    "        'user-agent' : fake_ua,\n",
    "        'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "        # -- sbth 없으면 정보 안가져옴 --\n",
    "        # -- update 필요할 수 있음 => 날짜 정보가 들어가 만료가 될 수 있음 -- \n",
    "        'sbth' : sbth,\n",
    "        # -- referer 없으면 정보 안가져옴 --\n",
    "        'referer' : 'https://search.shopping.naver.com/search/category',    \n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('sort','rel'),\n",
    "        ('pagingIndex',page_num),\n",
    "        ('pagingSize','40'),\n",
    "        ('viewType','list'),\n",
    "        # -- 여기에 해당하는 catId는 엑셀파일에 있는 catId --\n",
    "        ('catId', cat_id),\n",
    "    )\n",
    "\n",
    "\n",
    "    url = 'https://search.shopping.naver.com/api/search/category'\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    if res.status_code == 200:\n",
    "        results = res.json()\n",
    "        results = results['shoppingResult']['products'] # products 키 값안에 상품 정보 들어있음. \n",
    "        \n",
    "        prd_list = list()\n",
    "        for result in results:\n",
    "            prd_id = result['id']\n",
    "            prd_name = result['productName']\n",
    "            cat1_id = result['category1Id']\n",
    "            cat1_name = result['category1Name']\n",
    "            cat2_id = result['category2Id']\n",
    "            cat2_name = result['category2Name']\n",
    "            cat3_id = result['category3Id']\n",
    "            cat3_name = result['category3Name']\n",
    "            # -- cat 4 name -- \n",
    "            \n",
    "            prd_image_url = result['imageUrl']\n",
    "            prd_low_price = result['lowPrice']\n",
    "            prd_list.append([prd_id, prd_name, cat1_id, cat1_name, cat2_id, \n",
    "                             cat2_name, cat3_id, cat3_name, prd_image_url, prd_low_price]) \n",
    "    else:\n",
    "        prd_list.append([None, None, None, None, None,\n",
    "                         None, None, None, None, None]) # 변수들 None 값으로 처리 \n",
    "        \n",
    "    return prd_list\n",
    "\n",
    "\n",
    "\n",
    "def get_prd_info(cat_id: str, page_count = 250):\n",
    "    \"\"\"\n",
    "    cat_id -> 크롤링하고자 하는 category id\n",
    "    page_count -> 크롤링하고자 하는 페이지 수 (250 pages by default)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prd_list = list()\n",
    "    for page_num in range(1, page_count+1): # 250 페이지 가져오기 \n",
    "        prd_list += get_ns_prd_list(cat_id, page_num)\n",
    "        print('page %d done'%page_num)\n",
    "        \n",
    "        if page_num % 50 == 0:\n",
    "            print('-- paused for 5 sec in case not to be blocked --')\n",
    "            time.sleep(5)\n",
    "            \n",
    "    print('-- 데이터 개수 :  %d --'%(len(prd_list)))\n",
    "    \n",
    "    # -- prd_list에서 데이터 프레임 만들기 --\n",
    "    df = pd.DataFrame(prd_list, columns = ['prd_id', 'prd_name', 'cat1_id', 'cat1_name', 'cat2_id', \n",
    "                                           'cat2_name', 'cat3_id', 'cat3_name', 'prd_image_url', \n",
    "                                           'prd_low_price'])\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    return df\n",
    "\n",
    "\n",
    "get_prd_info('50000819', 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422573a",
   "metadata": {},
   "source": [
    "***catID url용 베이스 코드***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "sbth = get_sbth()\n",
    "ua = UserAgent(verify_ssl=False) \n",
    "fake_ua = ua.random\n",
    "headers = {\n",
    "    'user-agent' : fake_ua,\n",
    "    'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "    # -- sbth 없으면 정보 안가져옴 --\n",
    "    # -- update 필요할 수 있음 => 날짜 정보가 들어가 만료가 될 수 있음 -- \n",
    "    'sbth' : sbth,\n",
    "    # -- referer 없으면 정보 안가져옴 --\n",
    "    'referer' : 'https://search.shopping.naver.com/search/category',    \n",
    "}\n",
    "\n",
    "params = (\n",
    "    ('sort','rel'),\n",
    "    ('pagingIndex','2'),\n",
    "    ('pagingSize','40'),\n",
    "    ('viewType','list'),\n",
    "    # -- 여기에 해당하는 catId는 엑셀파일에 있는 catId --\n",
    "#     ('catId', cat_id),\n",
    ")\n",
    "\n",
    "\n",
    "# url = 'https://search.shopping.naver.com/api/search/category'\n",
    "# res = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "\n",
    "\n",
    "# url = 'https://search.shopping.naver.com/api/search/category/100005522'\n",
    "# url = 'https://search.shopping.naver.com/api/search/category/100005522?catId=50000832'\n",
    "url = 'https://search.shopping.naver.com/api/search/category/100000386?catId=50000425%2050000435%2050001487%2050001488'\n",
    "res = requests.get(url, headers=headers, params=params)\n",
    "data = res.json()\n",
    "data['shoppingResult']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907ba4d",
   "metadata": {},
   "source": [
    "***검색 쿼리용 베이스 코드***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb41e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703383ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "\n",
    "sbth = get_sbth() # sbth 가져오기\n",
    "ua = UserAgent(verify_ssl=False) # UserAgent 생성\n",
    "fake_ua = ua.random # UserAgent 생성\n",
    "\n",
    "headers = {\n",
    "    'user-agent' : fake_ua,\n",
    "    'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "    # -- sbth 없으면 정보 안가져옴 --\n",
    "    # -- 자동 sbth 생성 코드 적용 -- \n",
    "    'sbth' : sbth,\n",
    "    # -- referer 없으면 정보 안가져옴 --\n",
    "    'referer' : 'https://search.shopping.naver.com/search/all'\n",
    "}\n",
    "\n",
    "params = (\n",
    "    ('sort','rel'),\n",
    "#     ('origQuery', query),\n",
    "#     ('query', query),\n",
    "    ('productSet', 'total'), # 이게 뭐지?\n",
    "    ('pagingIndex', '2'),\n",
    "    ('pagingSize','40'),\n",
    "    ('viewType','list'),\n",
    ") \n",
    "\n",
    "url = 'https://search.shopping.naver.com/api/search/all?query=여성항공점퍼'\n",
    "res = requests.get(url, headers=headers, params=params)\n",
    "data = res.json()\n",
    "data['shoppingResult']['products']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34c950",
   "metadata": {},
   "source": [
    "***크롤링에 사용될 수 있도록 url 전처리 해주는 함수***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff655d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def get_final_url_df():\n",
    "    \"\"\"\n",
    "    카테고리 매칭 작업했던 파일을 가져오고\n",
    "    최종 url에서 'api'단어를 붙이기 위해 준비해주는 함수. \n",
    "    \"\"\"\n",
    "    df = pd.read_csv('./ns_카테고리 매칭 - 최종 카테고리.csv', header=1)\n",
    "    df['temp URL'] = df['temp URL'].fillna(df['최종 URL']) # 비어있는 temp URL 부분을 최종 URL로 채워줍니다. \n",
    "    return df \n",
    "\n",
    "def add_api_into_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    url 안에 'api'단어를 붙여주는 함수.\n",
    "    'api'단어가 있어야 크롤링 가능. \n",
    "    \"\"\"\n",
    "    new_url = None\n",
    "    try:\n",
    "        new_url = re.sub('(.com\\/search)', '.com/api/search', url)\n",
    "    except:\n",
    "        new_url = 'no_url'\n",
    "    return new_url\n",
    "\n",
    "def return_url_list():\n",
    "    \"\"\"\n",
    "    크롤링이 가능한 url을 반환해주는 함수.\n",
    "    \"\"\"\n",
    "    df = get_final_url_df()\n",
    "    df['url_for_crawling'] = df['temp URL'].map(lambda x : add_api_into_url(x))\n",
    "    url_list = list(df['url_for_crawling'])\n",
    "    url_list = [url for url in url_list if url != 'no_url']\n",
    "    \n",
    "    # -- 리스트 txt 파일로 만들어주기 -- \n",
    "    with open('ns_urls.txt', 'w+') as file:\n",
    "        file.write(','.join(url_list))\n",
    "        \n",
    "    return \n",
    "\n",
    "from ns_make_urls_for_crawling import return_url_list\n",
    "return_url_list()\n",
    "\n",
    "# list로 다시 불러오기\n",
    "with open('./ns_urls.txt', 'r') as f:\n",
    "    naver_urls = f.read()\n",
    "naver_url_list = naver_urls.split(',')\n",
    "\n",
    "print(len(naver_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31016b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ns_make_urls_for_crawling import return_url_list\n",
    "return_url_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65983500",
   "metadata": {},
   "source": [
    "***크롤링 최종 함수***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed683cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "def get_ns_prd_list(url: str, page_num: int) ->list:\n",
    "    \n",
    "    sbth = get_sbth()\n",
    "    ua = UserAgent(verify_ssl=False) \n",
    "    fake_ua = ua.random\n",
    "    headers = {\n",
    "        'user-agent' : fake_ua,\n",
    "        'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "        # -- sbth 없으면 정보 안가져옴 --\n",
    "        'sbth' : sbth,\n",
    "        # -- referer 없으면 정보 안가져옴 --\n",
    "        'referer' : 'https://search.shopping.naver.com/search/category',    \n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('sort','rel'),\n",
    "        ('pagingIndex', page_num),\n",
    "#         ('productSet', 'total'), # 전체 탭 보여주는 파라미터 (가격비교, 네이버페이 등으로 변환 가능)\n",
    "        ('pagingSize','40'),\n",
    "        ('viewType','list'),\n",
    "    )\n",
    "        \n",
    "    prd_list = list()\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    if (res.status_code == 200):\n",
    "        results = res.json()\n",
    "        results = results['shoppingResult']['products'] # products 키 값안에 상품 정보 들어있음. \n",
    "        \n",
    "        for result in results:\n",
    "            prd_id = result['id']\n",
    "            prd_name = result['productName']\n",
    "            cat1_id = result['category1Id']\n",
    "            cat1_name = result['category1Name']\n",
    "            cat2_id = result['category2Id']\n",
    "            cat2_name = result['category2Name']\n",
    "            cat3_id = result['category3Id']\n",
    "            cat3_name = result['category3Name']\n",
    "            cat4_id = result['category4Id']\n",
    "            cat4_name = result['category4Name']\n",
    "            \n",
    "            prd_image_url = result['imageUrl']\n",
    "            prd_low_price = result['lowPrice']\n",
    "            prd_list.append([prd_id, prd_name, cat1_id, cat1_name, cat2_id, \n",
    "                             cat2_name, cat3_id, cat3_name, cat4_id, cat4_name, \n",
    "                             prd_image_url, prd_low_price]) \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return prd_list\n",
    "\n",
    "\n",
    "\n",
    "def get_prd_info(url: str, page_count = 250):\n",
    "    \"\"\"\n",
    "    cat_id -> 크롤링하고자 하는 category id\n",
    "    page_count -> 크롤링하고자 하는 페이지 수 (250 pages by default)\n",
    "    \"\"\"\n",
    "    prd_list = list()\n",
    "    for page_num in range(1, page_count+1): # 250 페이지 가져오기 \n",
    "        prd_list += get_ns_prd_list(url, page_num)\n",
    "        print('page %d done'%page_num)\n",
    "        \n",
    "        if page_num % 50 == 0:\n",
    "            print('-- paused for 5 sec in case not to be blocked --')\n",
    "            time.sleep(5)\n",
    "            \n",
    "    print('-- 데이터 개수 :  %d --'%(len(prd_list)))\n",
    "    \n",
    "    # -- prd_list에서 데이터 프레임 만들기 --\n",
    "    df = pd.DataFrame(prd_list, columns = ['prd_id', 'prd_name', 'cat1_id', 'cat1_name', 'cat2_id', \n",
    "                                           'cat2_name', 'cat3_id', 'cat3_name', 'cat4_id', 'cat4_name',\n",
    "                                           'prd_image_url', 'prd_low_price'])\n",
    "    return df\n",
    "\n",
    "\n",
    "# url = 'https://search.shopping.naver.com/api/search/category/100005542?catId=50000841'\n",
    "url = 'https://search.shopping.naver.com/api/search/all?query=여성항공점퍼'\n",
    "get_prd_info(url, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08709ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
