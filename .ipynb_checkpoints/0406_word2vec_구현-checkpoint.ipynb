{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5975a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 이론 #####\n",
    "# word embedding\n",
    "\n",
    "# Word2Vec : 수많은 단어를 통해 단어 벡터를 학습\n",
    "#          : 유사 단어 간에 가까운 경향을 파악 \n",
    "\n",
    "# 많은 양의 데이터 + 단순한 모델이 적은 데이터 + 복잡한 모델보다 더 낫다는 것을 가정\n",
    "# 수십억 단어로 질 좋은 단어 벡터를 학습\n",
    "# 유사 단어 간에는 거리가 가까운 경향이 있고, 단어는 다양한 유사도를 가짐\n",
    "\n",
    "# Model architecture:\n",
    "    # LSA보다 뛰어난 선형 정규성\n",
    "    # LDA는 데이터 양이 많을수록 많은 연산을 필요로 함. (cost가 많이 듬)\n",
    "    # Word2Vec 복잡도 O=E x T x Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5315aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "    # CBOW (continuous bag of words) : 주변 단어를 보고 해당 타켓 단어를 예측하는 방법\n",
    "        # 1. sliding window=5\n",
    "        # 2. target word\n",
    "        # 3. context \n",
    "        # 이렇게 3가지 정도가 있다고 볼 수 있음.\n",
    "        \n",
    "        # V => unique한 단어 벡터의 수\n",
    "        # N => (사용자가 지정 가능한) N만큼 V의 값을 가져옴\n",
    "            # N만큼의 행렬이 생성 => 여기서 평균을 낸것이 hidden vector \n",
    "            \n",
    "    # skip-gram model\n",
    "        # 하나의 단어를 보고 주변 단어를 예측\n",
    "        # 낮은 연산 복잡도\n",
    "        # 높은 정확도\n",
    "        \n",
    "        \n",
    "# Transfer Learning / Fine Tuning\n",
    "    # transfer learning : 풍부한 데이터가 있는 분야에서 훈련된 모델을 재사용\n",
    "    # fine tuning : 사전 학습된 모델을 task에 적합한 데이터로 추가 학습하여 파라미터 미세 조정\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 필요과정\n",
    "    # Feed Forwarding\n",
    "    # Backpropagation\n",
    "    # Weights 갱신\n",
    "    # Loss 그래프\n",
    "    # 벡터간 유사도 측정\n",
    "    # 가장 유사한 벡터 목록 출력\n",
    "    \n",
    "# CBOW 학습\n",
    "    # 입력 문장 토큰화 (사용하지 않는 품사는 제거)\n",
    "    # 가중치 (=파라미터=단어벡터) 초기화\n",
    "    # 원핫 인코딩 (one hot encoding)\n",
    "    # epoch 만큼 반복 \n",
    "        # 중심단어 문맥단어 추출\n",
    "        # Feed Forward\n",
    "        # Loss / Gradient 계산\n",
    "        # Weight Parameter 계산\n",
    "\n",
    "## 학습 과정 ##\n",
    "# 1. 학습하는 가중치가 단어 벡터\n",
    "# 2. 입력 단어 (원핫 벡터)와 단어 벡터의 곱으로 hidden layer 계산 (Feed Forward 1)\n",
    "# 3. Hidden layer 선택된 \"하나의\" 벡터 \n",
    "# 4. Hidden layer와 단어 벡터의 곱으로 output layer 계산 (Feed Forward 2)\n",
    "# 5. (단어간 유사도 계산) 선택된 단어 벡터와 전체 단어 벡터간 유사도 ==> softmax()\n",
    "# 6. epoch 별 Loss / Gradient 계산 (W1 행렬, W2 행렬 편미분 그리고 가중치 업데이트)\n",
    "# 7. CrossEntrophy (혹은 BinaryEntrophy) : weights를 gradient * learning rate 만큼 갱신\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2606d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### fasttext #####\n",
    "# 단어를 bag of characters로 보고 n_gram의 characters embedding 함 (skip_gram model 사용)\n",
    "# Fasttext는 단어를 쪼갤 수 있음 ==> subwords (내부 단어)\n",
    "    # 때문에 내부 단어를 통해 모르는 단어 (OOV)에 대해서도 다른 단어와의 유사도를 계산 가능\n",
    "    \n",
    "# Fasttext의 경우 단어에 n_gram을 하기 때문에 학습 경우의 수가 많아지므로 정확도가 높아지는 경향이 있음.\n",
    "# Fasttext는 오타가 섞여도 일정 수준 성능을 보임 ==> 노이즈가 많은 말뭉치에 강함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd7cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9f961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19df7338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'will', 'never', 'know', 'until', 'you', 'try']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 문장 토큰화 \n",
    "text = 'you will never know until you try'\n",
    "tokens = text.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1728c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 (초기화)\n",
    "V, N = 6, 4\n",
    "\n",
    "W1 = np.random.rand(V,N)\n",
    "W2 = np.random.rand(V,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b299cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n",
      "{'try': 0, 'know': 1, 'until': 2, 'never': 3, 'you': 4, 'will': 5}\n",
      "{0: 'try', 1: 'know', 2: 'until', 3: 'never', 4: 'you', 5: 'will'}\n"
     ]
    }
   ],
   "source": [
    "# 원핫 인코딩 \n",
    "import numpy as np\n",
    "\n",
    "onehot_vector = np.zeros(shape=(V,V), dtype = np.int32)\n",
    "unique_tokens = set(tokens)\n",
    "unique_tokens\n",
    "\n",
    "\n",
    "word2idx = {}\n",
    "for idx, word in enumerate(unique_tokens):\n",
    "    onehot_vector[idx][idx] = 1\n",
    "    word2idx[word] = idx\n",
    "    \n",
    "idx2word = {v:k for k,v in word2idx.items()} # 단순히 key와 value값을 바꿔서 매핑 \n",
    "\n",
    "print(onehot_vector)\n",
    "print(word2idx)\n",
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5d89c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 생성\n",
    "# window_size = 1\n",
    "\n",
    "window_size=1\n",
    "training_data = []\n",
    "for idx, token in enumerate(tokens):\n",
    "    target_word = token\n",
    "    context_words = []\n",
    "    for window in range(1, window_size+1):\n",
    "        if idx-window >= 0:\n",
    "            context_words.append(tokens[idx-window])\n",
    "        if idx+window < len(tokens):\n",
    "            context_words.append(tokens[idx+window])  \n",
    "    #context_words = [tokens[idx-1], tokens[idx+1]] # window 사이즈가 1이고 주변 단어를 가져옴\n",
    "                              \n",
    "    training_data.append([context_words, target_word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6301baf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'will', 'never', 'know', 'until', 'you', 'try']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54974d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['will'], 'you'],\n",
       " [['you', 'never'], 'will'],\n",
       " [['will', 'know'], 'never'],\n",
       " [['never', 'until'], 'know'],\n",
       " [['know', 'you'], 'until'],\n",
       " [['until', 'try'], 'you'],\n",
       " [['you'], 'try']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65787c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01821127, -0.24519181,  0.26340309])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 값을 1아래로 만들어주는 함수 \n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y=exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "a = np.array([0.3, 2.9, 4.0])\n",
    "predict = softmax(a)\n",
    "output = np.array([0,0,1]) \n",
    "loss = output - predict # 예측한 것과 실제 값의 차이 \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 학습구현\n",
    "\n",
    "# 1. Feed Forward\n",
    "# - context 단어를 W1 행렬에 곱해서 hidden vector 만들기\n",
    "# - 만든 hidden vector들 평균내기\n",
    "\n",
    "# - 평균 낸 hidden vector를 가지고 W2에 곱해서 Vx1 크기의 벡터로 만들기\n",
    "# - softmax 씌어숴 합이 1이 되도록 조정\n",
    "\n",
    "# 2. Loss 계산\n",
    "# - cross entrophy loss\n",
    "\n",
    "# 3. Backpropagation\n",
    "# - 각 행렬에 대해서 편미분한 기울기 구하기\n",
    "# - 각 행렬에 대해 기울기 + learning rate 만큼 가중치 update\n",
    "\n",
    "# 4. Epoch\n",
    "# - epoch 수 만큼 반복 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9591ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d127d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472f0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267d91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd4777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e3ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a32556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245ee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
