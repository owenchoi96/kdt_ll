{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66438d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "# : 자연어 처리를 위해 용도에 맞도록 사전에 표준화하는 작업. \n",
    "\n",
    "# 필요성\n",
    "# : 텍스트 내 정보를 유지하고, 분석의 효율을 높임. \n",
    "\n",
    "# 분석하기 전에 텍스트를 분석에 적합한 형태로 변환하는 작업.\n",
    "# 전처리 단계는 텍스트를 토큰화하고 자연어처리에 필요 없는 조사, 특수문자, 불용어의 제거 과정을 포함\n",
    "# 전처리는 분석결과 모델 성능에 직접 영향을 미치기 때문에 전처리 단계는 매우 중요. \n",
    "\n",
    "\n",
    "## 전처리의 중요성 ##\n",
    "# 80%의 시간을 전처리에 보냄\n",
    "\n",
    "## 토큰화 ##\n",
    "# 토큰화\n",
    "# : 구두점으로 문서를 문장으로 분리하는 문장 토큰화\n",
    "# : 단어 단위로 분리하는 단어 토큰화\n",
    "\n",
    "# 문장 토큰화\n",
    "# : 문장을 기준으로 토큰화\n",
    "# : 온점, 느낌표 , 물음표 등으로 분류하면 해결 될 것으로 생각됨\n",
    "# : 하지만 단순하게 분리할 경우 정확한 분리가 어려움. \n",
    "# (단순하게 온점으로 분리하게 되면 원하는 데이터를 얻기 어려울 수 있음)\n",
    "# (SNS의 경우 온점을 다른 용도로 이용하는 경우가 있음.....)\n",
    "\n",
    "# 단어 토큰화\n",
    "# 단어를 기준으로 토큰화\n",
    "# 영문의 경우 공백을 기준으로 분리하면 유의미한 토큰화가 가능\n",
    "# 반면 한글의 경우 품사를 고려한 토큰화(=형태소분석)가 필요함. \n",
    "\n",
    "# 단어 토큰화 고려사항\n",
    "# : 특수 문자 여부. (구두점 및 특수문자를 단순하게 제외해서는 안됨.)\n",
    "# : 단어 내 띄어쓰기가 있는 경우. \n",
    "\n",
    "### processing (토큰 처리) ###\n",
    "# 품사 태깅 (Pos tagging)\n",
    "# : 각 토큰에 품사 정보를 추가\n",
    "# : 분석시에 불필요한 품사를 제거하거나 필요한 품사를 필터링 하기 위해 사용. \n",
    "\n",
    "\n",
    "# 개체명 인식\n",
    "# : 사람, 조직, 지역, 날짜, 숫자 등을 개체 유형을 식별\n",
    "# : 검색 엔진 색인에 활용. \n",
    "\n",
    "# 원형 복원\n",
    "# : 각 토큰의 원형 복원을 함으로써 토큰을 표준화. 불필요한 데이터 중복 방지 (= 단어의 수를 줄일 수 있어 연산의 효율성을 높임)\n",
    "\n",
    "# 어간 추출\n",
    "# : 품사를 무시하고 규칙에 기반하여 어간을 추출. \n",
    "\n",
    "# 표제어 추출\n",
    "# : 품사정보를 유지하여 표제어 추출 (사전 기반) \n",
    "\n",
    "# 불용어 처리 (stopwords)\n",
    "# : 불필요한 토큰 제거\n",
    "# : 분석 시 불필요한 품사를 제거하기도 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0c0fd",
   "metadata": {},
   "source": [
    "# 영어 전처리 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d96de8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n",
    "eng_text = eng_news[3].get_text()\n",
    "\n",
    "eng_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a05d0fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'aObama', 'likes', 'fried', 'very', 'much']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'Barack aObama likes fried very much'\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdaf06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "token1 = word_tokenize(eng_text)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알파벳이 아닌 문자를 구분하여 토큰화. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1bbb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = 'Barack Obama likes fried chicken very much'\n",
    "wordpuncttoken = WordPunctTokenizer().tokenize(text)\n",
    "print(wordpuncttoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f09701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2654ae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = 'Barack Obama likes fried chicken very much'\n",
    "treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n",
    "print(treebankwordtoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d88c4cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wonbinchoi/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'NNP'), ('aObama', 'NN'), ('likes', 'NNS'), ('fried', 'VBD'), ('very', 'RB'), ('much', 'JJ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "taggedToken = pos_tag(word_tokens)\n",
    "print(taggedToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개체명 인식 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4504cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/wonbinchoi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/wonbinchoi/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Barack/NNP)\n",
      "  (ORGANIZATION aObama/NN)\n",
      "  likes/NNS\n",
      "  fried/VBD\n",
      "  very/RB\n",
      "  much/JJ)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "from nltk import ne_chunk\n",
    "neToken = ne_chunk(taggedToken)\n",
    "print(neToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14997ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출 (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be7f7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "beautiful -> beauti\n",
      "believes -> believ\n",
      "using -> use\n",
      "conversation -> convers\n",
      "organization -> organ\n",
      "studies -> studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"running -> \" + ps.stem(\"running\"))\n",
    "print(\"beautiful -> \" + ps.stem(\"beautiful\"))\n",
    "print(\"believes -> \" + ps.stem(\"believes\"))\n",
    "print(\"using -> \" + ps.stem(\"using\"))\n",
    "print(\"conversation -> \" + ps.stem(\"conversation\"))\n",
    "print(\"organization -> \" + ps.stem(\"organization\"))\n",
    "print(\"studies -> \" + ps.stem(\"studies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43637c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d976cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wonbinchoi/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/wonbinchoi/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> running\n",
      "beautiful -> beautiful\n",
      "believes -> belief\n",
      "using -> using\n",
      "conversation -> conversation\n",
      "organization -> organization\n",
      "studies -> study\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
    "print(\"beautiful -> \" + wl.lemmatize(\"beautiful\"))\n",
    "print(\"believes -> \" + wl.lemmatize(\"believes\"))\n",
    "print(\"using -> \" + wl.lemmatize(\"using\"))\n",
    "print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n",
    "print(\"organization -> \" + wl.lemmatize(\"organization\"))\n",
    "print(\"studies -> \" + wl.lemmatize(\"studies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc292c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "# 처리하는 목적에 맞춰서 불용어 처리를 해주면 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "426569f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'aObama', 'likes', 'fried', 'very', 'much']\n"
     ]
    }
   ],
   "source": [
    "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']\n",
    "\n",
    "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
    "from collections import Counter\n",
    "Counter(taggedToken).most_common()\n",
    "\n",
    "stopWord = [',','be','able']\n",
    "\n",
    "word = []\n",
    "for tag in taggedToken:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])\n",
    "            \n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f01ad",
   "metadata": {},
   "source": [
    "# 한글 전처리 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbc3f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 코모란(Komoran) 토큰화\n",
    "from konlpy.tag import Komoran\n",
    "komoran= Komoran()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "komoran_tokens = komoran.morphs(kor_text)\n",
    "print(komoran_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ca577d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 한나눔(Hannanum) 토큰화\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum= Hannanum()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "hannanum_tokens = hannanum.morphs(kor_text)\n",
    "print(hannanum_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3022056b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m okt\u001b[38;5;241m=\u001b[39m Okt()\n\u001b[1;32m      4\u001b[0m kor_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m okt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mokt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkor_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(okt_tokens)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/konlpy/tag/_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[0;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/konlpy/tag/_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[0;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[0;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Okt 토큰화\n",
    "# sns를 분석할 때는 okt를 많이 씀.\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt= Okt()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "okt_tokens = okt.morphs(kor_text)\n",
    "print(okt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0567bbe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m kkma\u001b[38;5;241m=\u001b[39m Kkma()\n\u001b[1;32m      4\u001b[0m kor_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m kkma_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mkkma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkor_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(kkma_tokens)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/konlpy/tag/_kkma.py:95\u001b[0m, in \u001b[0;36mKkma.morphs\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/konlpy/tag/_kkma.py:66\u001b[0m, in \u001b[0;36mKkma.pos\u001b[0;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m:param flatten: If False, preserves eojeols.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m validate_phrase_inputs(phrase)\n\u001b[0;32m---> 66\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m morphemes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentences:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Kkma 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "kkma= Kkma()\n",
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "kkma_tokens = kkma.morphs(kor_text)\n",
    "print(kkma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff99b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ed9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1f379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43d8ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f9315a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'navernews_it', using template directory '/Users/wonbinchoi/opt/anaconda3/lib/python3.9/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/wonbinchoi/workspace/lecture/7_NLP/0_통계/실습자료/navernews_it\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd navernews_it\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject navernews_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea87de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wonbinchoi/workspace/lecture/7_NLP/0_통계/실습자료/navernews/navernews/spiders\n"
     ]
    }
   ],
   "source": [
    "cd ./navernews/navernews/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15c7f0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wonbinchoi\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf13a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05566b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e04aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84a4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec07ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9fec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d67ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4fb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f422a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db09332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531d47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7dae91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54950a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691a98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fb77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09279c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
